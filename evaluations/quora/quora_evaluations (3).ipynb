{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfhyxWokDTbH",
        "outputId": "f982a1cd-2443-46d2-9370-5f5691e092e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRD39YW9fJuT"
      },
      "source": [
        "TFIDF_EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-mxxr7PBXxh",
        "outputId": "c860660c-90e7-4ac2-ee55-f51bdc55eb0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, nltk, anyascii, textsearch, contractions\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyascii-0.3.3 contractions-0.1.73 nltk-3.8.1 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "Collecting country_converter\n",
            "  Downloading country_converter-1.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Downloading country_converter-1.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: country_converter\n",
            "Successfully installed country_converter-1.3\n",
            "Collecting datefinder\n",
            "  Downloading datefinder-0.7.3-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n",
            "Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: datefinder\n",
            "Successfully installed datefinder-0.7.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 522770/522770 [09:26<00:00, 922.42it/s]\n",
            "ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 522770/522770 [00:32<00:00, 16055.12it/s]\n",
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [17:29<00:00,  4.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== Evaluation for dataset: quora ==\n",
            "MAP: 66.82 %\n",
            "MRR: 70.9 %\n",
            "Mean Precision: 11.05 %\n",
            "Mean Recall: 81.78 %\n",
            "Mean Precision@10: 10.94 %\n"
          ]
        }
      ],
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù…\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH)\n",
        "\n",
        "\n",
        "\n",
        "def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "    df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "    index = defaultdict(set)\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³\"):\n",
        "        doc_id = str(row[\"doc_id\"])\n",
        "        tokens = str(row[\"processed_text\"]).split()\n",
        "        for token in tokens:\n",
        "            index[token].add(doc_id)\n",
        "    os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "    with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "    return index\n",
        "\n",
        "inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "    return df\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "\n",
        "tqdm.pandas(desc=\">> ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "\n",
        "retrieved_docs_dict = {}\n",
        "for i, (qid, tokens) in enumerate(tqdm(queries_tokens.items(), desc=\"ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"), start=1):\n",
        "    retrieved = retrieve_docs(tokens, inverted_index)\n",
        "    query_vec = represent_query(tokens)\n",
        "    ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "\n",
        "def cal_evaluations(dataset_name=\"quora\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs_dict).items()}\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "cal_evaluations(\"quora\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZGCu4VvfO_l"
      },
      "source": [
        "EMBEDDING_EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bly7eQk98Kg",
        "outputId": "3904689a-ab0b-4953-d1f4-51f93993926f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding_beir.joblib\n",
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: /content/drive/MyDrive/dataset_quora_dev/queries.tsv\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 5000\n",
            "ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\n",
            "  query_id                                               text  \\\n",
            "0      318                How does Quora look to a moderator?   \n",
            "1      378  How do I refuse to chose between different thi...   \n",
            "2      379  Did Ben Affleck shine more than Christian Bale...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0                 how does quora look to a moderator  \n",
            "1  how do i refuse to chose between different thi...  \n",
            "2  did ben affleck shine more than christian bale...  \n",
            "ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ 5000 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
            "ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ 5000 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
            "\n",
            "ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\n",
            "ğŸ“Œ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù„Ù„ØªÙ‚ÙŠÙŠÙ…: 5000\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª qrels: 5000\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹: 5000\n",
            "ğŸ”— Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ø¨ÙŠÙ† qrels ÙˆØ§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹: 5000\n",
            "â— Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø± Ù„Ù‡Ø§ ÙÙŠ qrels Ù„ÙƒÙ†Ù‡Ø§ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØªÙ…Ø«ÙŠÙ„: 0\n",
            "\n",
            "== Evaluation for dataset: quora ==\n",
            "MAP: 84.06 %\n",
            "MRR: 86.56 %\n",
            "Mean Precision: 2.93 %\n",
            "Mean Recall: 98.84 %\n",
            "Mean Precision@10: 13.21 %\n"
          ]
        }
      ],
      "source": [
        "import os, re, string, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ========\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = \"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/quora_cleaned_docs_beir\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_beir.joblib\")\n",
        "TOP_K = 50\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ù†ØµÙˆØµ ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØªÙ…Ø«ÙŠÙ„Ù‡Ø§ ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"ğŸš€ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§ØªØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"ğŸ—‚ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        print(\"âš ï¸ Ø¹Ù…ÙˆØ¯ 'light_clean_text' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ â€” Ø³ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡.\")\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    print(\"ğŸ§ª Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù†Ø¸ÙØ©:\")\n",
        "    print(df_docs[[\"doc_id\", \"light_clean_text\"]].head())\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ÙÙŠ: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: {path}\")\n",
        "    df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str)\n",
        "    print(f\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df)}\")\n",
        "\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(\"ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ======== ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ========\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "# ======== Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ========\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"quora\"):\n",
        "    print(\"\\nğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "    print(f\"ğŸ“Œ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù„Ù„ØªÙ‚ÙŠÙŠÙ…: {len(retrieved_docs_str)}\")\n",
        "    print(\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª qrels:\", len(qrel_dict))\n",
        "    print(\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹:\", len(retrieved_docs_str))\n",
        "    print(\"ğŸ”— Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ø¨ÙŠÙ† qrels ÙˆØ§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹:\", len(set(qrel_dict.keys()) & set(retrieved_docs_str.keys())))\n",
        "\n",
        "# â›” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØªÙ…Ø«ÙŠÙ„\n",
        "    all_qrels_doc_ids = {docid for rels in qrel_dict.values() for docid in rels}\n",
        "    missing_doc_ids = all_qrels_doc_ids - set(doc_ids)\n",
        "    print(f\"â— Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø± Ù„Ù‡Ø§ ÙÙŠ qrels Ù„ÙƒÙ†Ù‡Ø§ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØªÙ…Ø«ÙŠÙ„: {len(missing_doc_ids)}\")\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== ØªØ´ØºÙŠÙ„ ÙƒØ§Ù…Ù„ ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ {len(query_embeddings)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
        "\n",
        "    retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "    print(f\"ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ {len(retrieved_docs)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=\"quora\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BM25"
      ],
      "metadata": {
        "id": "Zc-gDdFskeQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkUhcXv0byyX",
        "outputId": "6b2f7816-5a1c-4d4c-deb1-a023dba0ce21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ BM25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BM25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [1:08:36<00:00,  1.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP: 70.65%\n",
            "MRR: 74.75%\n",
            "Mean Precision: 11.51%\n",
            "Mean Recall: 85.50%\n",
            "Precision@10: 11.51%\n"
          ]
        }
      ],
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… (ØªØ­ØªØ§Ø¬ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm rank_bm25\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ÙƒÙ…Ø§ ÙÙŠ ÙƒÙˆØ¯Ùƒ\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/quora_cleaned_docs_beir\"\n",
        "BM25_MODEL_PATH = f\"/content/drive/MyDrive/utils/bm25_model/{DATASET_NAME}_bm25.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "# === Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ BM25 ===\n",
        "def build_bm25_model(cleaned_docs_tsv=CLEANED_TSV_PATH, save_path=BM25_MODEL_PATH):\n",
        "    df = pd.read_csv(cleaned_docs_tsv, sep=\"\\t\")\n",
        "    df = df[df['processed_text'].notna() & (df['processed_text'].str.strip() != \"\")]\n",
        "\n",
        "    corpus = [doc.split() for doc in df[\"processed_text\"]]\n",
        "    doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "\n",
        "    bm25 = BM25Okapi(corpus)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    joblib.dump({\"bm25\": bm25, \"doc_ids\": doc_ids, \"corpus\": corpus}, save_path)\n",
        "    print(\"ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ BM25\")\n",
        "\n",
        "def load_bm25_model(path=BM25_MODEL_PATH):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(\"Ù†Ù…ÙˆØ°Ø¬ BM25 ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¦Ù‡ Ø£ÙˆÙ„Ø§Ù‹.\")\n",
        "    return joblib.load(path)\n",
        "\n",
        "def clean_and_tokenize_query(raw_query):\n",
        "    return clean_text(raw_query)\n",
        "\n",
        "def retrieve_bm25_docs(query_tokens, bm25_data, top_k=10):\n",
        "    bm25 = bm25_data[\"bm25\"]\n",
        "    doc_ids = bm25_data[\"doc_ids\"]\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "    results = [(doc_ids[i], scores[i]) for i in ranked_indices]\n",
        "    return results\n",
        "\n",
        "# === ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH)\n",
        "\n",
        "# === Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ BM25 ===\n",
        "build_bm25_model(CLEANED_TSV_PATH, BM25_MODEL_PATH)\n",
        "\n",
        "# === ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ===\n",
        "bm25_data = load_bm25_model(BM25_MODEL_PATH)\n",
        "\n",
        "# === ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ===\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "    return df\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# === Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BM25 ===\n",
        "retrieved_docs_dict = {}\n",
        "for qid, tokens in tqdm(queries_tokens.items(), desc=\"ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BM25\"):\n",
        "    ranked = retrieve_bm25_docs(tokens, bm25_data, top_k=10)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "# === Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù†ÙØ³Ù‡Ø§ Ù…Ù† ÙƒÙˆØ¯Ùƒ Ø§Ù„Ø£ØµÙ„ÙŠ ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall\n",
        "    return np.mean(list(recall_scores.values())) * 100\n",
        "\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "# === ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù€ qrels ===\n",
        "qrel_dict, real_relevant = get_qrels(QRELS_PATH)\n",
        "\n",
        "# === Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø¯Ø±Ø¬Ø§Øª ÙÙ‚Ø· ===\n",
        "retrieved_only_docs = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "# === ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ ===\n",
        "map_score = calculate_map(qrel_dict, retrieved_only_docs)\n",
        "mrr_score = calculate_mrr(qrel_dict, retrieved_only_docs)\n",
        "mean_precision = calculate_mean_precision(qrel_dict, retrieved_only_docs)\n",
        "mean_recall = calculate_mean_recall(qrel_dict, real_relevant, retrieved_only_docs)\n",
        "avg_p_at_k, p_at_k_dict = calculate_precision_at_k(qrel_dict, retrieved_only_docs, k=10)\n",
        "\n",
        "print(f\"MAP: {map_score:.2f}%\")\n",
        "print(f\"MRR: {mrr_score:.2f}%\")\n",
        "print(f\"Mean Precision: {mean_precision:.2f}%\")\n",
        "print(f\"Mean Recall: {mean_recall:.2f}%\")\n",
        "print(f\"Precision@10: {avg_p_at_k:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15zAIuIlfp2X"
      },
      "source": [
        "#BM25 WITH FACTORS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "YWiGMqwXfpXP",
        "outputId": "c33bd780-f352-4dc1-da88-99a415a72d0c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù†Ø¸ÙØ© Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„.\n",
            "k1    b     MAP      MRR      P@10     MeanPrec   MeanRecall\n",
            "0.5   0.2   66.84    70.52    11.14    11.14      82.98\n",
            "0.5   0.4   69.44    73.32    11.37    11.37      84.42\n",
            "0.5   0.6   70.97    74.95    11.48    11.48      85.27\n",
            "0.5   0.75  71.42    75.43    11.56    11.56      85.78\n",
            "0.5   0.8   71.51    75.51    11.59    11.59      85.90\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/rank_bm25.py:119: RuntimeWarning: invalid value encountered in divide\n",
            "  score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3988061629.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mbm25_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"bm25\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"doc_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"corpus\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         retrieved_docs_dict = {\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mqid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretrieve_bm25_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-3988061629.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         retrieved_docs_dict = {\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mqid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretrieve_bm25_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         }\n",
            "\u001b[0;32m/tmp/ipython-input-3-3988061629.py\u001b[0m in \u001b[0;36mretrieve_bm25_docs\u001b[0;34m(query_tokens, bm25_data, top_k)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mbm25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bm25\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mdoc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"doc_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mranked_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranked_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rank_bm25.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mq_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n\u001b[0m\u001b[1;32m    120\u001b[0m                                                (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm rank_bm25\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª (Ø¹Ø¯Ù„ Ø­Ø³Ø¨ Ù…ÙƒØ§Ù† Ù…Ù„ÙØ§ØªÙƒ)\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "BM25_MODEL_PATH = f\"/content/drive/MyDrive/utils/bm25_model/{DATASET_NAME}_bm25.joblib\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„Ù€lemmatization (ÙƒÙ…Ø§ ÙÙŠ ÙƒÙˆØ¯Ùƒ Ø§Ù„Ø³Ø§Ø¨Ù‚)\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def save_cleaned_docs(df, dataset_name=\"antique\", path_out=CLEANED_TSV_PATH):\n",
        "    \"\"\"\n",
        "    ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ Ù…Ù„Ù TSV Ù…Ø¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:\n",
        "    doc_id, text, processed_text, dataset_name, light_clean_text\n",
        "    \"\"\"\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "\n",
        "        # Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù…ÙŠÙ‚\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "\n",
        "        # Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø®ÙÙŠÙ\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ: {path_out}\")\n",
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "def load_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "    return df\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def build_bm25(corpus, k1, b):\n",
        "    return BM25Okapi(corpus, k1=k1, b=b)\n",
        "\n",
        "def retrieve_bm25_docs(query_tokens, bm25_data, top_k=10):\n",
        "    bm25 = bm25_data[\"bm25\"]\n",
        "    doc_ids = bm25_data[\"doc_ids\"]\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "    results = [(doc_ids[i], scores[i]) for i in ranked_indices]\n",
        "    return results\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, []))\n",
        "        if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall\n",
        "    return np.mean(list(recall_scores.values())) * 100\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "# ==== Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒØ§Ù…Ù„ ====\n",
        "\n",
        "# 1. ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)\n",
        "df = load_dataset(DOCS_PATH)\n",
        "if not os.path.exists(CLEANED_TSV_PATH):\n",
        "    save_cleaned_docs(df, CLEANED_TSV_PATH)\n",
        "else:\n",
        "    print(\"Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù†Ø¸ÙØ© Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„.\")\n",
        "\n",
        "# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©\n",
        "df_cleaned = pd.read_csv(CLEANED_TSV_PATH, sep=\"\\t\")\n",
        "corpus = [doc.split() if isinstance(doc, str) else [] for doc in df_cleaned[\"processed_text\"]]\n",
        "doc_ids = df_cleaned[\"doc_id\"].astype(str).tolist()\n",
        "\n",
        "# 3. ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
        "queries_df = load_queries(QUERIES_PATH)\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# 4. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù€ qrels Ù„Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "qrel_dict, real_relevant = get_qrels(QRELS_PATH)\n",
        "\n",
        "# 5. Ù‚Ø§Ø¦Ù…Ø© Ù‚ÙŠÙ… k1 Ùˆ b Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
        "k1_values = [0.5, 1.0, 1.5, 2.0]\n",
        "b_values = [0.2, 0.4, 0.6, 0.75, 0.8, 1.0]\n",
        "\n",
        "print(f\"{'k1':<5} {'b':<5} {'MAP':<8} {'MRR':<8} {'P@10':<8} {'MeanPrec':<10} {'MeanRecall'}\")\n",
        "\n",
        "# 6. ØªØ¬Ø±Ø¨Ø© ÙƒÙ„ ØªÙˆÙ„ÙŠÙØ© ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª\n",
        "for k1 in k1_values:\n",
        "    for b in b_values:\n",
        "        bm25 = build_bm25(corpus, k1, b)\n",
        "        bm25_data = {\"bm25\": bm25, \"doc_ids\": doc_ids, \"corpus\": corpus}\n",
        "\n",
        "        retrieved_docs_dict = {\n",
        "            qid: retrieve_bm25_docs(tokens, bm25_data, top_k=10)\n",
        "            for qid, tokens in queries_tokens.items()\n",
        "        }\n",
        "\n",
        "        retrieved_only_docs = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "        map_score = calculate_map(qrel_dict, retrieved_only_docs)\n",
        "        mrr_score = calculate_mrr(qrel_dict, retrieved_only_docs)\n",
        "        mean_precision = calculate_mean_precision(qrel_dict, retrieved_only_docs)\n",
        "        mean_recall = calculate_mean_recall(qrel_dict, real_relevant, retrieved_only_docs)\n",
        "        avg_p_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_only_docs, k=10)\n",
        "\n",
        "        print(f\"{k1:<5} {b:<5} {map_score:<8.2f} {mrr_score:<8.2f} {avg_p_at_k:<8.2f} {mean_precision:<10.2f} {mean_recall:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HYBRID"
      ],
      "metadata": {
        "id": "TJrJdp-7mmW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù…\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "embedding_file_path = f\"/content/drive/MyDrive/embedding_model_joblib_file/{DATASET_NAME}_embedding_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "            for word, pos in tokens_pos\n",
        "            if word not in stop_words and len(word) > 1]\n",
        "\n",
        "def clean_text(text):\n",
        "    return tokenize(processing(text))\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ TF-IDF\n",
        "tfidf_data = joblib.load(tfidf_model_path)\n",
        "vectorizer = tfidf_data[\"vectorizer\"]\n",
        "X = tfidf_data[\"vectors\"]\n",
        "tfidf_doc_ids = tfidf_data[\"doc_ids\"]\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embedding ÙˆØ§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª\n",
        "embedding_model_path = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "embedding_model = SentenceTransformer(embedding_model_path)\n",
        "embedding_data = joblib.load(embedding_file_path)\n",
        "doc_embeddings = embedding_data[\"embeddings\"]\n",
        "embedding_doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "def represent_query_tfidf(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def represent_query_embedding(query_text):\n",
        "    return embedding_model.encode([query_text], convert_to_numpy=True)[0]\n",
        "\n",
        "def hybrid_rank(query_tokens, query_text, top_k=10, alpha=0.5):\n",
        "    tfidf_vec = represent_query_tfidf(query_tokens)\n",
        "    embedding_vec = represent_query_embedding(query_text)\n",
        "\n",
        "    tfidf_sims = cosine_similarity(tfidf_vec, X).flatten()\n",
        "    embedding_sims = cosine_similarity([embedding_vec], doc_embeddings).flatten()\n",
        "\n",
        "    hybrid_sims = alpha * tfidf_sims + (1 - alpha) * embedding_sims\n",
        "    top_indices = np.argsort(-hybrid_sims)[:top_k]\n",
        "    return [(tfidf_doc_ids[i], hybrid_sims[i]) for i in top_indices]\n",
        "\n",
        "# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
        "retrieved_docs_dict = {}\n",
        "for qid, tokens in tqdm(queries_tokens.items(), desc=\"Hybrid Retrieval\"):\n",
        "    text = \" \".join(tokens)\n",
        "    results = hybrid_rank(tokens, text, top_k=10, alpha=0.5)\n",
        "    retrieved_docs_dict[qid] = results\n",
        "\n",
        "# Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Ù…Ø«Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚)\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0: relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0: continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, []))\n",
        "        if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(dataset_name=\"quora\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs_str, k=10)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "cal_evaluations(\"quora\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d21bQoff1iDT",
        "outputId": "0f707f05-6dfb-4624-a0cf-d39965e2350d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, nltk, anyascii, textsearch, contractions\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyascii-0.3.3 contractions-0.1.73 nltk-3.8.1 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "Collecting country_converter\n",
            "  Downloading country_converter-1.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Downloading country_converter-1.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: country_converter\n",
            "Successfully installed country_converter-1.3\n",
            "Collecting datefinder\n",
            "  Downloading datefinder-0.7.3-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n",
            "Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: datefinder\n",
            "Successfully installed datefinder-0.7.3\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 4.1.0\n",
            "    Uninstalling sentence-transformers-4.1.0:\n",
            "      Successfully uninstalled sentence-transformers-4.1.0\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "Hybrid Retrieval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [1:23:51<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== Evaluation for dataset: quora ==\n",
            "MAP: 74.16 %\n",
            "MRR: 77.68 %\n",
            "Mean Precision: 12.15 %\n",
            "Mean Recall: 89.22 %\n",
            "Mean Precision@10: 12.15 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMBEDDING WITH VECTOR STORE"
      ],
      "metadata": {
        "id": "erDvaYommggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm faiss-cpu\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import os, re, string, time, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ========\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = \"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/quora_cleaned_docs_beir\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_beir.joblib\")\n",
        "FAISS_INDEX_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding.index\")\n",
        "FAISS_META_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_meta.json\")\n",
        "TOP_K = 50\n",
        "RETRIEVAL_MODE = \"faiss\"  # Ø§Ø®ØªØ± Ø¨ÙŠÙ† \"cosine\" Ø£Ùˆ \"faiss\"\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== ØªÙ†Ø¸ÙŠÙ Ù†ØµÙˆØµ ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ ØªÙˆÙ„ÙŠØ¯ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"ğŸš€ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§ØªØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"ğŸ—‚ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    # Ø­ÙØ¸ metadata Ù„Ù€ FAISS Ù„Ø§Ø­Ù‚Ø§Ù‹\n",
        "    with open(FAISS_META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ÙÙŠ: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def build_and_save_faiss_index(embeddings, index_path, meta_path, doc_ids):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)  # Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙˆØ¹ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    # Ø­ÙØ¸ Ø§Ù„Ù€ metadata (doc_ids)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ: {index_path}\")\n",
        "\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ========\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: {path}\")\n",
        "    df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str)\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "def retrieve_with_faiss(query_embeddings_dict, index_path, doc_ids, top_k=10):\n",
        "    index = faiss.read_index(index_path)\n",
        "    query_ids = list(query_embeddings_dict.keys())\n",
        "    query_embeddings = np.array([query_embeddings_dict[qid] for qid in query_ids]).astype(\"float32\")\n",
        "    scores, indices = index.search(query_embeddings, top_k)\n",
        "\n",
        "    results = {}\n",
        "    for i, qid in enumerate(query_ids):\n",
        "        results[qid] = [(doc_ids[idx], float(scores[i][j])) for j, idx in enumerate(indices[i])]\n",
        "    return results\n",
        "\n",
        "def load_doc_ids_from_meta(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"quora\"):\n",
        "    print(\"\\nğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    build_and_save_faiss_index(embedding_data[\"embeddings\"], FAISS_INDEX_PATH, FAISS_META_PATH, embedding_data[\"doc_ids\"])\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ {len(query_embeddings)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
        "\n",
        "    if RETRIEVAL_MODE == \"cosine\":\n",
        "        print(\"\\nğŸš€ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… cosine_similarity...\")\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    elif RETRIEVAL_MODE == \"faiss\":\n",
        "        print(\"\\nâš¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS...\")\n",
        "        doc_ids = load_doc_ids_from_meta(FAISS_META_PATH)\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_faiss(query_embeddings, FAISS_INDEX_PATH, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    else:\n",
        "        raise ValueError(f\"âŒ Retrieval mode '{RETRIEVAL_MODE}' not supported.\")\n",
        "\n",
        "    print(f\"ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ {len(retrieved_docs)} Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ {round(duration, 2)} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=f\"{DATASET_NAME}-{RETRIEVAL_MODE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3nmstJ-T83D",
        "outputId": "b026225a-33b9-47f9-b769-c93b7250562f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding_beir.joblib\n",
            "âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding.index\n",
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: /content/drive/MyDrive/dataset_quora_dev/queries.tsv\n",
            "ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ 5000 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
            "\n",
            "âš¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS...\n",
            "ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ 5000 Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ 38.29 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\n",
            "\n",
            "== Evaluation for dataset: quora-faiss ==\n",
            "MAP: 84.07 %\n",
            "MRR: 86.57 %\n",
            "Mean Precision: 2.93 %\n",
            "Mean Recall: 98.84 %\n",
            "Mean Precision@10: 13.22 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMBEDDING WITH COSINE"
      ],
      "metadata": {
        "id": "aX1UKvGimZMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm faiss-cpu\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import os, re, string, time, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ========\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = \"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/quora_cleaned_docs_beir\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_beir.joblib\")\n",
        "FAISS_INDEX_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding.index\")\n",
        "FAISS_META_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_meta.json\")\n",
        "TOP_K = 50\n",
        "RETRIEVAL_MODE = \"cosine\"  # Ø§Ø®ØªØ± Ø¨ÙŠÙ† \"cosine\" Ø£Ùˆ \"faiss\"\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== ØªÙ†Ø¸ÙŠÙ Ù†ØµÙˆØµ ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ ØªÙˆÙ„ÙŠØ¯ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"ğŸš€ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§ØªØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"ğŸ—‚ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    # Ø­ÙØ¸ metadata Ù„Ù€ FAISS Ù„Ø§Ø­Ù‚Ø§Ù‹\n",
        "    with open(FAISS_META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ÙÙŠ: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def build_and_save_faiss_index(embeddings, index_path, meta_path, doc_ids):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)  # Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙˆØ¹ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    # Ø­ÙØ¸ Ø§Ù„Ù€ metadata (doc_ids)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ: {index_path}\")\n",
        "\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ========\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: {path}\")\n",
        "    df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str)\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "def retrieve_with_faiss(query_embeddings_dict, index_path, doc_ids, top_k=10):\n",
        "    index = faiss.read_index(index_path)\n",
        "    query_ids = list(query_embeddings_dict.keys())\n",
        "    query_embeddings = np.array([query_embeddings_dict[qid] for qid in query_ids]).astype(\"float32\")\n",
        "    scores, indices = index.search(query_embeddings, top_k)\n",
        "\n",
        "    results = {}\n",
        "    for i, qid in enumerate(query_ids):\n",
        "        results[qid] = [(doc_ids[idx], float(scores[i][j])) for j, idx in enumerate(indices[i])]\n",
        "    return results\n",
        "\n",
        "def load_doc_ids_from_meta(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"quora\"):\n",
        "    print(\"\\nğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    build_and_save_faiss_index(embedding_data[\"embeddings\"], FAISS_INDEX_PATH, FAISS_META_PATH, embedding_data[\"doc_ids\"])\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ {len(query_embeddings)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
        "\n",
        "    if RETRIEVAL_MODE == \"cosine\":\n",
        "        print(\"\\nğŸš€ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… cosine_similarity...\")\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    elif RETRIEVAL_MODE == \"faiss\":\n",
        "        print(\"\\nâš¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS...\")\n",
        "        doc_ids = load_doc_ids_from_meta(FAISS_META_PATH)\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_faiss(query_embeddings, FAISS_INDEX_PATH, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    else:\n",
        "        raise ValueError(f\"âŒ Retrieval mode '{RETRIEVAL_MODE}' not supported.\")\n",
        "\n",
        "    print(f\"ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ {len(retrieved_docs)} Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ {round(duration, 2)} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=f\"{DATASET_NAME}-{RETRIEVAL_MODE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqMEr2QGXeE-",
        "outputId": "494dfaf1-7dff-406a-ffc8-6d49fcb8c6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding_beir.joblib\n",
            "âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding.index\n",
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: /content/drive/MyDrive/dataset_quora_dev/queries.tsv\n",
            "ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ 5000 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
            "\n",
            "ğŸš€ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… cosine_similarity...\n",
            "ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ 5000 Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ 4256.87 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\n",
            "\n",
            "== Evaluation for dataset: quora-cosine ==\n",
            "MAP: 84.06 %\n",
            "MRR: 86.56 %\n",
            "Mean Precision: 2.93 %\n",
            "Mean Recall: 98.84 %\n",
            "Mean Precision@10: 13.21 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù…\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø§Ù†Ù‚Ø¨Ø§Ø¶Ø§Øª (contractions)\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø§Ù†Ù‚Ø¨Ø§Ø¶Ø§Øª\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n",
        "# def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "#     cleaned_data = []\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"):\n",
        "#         doc_id = row[\"doc_id\"]\n",
        "#         raw_text = row[\"text\"]\n",
        "#         processed_tokens = clean_text(raw_text)\n",
        "#         processed_text = \" \".join(processed_tokens)\n",
        "#         light_text = light_clean(raw_text)\n",
        "#         cleaned_data.append({\n",
        "#             \"doc_id\": doc_id,\n",
        "#             \"text\": raw_text,\n",
        "#             \"processed_text\": processed_text,\n",
        "#             \"dataset_name\": dataset_name,\n",
        "#             \"light_clean_text\": light_text\n",
        "#         })\n",
        "#     cleaned_df = pd.DataFrame(cleaned_data)\n",
        "#     cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df)\n",
        "\n",
        "# # Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø¹ÙƒØ³ÙŠ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù†Ø¸ÙØ©\n",
        "# def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "#     df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "#     index = defaultdict(set)\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³\"):\n",
        "#         doc_id = str(row[\"doc_id\"])\n",
        "#         tokens = str(row[\"processed_text\"]).split()\n",
        "#         for token in tokens:\n",
        "#             index[token].add(doc_id)\n",
        "#     os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "#     with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "#         json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "#     return index\n",
        "\n",
        "# inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
        "# def load_and_clean_queries(path):\n",
        "#     data = []\n",
        "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(\"\\t\")\n",
        "#             if len(parts) >= 2:\n",
        "#                 query_id = parts[0]\n",
        "#                 text = \" \".join(parts[1:])\n",
        "#                 data.append((query_id, text))\n",
        "#     df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "#     df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "#     with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "#         for _, row in df.iterrows():\n",
        "#             f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "#     return df\n",
        "\n",
        "# queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§Øª Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¹Ø¨Ø± Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¹ÙƒØ³ÙŠ\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¹ÙƒØ³ÙŠ\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "# ØªØ¬Ù‡ÙŠØ² Ù†Ù…ÙˆØ°Ø¬ TF-IDF\n",
        "tqdm.pandas(desc=\">> ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: ØªØ¶ÙŠÙ Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª Ø¹Ø¨Ø± WordNet Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø©\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def query_expansion(tokens):\n",
        "    expanded_tokens = set(tokens)\n",
        "    for token in tokens:\n",
        "        synsets = wordnet.synsets(token)\n",
        "        for syn in synsets:\n",
        "            for lemma in syn.lemmas():\n",
        "                lemma_name = lemma.name().replace('_', ' ').lower()\n",
        "                if lemma_name not in stop_words and lemma_name.isalpha():\n",
        "                    expanded_tokens.add(lemma_name)\n",
        "    return list(expanded_tokens)\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© ØªØµØ­ÙŠØ­ Ø¨Ø³ÙŠØ·Ø©: Ø­Ø°Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ùˆ ØºÙŠØ± Ø§Ù„Ø£Ø¨Ø¬Ø¯ÙŠØ© (Ù…Ø«Ø§Ù„ ØªÙˆØ¶ÙŠØ­ÙŠ)\n",
        "def correct_tokens(tokens):\n",
        "    corrected = []\n",
        "    for t in tokens:\n",
        "        if t.isalpha() and len(t) > 1:\n",
        "            corrected.append(t)\n",
        "    return corrected\n",
        "\n",
        "# Ø¯ÙˆØ§Ù„ Ù„ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø£Ù†Ø¸Ù…Ø© Ù…Ø®ØªÙ„ÙØ©\n",
        "\n",
        "def process_queries_original(queries_tokens):\n",
        "    # Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØµÙ„ÙŠ Ø¨Ø¯ÙˆÙ† ØªÙˆØ³Ø¹Ø© Ø£Ùˆ ØªØµØ­ÙŠØ­\n",
        "    return queries_tokens\n",
        "\n",
        "def process_queries_with_features(queries_tokens):\n",
        "    # Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØªØµØ­ÙŠØ­ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª\n",
        "    new_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        corrected = correct_tokens(tokens)\n",
        "        expanded = query_expansion(corrected)\n",
        "        new_queries[qid] = expanded\n",
        "    return new_queries\n",
        "\n",
        "# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    if average_precisions:\n",
        "        return np.mean(average_precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    if rr_list:\n",
        "        return np.mean(rr_list) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        if num_retrieved == 0:\n",
        "            precisions.append(0)\n",
        "        else:\n",
        "            precisions.append(num_relevant / num_retrieved)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    if recalls:\n",
        "        return np.mean(recalls) * 100, recalls\n",
        "    return 0, []\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100, precisions\n",
        "    return 0, []\n",
        "\n",
        "# Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ù†Ø¸Ø§Ù… (Ù‚Ø¨Ù„ ÙˆØ¨Ø¹Ø¯)\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"Original\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# === ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ===\n",
        "\n",
        "queries_original = process_queries_original(queries_tokens)\n",
        "queries_with_features = process_queries_with_features(queries_tokens)\n",
        "\n",
        "evaluate_system(queries_original, \"Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØµÙ„ÙŠ (Ø¨Ø¯ÙˆÙ† ØªÙˆØ³Ø¹Ø© Ø£Ùˆ ØªØµØ­ÙŠØ­)\")\n",
        "evaluate_system(queries_with_features, \"Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØªØµØ­ÙŠØ­ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_3PfjSXJiKo",
        "outputId": "e93c1bd8-807e-473f-f584-2f97c331418d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØµÙ„ÙŠ (Ø¨Ø¯ÙˆÙ† ØªÙˆØ³Ø¹Ø© Ø£Ùˆ ØªØµØ­ÙŠØ­): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [17:35<00:00,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ØµÙ„ÙŠ (Ø¨Ø¯ÙˆÙ† ØªÙˆØ³Ø¹Ø© Ø£Ùˆ ØªØµØ­ÙŠØ­) ==\n",
            "MAP: 66.6 %\n",
            "MRR: 70.66 %\n",
            "Mean Precision: 11.03 %\n",
            "Mean Recall: 81.62 %\n",
            "Mean Precision@10: 10.92 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØªØµØ­ÙŠØ­ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [23:06<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØªØµØ­ÙŠØ­ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª ==\n",
            "MAP: 45.0 %\n",
            "MRR: 47.94 %\n",
            "Mean Precision: 8.31 %\n",
            "Mean Recall: 63.39 %\n",
            "Mean Precision@10: 8.21 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù…\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm textblob\n",
        "!python -m textblob.download_corpora\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "from textblob import TextBlob\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "#     cleaned_data = []\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"):\n",
        "#         doc_id = row[\"doc_id\"]\n",
        "#         raw_text = row[\"text\"]\n",
        "#         processed_tokens = clean_text(raw_text)\n",
        "#         processed_text = \" \".join(processed_tokens)\n",
        "#         light_text = light_clean(raw_text)\n",
        "#         cleaned_data.append({\n",
        "#             \"doc_id\": doc_id,\n",
        "#             \"text\": raw_text,\n",
        "#             \"processed_text\": processed_text,\n",
        "#             \"dataset_name\": dataset_name,\n",
        "#             \"light_clean_text\": light_text\n",
        "#         })\n",
        "#     cleaned_df = pd.DataFrame(cleaned_data)\n",
        "#     cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df)\n",
        "\n",
        "# def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "#     df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "#     index = defaultdict(set)\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³\"):\n",
        "#         doc_id = str(row[\"doc_id\"])\n",
        "#         tokens = str(row[\"processed_text\"]).split()\n",
        "#         for token in tokens:\n",
        "#             index[token].add(doc_id)\n",
        "#     os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "#     with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "#         json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "#     return index\n",
        "\n",
        "# inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# === âœ… Ù…ÙŠØ²Ø© ØªØµØ­ÙŠØ­ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙ‚Ø· ===\n",
        "\n",
        "def correct_query_spelling(tokens):\n",
        "    corrected = []\n",
        "    for word in tokens:\n",
        "        blob = TextBlob(word)\n",
        "        correction = str(blob.correct())\n",
        "        corrected.append(correction if correction else word)\n",
        "    return corrected\n",
        "\n",
        "def process_queries_with_correction_only(queries_tokens):\n",
        "    corrected_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        corrected_tokens = correct_query_spelling(tokens)\n",
        "        corrected_queries[qid] = corrected_tokens\n",
        "    return corrected_queries\n",
        "\n",
        "# === âœ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    return np.mean(average_precisions) * 100 if average_precisions else 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    return np.mean(rr_list) * 100 if rr_list else 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        precisions.append(num_relevant / num_retrieved if num_retrieved > 0 else 0)\n",
        "    return np.mean(precisions) * 100 if precisions else 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    return np.mean(recalls) * 100 if recalls else 0, recalls\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    return np.mean(precisions) * 100 if precisions else 0, precisions\n",
        "\n",
        "# === âœ… ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ ÙÙ‚Ø· ===\n",
        "\n",
        "queries_with_correction = process_queries_with_correction_only(queries_tokens)\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"With Spelling Correction Only\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "evaluate_system(queries_with_correction, \"Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ ÙÙ‚Ø·\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtdKyZRPcK9v",
        "outputId": "63a0cb6f-6a93-4301-fa0c-247a9ca17f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "INFO: pip is looking at multiple versions of textblob to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: textblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.19.0\n",
            "    Uninstalling textblob-0.19.0:\n",
            "      Successfully uninstalled textblob-0.19.0\n",
            "Successfully installed textblob-0.18.0.post0\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ ÙÙ‚Ø·: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [18:48<00:00,  4.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ ÙÙ‚Ø· ==\n",
            "MAP: 51.31 %\n",
            "MRR: 54.71 %\n",
            "Mean Precision: 8.91 %\n",
            "Mean Recall: 66.54 %\n",
            "Mean Precision@10: 8.84 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù…\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "#     cleaned_data = []\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"):\n",
        "#         doc_id = row[\"doc_id\"]\n",
        "#         raw_text = row[\"text\"]\n",
        "#         processed_tokens = clean_text(raw_text)\n",
        "#         processed_text = \" \".join(processed_tokens)\n",
        "#         light_text = light_clean(raw_text)\n",
        "#         cleaned_data.append({\n",
        "#             \"doc_id\": doc_id,\n",
        "#             \"text\": raw_text,\n",
        "#             \"processed_text\": processed_text,\n",
        "#             \"dataset_name\": dataset_name,\n",
        "#             \"light_clean_text\": light_text\n",
        "#         })\n",
        "#     cleaned_df = pd.DataFrame(cleaned_data)\n",
        "#     cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df)\n",
        "\n",
        "# def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "#     df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "#     index = defaultdict(set)\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³\"):\n",
        "#         doc_id = str(row[\"doc_id\"])\n",
        "#         tokens = str(row[\"processed_text\"]).split()\n",
        "#         for token in tokens:\n",
        "#             index[token].add(doc_id)\n",
        "#     os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "#     with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "#         json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "#     return index\n",
        "\n",
        "# inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "tqdm.pandas(desc=\">> ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# === âœ… Ù…ÙŠØ²Ø© ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙ‚Ø· ===\n",
        "\n",
        "def query_expansion(tokens):\n",
        "    expanded_tokens = set(tokens)\n",
        "    for token in tokens:\n",
        "        synsets = wordnet.synsets(token)\n",
        "        for syn in synsets:\n",
        "            for lemma in syn.lemmas():\n",
        "                lemma_name = lemma.name().replace('_', ' ').lower()\n",
        "                if lemma_name not in stop_words and lemma_name.isalpha():\n",
        "                    expanded_tokens.add(lemma_name)\n",
        "    return list(expanded_tokens)\n",
        "\n",
        "def process_queries_with_expansion_only(queries_tokens):\n",
        "    new_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        expanded = query_expansion(tokens)\n",
        "        new_queries[qid] = expanded\n",
        "    return new_queries\n",
        "\n",
        "# === âœ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    if average_precisions:\n",
        "        return np.mean(average_precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    if rr_list:\n",
        "        return np.mean(rr_list) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        if num_retrieved == 0:\n",
        "            precisions.append(0)\n",
        "        else:\n",
        "            precisions.append(num_relevant / num_retrieved)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    if recalls:\n",
        "        return np.mean(recalls) * 100, recalls\n",
        "    return 0, []\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100, precisions\n",
        "    return 0, []\n",
        "\n",
        "# === âœ… ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ Ø§Ù„ØªÙˆØ³Ø¹Ø© ÙÙ‚Ø· ===\n",
        "\n",
        "queries_with_expansion = process_queries_with_expansion_only(queries_tokens)\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"With Expansion Only\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "evaluate_system(queries_with_expansion, \"Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙ‚Ø·\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR_QteRZsM0U",
        "outputId": "42afee63-4220-4ebc-b888-9cb91151918d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙ‚Ø·: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [26:46<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙ‚Ø· ==\n",
            "MAP: 46.03 %\n",
            "MRR: 49.04 %\n",
            "Mean Precision: 8.47 %\n",
            "Mean Recall: 64.52 %\n",
            "Mean Precision@10: 8.37 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù…\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm fuzzywuzzy[speedup]\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np, pandas as pd\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "tqdm.pandas(desc=\">> ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# ===== Ø¯Ù…Ø¬ QueryRefiner Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… =====\n",
        "\n",
        "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
        "processed_docs_df = pd.read_csv(CLEANED_TSV_PATH, sep=\"\\t\")\n",
        "\n",
        "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ processed_text\n",
        "all_processed_terms = [\n",
        "    term\n",
        "    for doc in processed_docs_df[\"processed_text\"]\n",
        "    for term in str(doc).split()\n",
        "]\n",
        "\n",
        "term_frequencies = Counter(all_processed_terms)\n",
        "processed_terms_set = set(term_frequencies)\n",
        "\n",
        "class QueryRefiner:\n",
        "    def __init__(self, processed_terms):\n",
        "        self.term_frequencies = Counter(processed_terms)\n",
        "        self.processed_terms = set(processed_terms)\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def reduce_repeated_letters(self, word):\n",
        "        return re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
        "\n",
        "    def suggest_correction(self, query):\n",
        "        words = query.split()\n",
        "        corrected = []\n",
        "        for word in words:\n",
        "            lw = self.reduce_repeated_letters(word.lower())\n",
        "            if lw in self.stop_words or all(c in string.punctuation for c in lw):\n",
        "                corrected.append(word)\n",
        "                continue\n",
        "            best_match = process.extractOne(lw, self.processed_terms, scorer=fuzz.ratio)\n",
        "            if best_match and best_match[1] > 85:\n",
        "                corrected.append(best_match[0])\n",
        "            else:\n",
        "                corrected.append(word)\n",
        "        corrected_query = ' '.join(corrected)\n",
        "        if corrected_query.lower() != query.lower():\n",
        "            return corrected_query\n",
        "        return None\n",
        "\n",
        "    def get_synonyms(self, word):\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                name = lemma.name().lower()\n",
        "                if (\n",
        "                    name != word.lower()\n",
        "                    and name in self.processed_terms\n",
        "                    and len(name) > 2\n",
        "                    and name.isalpha()\n",
        "                ):\n",
        "                    synonyms.add(name)\n",
        "        return list(synonyms)\n",
        "\n",
        "    def expand_query(self, query):\n",
        "        words = query.split()\n",
        "        expanded_terms = set()\n",
        "        for word in words:\n",
        "            corrected_word = self.suggest_correction(word)\n",
        "            base_word = corrected_word if corrected_word is not None else word\n",
        "            base_word = base_word.lower()\n",
        "            if base_word in self.processed_terms:\n",
        "                expanded_terms.add(base_word)\n",
        "                syns = self.get_synonyms(base_word)\n",
        "                expanded_terms.update(syns)\n",
        "        if not expanded_terms:\n",
        "            return query\n",
        "        return ' '.join(expanded_terms)\n",
        "\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† QueryRefiner\n",
        "query_refiner = QueryRefiner(all_processed_terms)\n",
        "\n",
        "# ØªØ¹Ø¯ÙŠÙ„ Ø¯Ø§Ù„Ø© ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„ØªØ³ØªØ®Ø¯Ù… QueryRefiner\n",
        "def process_queries_with_expansion_refiner(queries_tokens):\n",
        "    new_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        original_query = \" \".join(tokens)\n",
        "        expanded_query = query_refiner.expand_query(original_query)\n",
        "        # Ø±Ø¬Ø¹Ù†Ø§ ÙƒÙ€ list Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª\n",
        "        new_queries[qid] = expanded_query.split()\n",
        "    return new_queries\n",
        "\n",
        "# === ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© QueryRefiner ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    if average_precisions:\n",
        "        return np.mean(average_precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    if rr_list:\n",
        "        return np.mean(rr_list) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        if num_retrieved == 0:\n",
        "            precisions.append(0)\n",
        "        else:\n",
        "            precisions.append(num_relevant / num_retrieved)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    if recalls:\n",
        "        return np.mean(recalls) * 100, recalls\n",
        "    return 0, []\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100, precisions\n",
        "    return 0, []\n",
        "\n",
        "# ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… QueryRefiner\n",
        "queries_with_expansion = process_queries_with_expansion_refiner(queries_tokens)\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"With QueryRefiner Expansion\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© QueryRefiner\n",
        "evaluate_system(queries_with_expansion, \"Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© QueryRefiner\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE64xpR5ViUI",
        "outputId": "aa344e23-1bae-4c8c-98c2-bf02321db32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting fuzzywuzzy[speedup]\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting python-levenshtein>=0.12 (from fuzzywuzzy[speedup])\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-levenshtein>=0.12->fuzzywuzzy[speedup])\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-levenshtein>=0.12->fuzzywuzzy[speedup])\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, pyahocorasick, nltk, anyascii, textsearch, Levenshtein, python-levenshtein, contractions\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.27.1 anyascii-0.3.3 contractions-0.1.73 fuzzywuzzy-0.18.0 nltk-3.8.1 pyahocorasick-2.2.0 python-levenshtein-0.27.1 rapidfuzz-3.13.0 textsearch-0.0.24\n",
            "Collecting country_converter\n",
            "  Downloading country_converter-1.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Downloading country_converter-1.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: country_converter\n",
            "Successfully installed country_converter-1.3\n",
            "Collecting datefinder\n",
            "  Downloading datefinder-0.7.3-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n",
            "Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: datefinder\n",
            "Successfully installed datefinder-0.7.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© QueryRefiner: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [26:49<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© QueryRefiner ==\n",
            "MAP: 47.63 %\n",
            "MRR: 50.68 %\n",
            "Mean Precision: 8.65 %\n",
            "Mean Recall: 65.93 %\n",
            "Mean Precision@10: 8.55 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© =====\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm fuzzywuzzy[speedup] symspellpy marisa-trie\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# ===== Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª =====\n",
        "import os, re, string, json, joblib\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np, pandas as pd\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "from fuzzywuzzy import process, fuzz\n",
        "from symspellpy import SymSpell\n",
        "import marisa_trie\n",
        "from functools import lru_cache\n",
        "\n",
        "# ===== ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK =====\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# ===== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª =====\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "tqdm.pandas(desc=\">> ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# ===== Ø¯Ù…Ø¬ QueryRefiner Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… =====\n",
        "\n",
        "# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
        "processed_docs_df = pd.read_csv(CLEANED_TSV_PATH, sep=\"\\t\")\n",
        "\n",
        "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ processed_text\n",
        "all_processed_terms = [\n",
        "    term\n",
        "    for doc in processed_docs_df[\"processed_text\"]\n",
        "    for term in str(doc).split()\n",
        "]\n",
        "\n",
        "term_frequencies = Counter(all_processed_terms)\n",
        "processed_terms_set = set(term_frequencies)\n",
        "\n",
        "# ===== ÙƒÙ„Ø§Ø³ QueryRefiner Ø§Ù„Ø¬Ø¯ÙŠØ¯ =====\n",
        "class QueryRefiner:\n",
        "    def __init__(self, processed_terms):\n",
        "        self.processed_terms = set(processed_terms)\n",
        "        self.term_frequencies = Counter(processed_terms)\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "        # Ø¥Ø¹Ø¯Ø§Ø¯ SymSpell Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©\n",
        "        self.sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "        dictionary_path = os.path.join(\"/content/drive/MyDrive/utils\", \"symspell_data\", \"frequency_dictionary_en_82_765.txt\")\n",
        "        if not self.sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
        "            raise FileNotFoundError(\"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³\")\n",
        "\n",
        "        # Ø¥Ø¯Ø®Ø§Ù„ ÙƒÙ„Ù…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©\n",
        "        self.sym_spell.create_dictionary_entry(\"change\", 5000)\n",
        "        self.sym_spell.create_dictionary_entry(\"environment\", 5000)\n",
        "        self.sym_spell.create_dictionary_entry(\"surroundings\", 3000)\n",
        "\n",
        "        # Ø¥Ø¹Ø¯Ø§Ø¯ Trie Ù„Ù„Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹\n",
        "        self.trie = marisa_trie.Trie(self.processed_terms)\n",
        "\n",
        "    def reduce_repeated_letters(self, word):\n",
        "        return re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
        "\n",
        "    def correct_spelling(self, query: str) -> str:\n",
        "        query = self.reduce_repeated_letters(query.lower())\n",
        "        suggestions = self.sym_spell.lookup_compound(query, max_edit_distance=2)\n",
        "        return suggestions[0].term if suggestions else query\n",
        "\n",
        "    def suggest_correction(self, query: str) -> str:\n",
        "        corrected = self.correct_spelling(query)\n",
        "        return corrected if corrected.lower() != query.lower() else None\n",
        "\n",
        "    @lru_cache(maxsize=10000)\n",
        "    def get_synonyms(self, word, max_synonyms=10):\n",
        "        synonyms = set()\n",
        "        word = word.lower()\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                name = lemma.name().lower()\n",
        "                if name != word and len(name) > 2 and name.isalpha():\n",
        "                    synonyms.add(name.replace(\"_\", \" \"))\n",
        "                    if len(synonyms) >= max_synonyms:\n",
        "                        return list(synonyms)\n",
        "        return list(synonyms)\n",
        "\n",
        "    def expand_query(self, query):\n",
        "        corrected = self.correct_spelling(query)\n",
        "        words = corrected.lower().split()\n",
        "        expanded_terms = set()\n",
        "\n",
        "        for word in words:\n",
        "            if word in self.stop_words:\n",
        "                continue\n",
        "            expanded_terms.add(word)\n",
        "            expanded_terms.update(self.get_synonyms(word))\n",
        "\n",
        "        return \" \".join(expanded_terms) if expanded_terms else query\n",
        "\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† QueryRefiner\n",
        "query_refiner = QueryRefiner(all_processed_terms)\n",
        "\n",
        "def process_queries_with_expansion_refiner(queries_tokens):\n",
        "    new_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        original_query = \" \".join(tokens)\n",
        "        expanded_query = query_refiner.expand_query(original_query)\n",
        "        new_queries[qid] = expanded_query.split()\n",
        "    return new_queries\n",
        "\n",
        "# ===== Ù…Ø«Ø§Ù„ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙˆØ³Ø¹Ø© =====\n",
        "expanded_queries_tokens = process_queries_with_expansion_refiner(queries_tokens)\n",
        "\n",
        "# ===== Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…ÙˆØ³Ø¹ =====\n",
        "for qid, tokens in list(expanded_queries_tokens.items())[:3]:  # Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ 3 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
        "    retrieved = retrieve_docs(tokens, inverted_index)\n",
        "    query_vec = represent_query(tokens)\n",
        "    ranked_docs = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=5)\n",
        "    print(f\"Query ID: {qid}\")\n",
        "    print(f\"Expanded Query Tokens: {tokens}\")\n",
        "    print(\"Top Documents:\", ranked_docs)\n",
        "    print(\"=\"*50)\n",
        "\n",
        "\n",
        "# ===== Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… =====\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    if average_precisions:\n",
        "        return np.mean(average_precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    if rr_list:\n",
        "        return np.mean(rr_list) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        if num_retrieved == 0:\n",
        "            precisions.append(0)\n",
        "        else:\n",
        "            precisions.append(num_relevant / num_retrieved)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    if recalls:\n",
        "        return np.mean(recalls) * 100, recalls\n",
        "    return 0, []\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100, precisions\n",
        "    return 0, []\n",
        "\n",
        "# ===== Ø¯Ø§Ù„Ø© ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… =====\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"With QueryRefiner Expansion\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ===== ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… =====\n",
        "evaluate_system(expanded_queries_tokens, \"Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© QueryRefiner\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7wtCZOcnjWG",
        "outputId": "d0a87e76-eaff-4f23-e332-3736c433c1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.11/dist-packages (6.9.0)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: fuzzywuzzy[speedup] in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-levenshtein>=0.12 in /usr/local/lib/python3.11/dist-packages (from fuzzywuzzy[speedup]) (0.27.1)\n",
            "Requirement already satisfied: editdistpy>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from symspellpy) (0.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from marisa-trie) (75.2.0)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.11/dist-packages (from python-levenshtein>=0.12->fuzzywuzzy[speedup]) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein==0.27.1->python-levenshtein>=0.12->fuzzywuzzy[speedup]) (3.13.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query ID: query_id\n",
            "Expanded Query Tokens: ['schoolbook', 'text', 'textbook']\n",
            "Top Documents: [('152620', np.float64(0.6575043527286554)), ('499049', np.float64(0.626028443010714)), ('12220', np.float64(0.6076405510167578)), ('10830', np.float64(0.6032707625876371)), ('499048', np.float64(0.5939698364225973))]\n",
            "==================================================\n",
            "Query ID: 318\n",
            "Expanded Query Tokens: ['flavor', 'aspect', 'moderator', 'expression', 'look', 'face', 'flavour', 'feel', 'quota', 'feeling', 'tone', 'looking', 'spirit']\n",
            "Top Documents: [('78199', np.float64(0.372801521466937)), ('374620', np.float64(0.3650840664733963)), ('374619', np.float64(0.3650840664733963)), ('130782', np.float64(0.36361559301751706)), ('235521', np.float64(0.358664252208077))]\n",
            "==================================================\n",
            "Query ID: 378\n",
            "Expanded Query Tokens: ['sprightliness', 'spirit', 'choose', 'animation', 'biography', 'affair', 'defy', 'lifetime', 'refuse', 'deny', 'prefer', 'matter', 'life', 'living', 'select', 'liveliness', 'scraps', 'opt', 'reject', 'garbage', 'take', 'unlike', 'dissimilar', 'lifespan', 'different', 'thing', 'resist', 'aliveness', 'decline']\n",
            "Top Documents: [('163894', np.float64(0.29800387566047715)), ('163895', np.float64(0.2684110323076999)), ('497482', np.float64(0.23343770506422953)), ('257197', np.float64(0.23170656285378238)), ('257196', np.float64(0.23170656285378238))]\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© QueryRefiner: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [22:13<00:00,  3.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨ÙˆØ§Ø³Ø·Ø© QueryRefiner ==\n",
            "MAP: 40.28 %\n",
            "MRR: 43.12 %\n",
            "Mean Precision: 7.56 %\n",
            "Mean Recall: 58.08 %\n",
            "Mean Precision@10: 7.53 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9gD5VYSmnodc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}