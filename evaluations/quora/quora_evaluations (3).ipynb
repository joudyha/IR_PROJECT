{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfhyxWokDTbH",
        "outputId": "f982a1cd-2443-46d2-9370-5f5691e092e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRD39YW9fJuT"
      },
      "source": [
        "TFIDF_EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-mxxr7PBXxh",
        "outputId": "c860660c-90e7-4ac2-ee55-f51bdc55eb0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, nltk, anyascii, textsearch, contractions\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyascii-0.3.3 contractions-0.1.73 nltk-3.8.1 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "Collecting country_converter\n",
            "  Downloading country_converter-1.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Downloading country_converter-1.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: country_converter\n",
            "Successfully installed country_converter-1.3\n",
            "Collecting datefinder\n",
            "  Downloading datefinder-0.7.3-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n",
            "Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: datefinder\n",
            "Successfully installed datefinder-0.7.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "🧼 تنظيف وتخزين المستندات: 100%|██████████| 522770/522770 [09:26<00:00, 922.42it/s]\n",
            "🔧 بناء الفهرس: 100%|██████████| 522770/522770 [00:32<00:00, 16055.12it/s]\n",
            "🔍 استرجاع وترتيب المستندات: 100%|██████████| 5000/5000 [17:29<00:00,  4.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== Evaluation for dataset: quora ==\n",
            "MAP: 66.82 %\n",
            "MRR: 70.9 %\n",
            "Mean Precision: 11.05 %\n",
            "Mean Recall: 81.78 %\n",
            "Mean Precision@10: 10.94 %\n"
          ]
        }
      ],
      "source": [
        "# تثبيت الحزم\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "# تحميل موارد NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🧼 تنظيف وتخزين المستندات\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH)\n",
        "\n",
        "\n",
        "\n",
        "def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "    df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "    index = defaultdict(set)\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🔧 بناء الفهرس\"):\n",
        "        doc_id = str(row[\"doc_id\"])\n",
        "        tokens = str(row[\"processed_text\"]).split()\n",
        "        for token in tokens:\n",
        "            index[token].add(doc_id)\n",
        "    os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "    with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "    return index\n",
        "\n",
        "inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "    return df\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "\n",
        "tqdm.pandas(desc=\">> تجهيز النصوص\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "\n",
        "retrieved_docs_dict = {}\n",
        "for i, (qid, tokens) in enumerate(tqdm(queries_tokens.items(), desc=\"🔍 استرجاع وترتيب المستندات\"), start=1):\n",
        "    retrieved = retrieve_docs(tokens, inverted_index)\n",
        "    query_vec = represent_query(tokens)\n",
        "    ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "\n",
        "def cal_evaluations(dataset_name=\"quora\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs_dict).items()}\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "cal_evaluations(\"quora\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZGCu4VvfO_l"
      },
      "source": [
        "EMBEDDING_EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bly7eQk98Kg",
        "outputId": "3904689a-ab0b-4953-d1f4-51f93993926f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 تحميل التمثيلات من: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding_beir.joblib\n",
            "📥 تحميل الاستعلامات من: /content/drive/MyDrive/dataset_quora_dev/queries.tsv\n",
            "🔎 عدد الاستعلامات بعد التنظيف: 5000\n",
            "🧪 عينة استعلامات:\n",
            "  query_id                                               text  \\\n",
            "0      318                How does Quora look to a moderator?   \n",
            "1      378  How do I refuse to chose between different thi...   \n",
            "2      379  Did Ben Affleck shine more than Christian Bale...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0                 how does quora look to a moderator  \n",
            "1  how do i refuse to chose between different thi...  \n",
            "2  did ben affleck shine more than christian bale...  \n",
            "🧠 تم تمثيل 5000 استعلام\n",
            "📥 تم استرجاع نتائج لـ 5000 استعلام\n",
            "\n",
            "📊 بدء التقييم...\n",
            "📌 استعلامات للتقييم: 5000\n",
            "🔎 عدد استعلامات qrels: 5000\n",
            "🔎 عدد استعلامات الاسترجاع: 5000\n",
            "🔗 عدد الاستعلامات المشتركة بين qrels والاسترجاع: 5000\n",
            "❗ عدد المستندات المشار لها في qrels لكنها غير موجودة في التمثيل: 0\n",
            "\n",
            "== Evaluation for dataset: quora ==\n",
            "MAP: 84.06 %\n",
            "MRR: 86.56 %\n",
            "Mean Precision: 2.93 %\n",
            "Mean Recall: 98.84 %\n",
            "Mean Precision@10: 13.21 %\n"
          ]
        }
      ],
      "source": [
        "import os, re, string, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ======== تحميل موارد NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== إعداد المسارات ========\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = \"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/quora_cleaned_docs_beir\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_beir.joblib\")\n",
        "TOP_K = 50\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== الدوال المساعدة للنصوص ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== تحميل المستندات وتمثيلها ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"📦 تحميل التمثيلات من: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"🚀 لم يتم العثور على التمثيلات، جاري التوليد...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"🗂️ عدد المستندات المحملة: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        print(\"⚠️ عمود 'light_clean_text' غير موجود — سيتم توليده.\")\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    print(\"🧪 عينة من المستندات المنظفة:\")\n",
        "    print(df_docs[[\"doc_id\", \"light_clean_text\"]].head())\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    print(f\"✅ تم حفظ التمثيلات في: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"📥 تحميل الاستعلامات من: {path}\")\n",
        "    df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str)\n",
        "    print(f\"🔎 عدد الاستعلامات بعد التنظيف: {len(df)}\")\n",
        "\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(\"🧪 عينة استعلامات:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ======== تمثيل الاستعلامات ========\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== الاسترجاع ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "# ======== تحميل QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== التقييم ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "# ======== التقييم النهائي ========\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"quora\"):\n",
        "    print(\"\\n📊 بدء التقييم...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "    print(f\"📌 استعلامات للتقييم: {len(retrieved_docs_str)}\")\n",
        "    print(\"🔎 عدد استعلامات qrels:\", len(qrel_dict))\n",
        "    print(\"🔎 عدد استعلامات الاسترجاع:\", len(retrieved_docs_str))\n",
        "    print(\"🔗 عدد الاستعلامات المشتركة بين qrels والاسترجاع:\", len(set(qrel_dict.keys()) & set(retrieved_docs_str.keys())))\n",
        "\n",
        "# ⛔ تحليل المستندات المفقودة في التمثيل\n",
        "    all_qrels_doc_ids = {docid for rels in qrel_dict.values() for docid in rels}\n",
        "    missing_doc_ids = all_qrels_doc_ids - set(doc_ids)\n",
        "    print(f\"❗ عدد المستندات المشار لها في qrels لكنها غير موجودة في التمثيل: {len(missing_doc_ids)}\")\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== تشغيل كامل ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"🧠 تم تمثيل {len(query_embeddings)} استعلام\")\n",
        "\n",
        "    retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "    print(f\"📥 تم استرجاع نتائج لـ {len(retrieved_docs)} استعلام\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=\"quora\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BM25"
      ],
      "metadata": {
        "id": "Zc-gDdFskeQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkUhcXv0byyX",
        "outputId": "6b2f7816-5a1c-4d4c-deb1-a023dba0ce21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "تم بناء وحفظ نموذج BM25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع المستندات باستخدام BM25: 100%|██████████| 5000/5000 [1:08:36<00:00,  1.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP: 70.65%\n",
            "MRR: 74.75%\n",
            "Mean Precision: 11.51%\n",
            "Mean Recall: 85.50%\n",
            "Precision@10: 11.51%\n"
          ]
        }
      ],
      "source": [
        "# تثبيت الحزم (تحتاج مرة واحدة فقط)\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm rank_bm25\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm import tqdm\n",
        "\n",
        "# تحميل موارد NLTK (مرة واحدة)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات كما في كودك\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/quora_cleaned_docs_beir\"\n",
        "BM25_MODEL_PATH = f\"/content/drive/MyDrive/utils/bm25_model/{DATASET_NAME}_bm25.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🧼 تنظيف وتخزين المستندات\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "# === بناء نموذج BM25 ===\n",
        "def build_bm25_model(cleaned_docs_tsv=CLEANED_TSV_PATH, save_path=BM25_MODEL_PATH):\n",
        "    df = pd.read_csv(cleaned_docs_tsv, sep=\"\\t\")\n",
        "    df = df[df['processed_text'].notna() & (df['processed_text'].str.strip() != \"\")]\n",
        "\n",
        "    corpus = [doc.split() for doc in df[\"processed_text\"]]\n",
        "    doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "\n",
        "    bm25 = BM25Okapi(corpus)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    joblib.dump({\"bm25\": bm25, \"doc_ids\": doc_ids, \"corpus\": corpus}, save_path)\n",
        "    print(\"تم بناء وحفظ نموذج BM25\")\n",
        "\n",
        "def load_bm25_model(path=BM25_MODEL_PATH):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(\"نموذج BM25 غير موجود، قم ببنائه أولاً.\")\n",
        "    return joblib.load(path)\n",
        "\n",
        "def clean_and_tokenize_query(raw_query):\n",
        "    return clean_text(raw_query)\n",
        "\n",
        "def retrieve_bm25_docs(query_tokens, bm25_data, top_k=10):\n",
        "    bm25 = bm25_data[\"bm25\"]\n",
        "    doc_ids = bm25_data[\"doc_ids\"]\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "    results = [(doc_ids[i], scores[i]) for i in ranked_indices]\n",
        "    return results\n",
        "\n",
        "# === تحميل وتنظيف البيانات ===\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH)\n",
        "\n",
        "# === بناء نموذج BM25 ===\n",
        "build_bm25_model(CLEANED_TSV_PATH, BM25_MODEL_PATH)\n",
        "\n",
        "# === تحميل واستدعاء النموذج ===\n",
        "bm25_data = load_bm25_model(BM25_MODEL_PATH)\n",
        "\n",
        "# === تحميل وتنظيف الاستعلامات ===\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "    return df\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# === استرجاع المستندات باستخدام BM25 ===\n",
        "retrieved_docs_dict = {}\n",
        "for qid, tokens in tqdm(queries_tokens.items(), desc=\"🔍 استرجاع المستندات باستخدام BM25\"):\n",
        "    ranked = retrieve_bm25_docs(tokens, bm25_data, top_k=10)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "# === دوال التقييم نفسها من كودك الأصلي ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall\n",
        "    return np.mean(list(recall_scores.values())) * 100\n",
        "\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "# === تحميل الـ qrels ===\n",
        "qrel_dict, real_relevant = get_qrels(QRELS_PATH)\n",
        "\n",
        "# === استرجاع المستندات بدون الدرجات فقط ===\n",
        "retrieved_only_docs = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "# === تقييم الأداء ===\n",
        "map_score = calculate_map(qrel_dict, retrieved_only_docs)\n",
        "mrr_score = calculate_mrr(qrel_dict, retrieved_only_docs)\n",
        "mean_precision = calculate_mean_precision(qrel_dict, retrieved_only_docs)\n",
        "mean_recall = calculate_mean_recall(qrel_dict, real_relevant, retrieved_only_docs)\n",
        "avg_p_at_k, p_at_k_dict = calculate_precision_at_k(qrel_dict, retrieved_only_docs, k=10)\n",
        "\n",
        "print(f\"MAP: {map_score:.2f}%\")\n",
        "print(f\"MRR: {mrr_score:.2f}%\")\n",
        "print(f\"Mean Precision: {mean_precision:.2f}%\")\n",
        "print(f\"Mean Recall: {mean_recall:.2f}%\")\n",
        "print(f\"Precision@10: {avg_p_at_k:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15zAIuIlfp2X"
      },
      "source": [
        "#BM25 WITH FACTORS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "YWiGMqwXfpXP",
        "outputId": "c33bd780-f352-4dc1-da88-99a415a72d0c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ملف المستندات المنظفة موجود بالفعل.\n",
            "k1    b     MAP      MRR      P@10     MeanPrec   MeanRecall\n",
            "0.5   0.2   66.84    70.52    11.14    11.14      82.98\n",
            "0.5   0.4   69.44    73.32    11.37    11.37      84.42\n",
            "0.5   0.6   70.97    74.95    11.48    11.48      85.27\n",
            "0.5   0.75  71.42    75.43    11.56    11.56      85.78\n",
            "0.5   0.8   71.51    75.51    11.59    11.59      85.90\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/rank_bm25.py:119: RuntimeWarning: invalid value encountered in divide\n",
            "  score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3988061629.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mbm25_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"bm25\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"doc_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"corpus\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         retrieved_docs_dict = {\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mqid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretrieve_bm25_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-3988061629.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         retrieved_docs_dict = {\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mqid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretrieve_bm25_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         }\n",
            "\u001b[0;32m/tmp/ipython-input-3-3988061629.py\u001b[0m in \u001b[0;36mretrieve_bm25_docs\u001b[0;34m(query_tokens, bm25_data, top_k)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mbm25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bm25\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mdoc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"doc_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0mranked_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranked_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/rank_bm25.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mq_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_freqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n\u001b[0m\u001b[1;32m    120\u001b[0m                                                (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# تثبيت الحزم (مرة واحدة فقط)\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm rank_bm25\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm import tqdm\n",
        "\n",
        "# تحميل موارد NLTK (مرة واحدة)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# تعريف المسارات (عدل حسب مكان ملفاتك)\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "BM25_MODEL_PATH = f\"/content/drive/MyDrive/utils/bm25_model/{DATASET_NAME}_bm25.joblib\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# الدوال المساعدة للتنظيف والـlemmatization (كما في كودك السابق)\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def save_cleaned_docs(df, dataset_name=\"antique\", path_out=CLEANED_TSV_PATH):\n",
        "    \"\"\"\n",
        "    تنظيف المستندات وتخزينها في ملف TSV مع الأعمدة التالية:\n",
        "    doc_id, text, processed_text, dataset_name, light_clean_text\n",
        "    \"\"\"\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🧼 تنظيف وتخزين المستندات\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "\n",
        "        # التنظيف العميق\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "\n",
        "        # التنظيف الخفيف\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "    print(f\"✅ تم حفظ الملف في: {path_out}\")\n",
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "def load_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "    return df\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def build_bm25(corpus, k1, b):\n",
        "    return BM25Okapi(corpus, k1=k1, b=b)\n",
        "\n",
        "def retrieve_bm25_docs(query_tokens, bm25_data, top_k=10):\n",
        "    bm25 = bm25_data[\"bm25\"]\n",
        "    doc_ids = bm25_data[\"doc_ids\"]\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "    results = [(doc_ids[i], scores[i]) for i in ranked_indices]\n",
        "    return results\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, []))\n",
        "        if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall\n",
        "    return np.mean(list(recall_scores.values())) * 100\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "# ==== التنفيذ الكامل ====\n",
        "\n",
        "# 1. تحميل وتنظيف المستندات (مرة واحدة فقط)\n",
        "df = load_dataset(DOCS_PATH)\n",
        "if not os.path.exists(CLEANED_TSV_PATH):\n",
        "    save_cleaned_docs(df, CLEANED_TSV_PATH)\n",
        "else:\n",
        "    print(\"ملف المستندات المنظفة موجود بالفعل.\")\n",
        "\n",
        "# 2. تحميل البيانات النظيفة\n",
        "df_cleaned = pd.read_csv(CLEANED_TSV_PATH, sep=\"\\t\")\n",
        "corpus = [doc.split() if isinstance(doc, str) else [] for doc in df_cleaned[\"processed_text\"]]\n",
        "doc_ids = df_cleaned[\"doc_id\"].astype(str).tolist()\n",
        "\n",
        "# 3. تحميل وتنظيف الاستعلامات\n",
        "queries_df = load_queries(QUERIES_PATH)\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# 4. تحميل الـ qrels للتقييم\n",
        "qrel_dict, real_relevant = get_qrels(QRELS_PATH)\n",
        "\n",
        "# 5. قائمة قيم k1 و b للاختبار\n",
        "k1_values = [0.5, 1.0, 1.5, 2.0]\n",
        "b_values = [0.2, 0.4, 0.6, 0.75, 0.8, 1.0]\n",
        "\n",
        "print(f\"{'k1':<5} {'b':<5} {'MAP':<8} {'MRR':<8} {'P@10':<8} {'MeanPrec':<10} {'MeanRecall'}\")\n",
        "\n",
        "# 6. تجربة كل توليفة وحساب التقييمات\n",
        "for k1 in k1_values:\n",
        "    for b in b_values:\n",
        "        bm25 = build_bm25(corpus, k1, b)\n",
        "        bm25_data = {\"bm25\": bm25, \"doc_ids\": doc_ids, \"corpus\": corpus}\n",
        "\n",
        "        retrieved_docs_dict = {\n",
        "            qid: retrieve_bm25_docs(tokens, bm25_data, top_k=10)\n",
        "            for qid, tokens in queries_tokens.items()\n",
        "        }\n",
        "\n",
        "        retrieved_only_docs = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "        map_score = calculate_map(qrel_dict, retrieved_only_docs)\n",
        "        mrr_score = calculate_mrr(qrel_dict, retrieved_only_docs)\n",
        "        mean_precision = calculate_mean_precision(qrel_dict, retrieved_only_docs)\n",
        "        mean_recall = calculate_mean_recall(qrel_dict, real_relevant, retrieved_only_docs)\n",
        "        avg_p_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_only_docs, k=10)\n",
        "\n",
        "        print(f\"{k1:<5} {b:<5} {map_score:<8.2f} {mrr_score:<8.2f} {avg_p_at_k:<8.2f} {mean_precision:<10.2f} {mean_recall:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HYBRID"
      ],
      "metadata": {
        "id": "TJrJdp-7mmW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت الحزم\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "# تحميل موارد NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "embedding_file_path = f\"/content/drive/MyDrive/embedding_model_joblib_file/{DATASET_NAME}_embedding_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "            for word, pos in tokens_pos\n",
        "            if word not in stop_words and len(word) > 1]\n",
        "\n",
        "def clean_text(text):\n",
        "    return tokenize(processing(text))\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "# تحميل بيانات الاستعلامات\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# تحميل نموذج TF-IDF\n",
        "tfidf_data = joblib.load(tfidf_model_path)\n",
        "vectorizer = tfidf_data[\"vectorizer\"]\n",
        "X = tfidf_data[\"vectors\"]\n",
        "tfidf_doc_ids = tfidf_data[\"doc_ids\"]\n",
        "\n",
        "# تحميل نموذج Embedding والتمثيلات\n",
        "embedding_model_path = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "embedding_model = SentenceTransformer(embedding_model_path)\n",
        "embedding_data = joblib.load(embedding_file_path)\n",
        "doc_embeddings = embedding_data[\"embeddings\"]\n",
        "embedding_doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "def represent_query_tfidf(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def represent_query_embedding(query_text):\n",
        "    return embedding_model.encode([query_text], convert_to_numpy=True)[0]\n",
        "\n",
        "def hybrid_rank(query_tokens, query_text, top_k=10, alpha=0.5):\n",
        "    tfidf_vec = represent_query_tfidf(query_tokens)\n",
        "    embedding_vec = represent_query_embedding(query_text)\n",
        "\n",
        "    tfidf_sims = cosine_similarity(tfidf_vec, X).flatten()\n",
        "    embedding_sims = cosine_similarity([embedding_vec], doc_embeddings).flatten()\n",
        "\n",
        "    hybrid_sims = alpha * tfidf_sims + (1 - alpha) * embedding_sims\n",
        "    top_indices = np.argsort(-hybrid_sims)[:top_k]\n",
        "    return [(tfidf_doc_ids[i], hybrid_sims[i]) for i in top_indices]\n",
        "\n",
        "# تطبيق الاسترجاع لجميع الاستعلامات\n",
        "retrieved_docs_dict = {}\n",
        "for qid, tokens in tqdm(queries_tokens.items(), desc=\"Hybrid Retrieval\"):\n",
        "    text = \" \".join(tokens)\n",
        "    results = hybrid_rank(tokens, text, top_k=10, alpha=0.5)\n",
        "    retrieved_docs_dict[qid] = results\n",
        "\n",
        "# التقييم (مثل السابق)\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0: relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0: continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, []))\n",
        "        if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(dataset_name=\"quora\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs_str, k=10)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# استدعاء التقييم\n",
        "cal_evaluations(\"quora\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d21bQoff1iDT",
        "outputId": "0f707f05-6dfb-4624-a0cf-d39965e2350d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, nltk, anyascii, textsearch, contractions\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyascii-0.3.3 contractions-0.1.73 nltk-3.8.1 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "Collecting country_converter\n",
            "  Downloading country_converter-1.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Downloading country_converter-1.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: country_converter\n",
            "Successfully installed country_converter-1.3\n",
            "Collecting datefinder\n",
            "  Downloading datefinder-0.7.3-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n",
            "Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: datefinder\n",
            "Successfully installed datefinder-0.7.3\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 4.1.0\n",
            "    Uninstalling sentence-transformers-4.1.0:\n",
            "      Successfully uninstalled sentence-transformers-4.1.0\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "Hybrid Retrieval: 100%|██████████| 5000/5000 [1:23:51<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== Evaluation for dataset: quora ==\n",
            "MAP: 74.16 %\n",
            "MRR: 77.68 %\n",
            "Mean Precision: 12.15 %\n",
            "Mean Recall: 89.22 %\n",
            "Mean Precision@10: 12.15 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMBEDDING WITH VECTOR STORE"
      ],
      "metadata": {
        "id": "erDvaYommggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm faiss-cpu\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import os, re, string, time, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ======== تحميل موارد NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== إعداد المسارات ========\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = \"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/quora_cleaned_docs_beir\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_beir.joblib\")\n",
        "FAISS_INDEX_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding.index\")\n",
        "FAISS_META_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_meta.json\")\n",
        "TOP_K = 50\n",
        "RETRIEVAL_MODE = \"faiss\"  # اختر بين \"cosine\" أو \"faiss\"\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== تنظيف نصوص ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== تحميل أو توليد تمثيلات المستندات ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"📦 تحميل التمثيلات من: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"🚀 لم يتم العثور على التمثيلات، جاري التوليد...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"🗂️ عدد المستندات المحملة: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    # حفظ metadata لـ FAISS لاحقاً\n",
        "    with open(FAISS_META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"✅ تم حفظ التمثيلات في: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def build_and_save_faiss_index(embeddings, index_path, meta_path, doc_ids):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)  # استخدم نوع الفهرس المناسب\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    # حفظ الـ metadata (doc_ids)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"✅ تم بناء وحفظ فهرس FAISS في: {index_path}\")\n",
        "\n",
        "\n",
        "# ======== تحميل الاستعلامات ========\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"📥 تحميل الاستعلامات من: {path}\")\n",
        "    df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str)\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== استرجاع ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "def retrieve_with_faiss(query_embeddings_dict, index_path, doc_ids, top_k=10):\n",
        "    index = faiss.read_index(index_path)\n",
        "    query_ids = list(query_embeddings_dict.keys())\n",
        "    query_embeddings = np.array([query_embeddings_dict[qid] for qid in query_ids]).astype(\"float32\")\n",
        "    scores, indices = index.search(query_embeddings, top_k)\n",
        "\n",
        "    results = {}\n",
        "    for i, qid in enumerate(query_ids):\n",
        "        results[qid] = [(doc_ids[idx], float(scores[i][j])) for j, idx in enumerate(indices[i])]\n",
        "    return results\n",
        "\n",
        "def load_doc_ids_from_meta(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ======== تحميل QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== التقييم ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"quora\"):\n",
        "    print(\"\\n📊 بدء التقييم...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== التشغيل الكامل ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    build_and_save_faiss_index(embedding_data[\"embeddings\"], FAISS_INDEX_PATH, FAISS_META_PATH, embedding_data[\"doc_ids\"])\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"🧠 تم تمثيل {len(query_embeddings)} استعلام\")\n",
        "\n",
        "    if RETRIEVAL_MODE == \"cosine\":\n",
        "        print(\"\\n🚀 الاسترجاع باستخدام cosine_similarity...\")\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    elif RETRIEVAL_MODE == \"faiss\":\n",
        "        print(\"\\n⚡ الاسترجاع باستخدام FAISS...\")\n",
        "        doc_ids = load_doc_ids_from_meta(FAISS_META_PATH)\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_faiss(query_embeddings, FAISS_INDEX_PATH, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    else:\n",
        "        raise ValueError(f\"❌ Retrieval mode '{RETRIEVAL_MODE}' not supported.\")\n",
        "\n",
        "    print(f\"📥 تم استرجاع نتائج لـ {len(retrieved_docs)} استعلام في {round(duration, 2)} ثانية\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=f\"{DATASET_NAME}-{RETRIEVAL_MODE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3nmstJ-T83D",
        "outputId": "b026225a-33b9-47f9-b769-c93b7250562f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 تحميل التمثيلات من: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding_beir.joblib\n",
            "✅ تم بناء وحفظ فهرس FAISS في: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding.index\n",
            "📥 تحميل الاستعلامات من: /content/drive/MyDrive/dataset_quora_dev/queries.tsv\n",
            "🧠 تم تمثيل 5000 استعلام\n",
            "\n",
            "⚡ الاسترجاع باستخدام FAISS...\n",
            "📥 تم استرجاع نتائج لـ 5000 استعلام في 38.29 ثانية\n",
            "\n",
            "📊 بدء التقييم...\n",
            "\n",
            "== Evaluation for dataset: quora-faiss ==\n",
            "MAP: 84.07 %\n",
            "MRR: 86.57 %\n",
            "Mean Precision: 2.93 %\n",
            "Mean Recall: 98.84 %\n",
            "Mean Precision@10: 13.22 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMBEDDING WITH COSINE"
      ],
      "metadata": {
        "id": "aX1UKvGimZMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm faiss-cpu\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import os, re, string, time, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ======== تحميل موارد NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== إعداد المسارات ========\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = \"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/quora_cleaned_docs_beir\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_beir.joblib\")\n",
        "FAISS_INDEX_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding.index\")\n",
        "FAISS_META_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_meta.json\")\n",
        "TOP_K = 50\n",
        "RETRIEVAL_MODE = \"cosine\"  # اختر بين \"cosine\" أو \"faiss\"\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== تنظيف نصوص ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== تحميل أو توليد تمثيلات المستندات ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"📦 تحميل التمثيلات من: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"🚀 لم يتم العثور على التمثيلات، جاري التوليد...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"🗂️ عدد المستندات المحملة: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    # حفظ metadata لـ FAISS لاحقاً\n",
        "    with open(FAISS_META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"✅ تم حفظ التمثيلات في: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def build_and_save_faiss_index(embeddings, index_path, meta_path, doc_ids):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)  # استخدم نوع الفهرس المناسب\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    # حفظ الـ metadata (doc_ids)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"✅ تم بناء وحفظ فهرس FAISS في: {index_path}\")\n",
        "\n",
        "\n",
        "# ======== تحميل الاستعلامات ========\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"📥 تحميل الاستعلامات من: {path}\")\n",
        "    df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str)\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== استرجاع ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "def retrieve_with_faiss(query_embeddings_dict, index_path, doc_ids, top_k=10):\n",
        "    index = faiss.read_index(index_path)\n",
        "    query_ids = list(query_embeddings_dict.keys())\n",
        "    query_embeddings = np.array([query_embeddings_dict[qid] for qid in query_ids]).astype(\"float32\")\n",
        "    scores, indices = index.search(query_embeddings, top_k)\n",
        "\n",
        "    results = {}\n",
        "    for i, qid in enumerate(query_ids):\n",
        "        results[qid] = [(doc_ids[idx], float(scores[i][j])) for j, idx in enumerate(indices[i])]\n",
        "    return results\n",
        "\n",
        "def load_doc_ids_from_meta(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ======== تحميل QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== التقييم ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"quora\"):\n",
        "    print(\"\\n📊 بدء التقييم...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== التشغيل الكامل ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    build_and_save_faiss_index(embedding_data[\"embeddings\"], FAISS_INDEX_PATH, FAISS_META_PATH, embedding_data[\"doc_ids\"])\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"🧠 تم تمثيل {len(query_embeddings)} استعلام\")\n",
        "\n",
        "    if RETRIEVAL_MODE == \"cosine\":\n",
        "        print(\"\\n🚀 الاسترجاع باستخدام cosine_similarity...\")\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    elif RETRIEVAL_MODE == \"faiss\":\n",
        "        print(\"\\n⚡ الاسترجاع باستخدام FAISS...\")\n",
        "        doc_ids = load_doc_ids_from_meta(FAISS_META_PATH)\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_faiss(query_embeddings, FAISS_INDEX_PATH, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    else:\n",
        "        raise ValueError(f\"❌ Retrieval mode '{RETRIEVAL_MODE}' not supported.\")\n",
        "\n",
        "    print(f\"📥 تم استرجاع نتائج لـ {len(retrieved_docs)} استعلام في {round(duration, 2)} ثانية\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=f\"{DATASET_NAME}-{RETRIEVAL_MODE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqMEr2QGXeE-",
        "outputId": "494dfaf1-7dff-406a-ffc8-6d49fcb8c6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 تحميل التمثيلات من: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding_beir.joblib\n",
            "✅ تم بناء وحفظ فهرس FAISS في: /content/drive/MyDrive/embedding_model_joblib_file/quora_embedding.index\n",
            "📥 تحميل الاستعلامات من: /content/drive/MyDrive/dataset_quora_dev/queries.tsv\n",
            "🧠 تم تمثيل 5000 استعلام\n",
            "\n",
            "🚀 الاسترجاع باستخدام cosine_similarity...\n",
            "📥 تم استرجاع نتائج لـ 5000 استعلام في 4256.87 ثانية\n",
            "\n",
            "📊 بدء التقييم...\n",
            "\n",
            "== Evaluation for dataset: quora-cosine ==\n",
            "MAP: 84.06 %\n",
            "MRR: 86.56 %\n",
            "Mean Precision: 2.93 %\n",
            "Mean Recall: 98.84 %\n",
            "Mean Precision@10: 13.21 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت الحزم\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "\n",
        "# تحميل موارد NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "# دالة توسيع الانقباضات (contractions)\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# دالة تنظيف النصوص مع توسيع الانقباضات\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# # تحميل البيانات وتخزينها بعد التنظيف\n",
        "# def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "#     cleaned_data = []\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🧼 تنظيف وتخزين المستندات\"):\n",
        "#         doc_id = row[\"doc_id\"]\n",
        "#         raw_text = row[\"text\"]\n",
        "#         processed_tokens = clean_text(raw_text)\n",
        "#         processed_text = \" \".join(processed_tokens)\n",
        "#         light_text = light_clean(raw_text)\n",
        "#         cleaned_data.append({\n",
        "#             \"doc_id\": doc_id,\n",
        "#             \"text\": raw_text,\n",
        "#             \"processed_text\": processed_text,\n",
        "#             \"dataset_name\": dataset_name,\n",
        "#             \"light_clean_text\": light_text\n",
        "#         })\n",
        "#     cleaned_df = pd.DataFrame(cleaned_data)\n",
        "#     cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df)\n",
        "\n",
        "# # بناء فهرس عكسي من المستندات المنظفة\n",
        "# def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "#     df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "#     index = defaultdict(set)\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🔧 بناء الفهرس\"):\n",
        "#         doc_id = str(row[\"doc_id\"])\n",
        "#         tokens = str(row[\"processed_text\"]).split()\n",
        "#         for token in tokens:\n",
        "#             index[token].add(doc_id)\n",
        "#     os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "#     with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "#         json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "#     return index\n",
        "\n",
        "# inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "# تحميل وتنظيف الاستعلامات\n",
        "# def load_and_clean_queries(path):\n",
        "#     data = []\n",
        "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(\"\\t\")\n",
        "#             if len(parts) >= 2:\n",
        "#                 query_id = parts[0]\n",
        "#                 text = \" \".join(parts[1:])\n",
        "#                 data.append((query_id, text))\n",
        "#     df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "#     df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "#     with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "#         for _, row in df.iterrows():\n",
        "#             f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "#     return df\n",
        "\n",
        "# queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "# تحميل التوكينات للاستعلامات\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# دالة استرجاع المستندات عبر الفهرس العكسي\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "# تحميل الفهرس العكسي\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "# تجهيز نموذج TF-IDF\n",
        "tqdm.pandas(desc=\">> تجهيز النصوص\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# دالة توسعة الاستعلام: تضيف المرادفات عبر WordNet لكل كلمة\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def query_expansion(tokens):\n",
        "    expanded_tokens = set(tokens)\n",
        "    for token in tokens:\n",
        "        synsets = wordnet.synsets(token)\n",
        "        for syn in synsets:\n",
        "            for lemma in syn.lemmas():\n",
        "                lemma_name = lemma.name().replace('_', ' ').lower()\n",
        "                if lemma_name not in stop_words and lemma_name.isalpha():\n",
        "                    expanded_tokens.add(lemma_name)\n",
        "    return list(expanded_tokens)\n",
        "\n",
        "# دالة تصحيح بسيطة: حذف الكلمات غير الإنجليزية أو غير الأبجدية (مثال توضيحي)\n",
        "def correct_tokens(tokens):\n",
        "    corrected = []\n",
        "    for t in tokens:\n",
        "        if t.isalpha() and len(t) > 1:\n",
        "            corrected.append(t)\n",
        "    return corrected\n",
        "\n",
        "# دوال لتجهيز الاستعلامات بأنظمة مختلفة\n",
        "\n",
        "def process_queries_original(queries_tokens):\n",
        "    # النظام الأصلي بدون توسعة أو تصحيح\n",
        "    return queries_tokens\n",
        "\n",
        "def process_queries_with_features(queries_tokens):\n",
        "    # النظام مع توسعة الاستعلام وتصحيح التوكنات\n",
        "    new_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        corrected = correct_tokens(tokens)\n",
        "        expanded = query_expansion(corrected)\n",
        "        new_queries[qid] = expanded\n",
        "    return new_queries\n",
        "\n",
        "# دوال التقييم\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    if average_precisions:\n",
        "        return np.mean(average_precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    if rr_list:\n",
        "        return np.mean(rr_list) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        if num_retrieved == 0:\n",
        "            precisions.append(0)\n",
        "        else:\n",
        "            precisions.append(num_relevant / num_retrieved)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    if recalls:\n",
        "        return np.mean(recalls) * 100, recalls\n",
        "    return 0, []\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100, precisions\n",
        "    return 0, []\n",
        "\n",
        "# دالة التقييم العامة للنظام (قبل وبعد)\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"Original\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"استخراج المستندات للنظام: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== تقييم النظام: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# === تشغيل المقارنة ===\n",
        "\n",
        "queries_original = process_queries_original(queries_tokens)\n",
        "queries_with_features = process_queries_with_features(queries_tokens)\n",
        "\n",
        "evaluate_system(queries_original, \"النظام الأصلي (بدون توسعة أو تصحيح)\")\n",
        "evaluate_system(queries_with_features, \"النظام مع توسعة الاستعلام وتصحيح التوكنات\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_3PfjSXJiKo",
        "outputId": "e93c1bd8-807e-473f-f584-2f97c331418d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "استخراج المستندات للنظام: النظام الأصلي (بدون توسعة أو تصحيح): 100%|██████████| 5000/5000 [17:35<00:00,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== تقييم النظام: النظام الأصلي (بدون توسعة أو تصحيح) ==\n",
            "MAP: 66.6 %\n",
            "MRR: 70.66 %\n",
            "Mean Precision: 11.03 %\n",
            "Mean Recall: 81.62 %\n",
            "Mean Precision@10: 10.92 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "استخراج المستندات للنظام: النظام مع توسعة الاستعلام وتصحيح التوكنات: 100%|██████████| 5000/5000 [23:06<00:00,  3.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== تقييم النظام: النظام مع توسعة الاستعلام وتصحيح التوكنات ==\n",
            "MAP: 45.0 %\n",
            "MRR: 47.94 %\n",
            "Mean Precision: 8.31 %\n",
            "Mean Recall: 63.39 %\n",
            "Mean Precision@10: 8.21 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت الحزم\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm textblob\n",
        "!python -m textblob.download_corpora\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "from textblob import TextBlob\n",
        "\n",
        "# تحميل موارد NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "#     cleaned_data = []\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🧼 تنظيف وتخزين المستندات\"):\n",
        "#         doc_id = row[\"doc_id\"]\n",
        "#         raw_text = row[\"text\"]\n",
        "#         processed_tokens = clean_text(raw_text)\n",
        "#         processed_text = \" \".join(processed_tokens)\n",
        "#         light_text = light_clean(raw_text)\n",
        "#         cleaned_data.append({\n",
        "#             \"doc_id\": doc_id,\n",
        "#             \"text\": raw_text,\n",
        "#             \"processed_text\": processed_text,\n",
        "#             \"dataset_name\": dataset_name,\n",
        "#             \"light_clean_text\": light_text\n",
        "#         })\n",
        "#     cleaned_df = pd.DataFrame(cleaned_data)\n",
        "#     cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df)\n",
        "\n",
        "# def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "#     df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "#     index = defaultdict(set)\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🔧 بناء الفهرس\"):\n",
        "#         doc_id = str(row[\"doc_id\"])\n",
        "#         tokens = str(row[\"processed_text\"]).split()\n",
        "#         for token in tokens:\n",
        "#             index[token].add(doc_id)\n",
        "#     os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "#     with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "#         json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "#     return index\n",
        "\n",
        "# inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# === ✅ ميزة تصحيح الاستعلام فقط ===\n",
        "\n",
        "def correct_query_spelling(tokens):\n",
        "    corrected = []\n",
        "    for word in tokens:\n",
        "        blob = TextBlob(word)\n",
        "        correction = str(blob.correct())\n",
        "        corrected.append(correction if correction else word)\n",
        "    return corrected\n",
        "\n",
        "def process_queries_with_correction_only(queries_tokens):\n",
        "    corrected_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        corrected_tokens = correct_query_spelling(tokens)\n",
        "        corrected_queries[qid] = corrected_tokens\n",
        "    return corrected_queries\n",
        "\n",
        "# === ✅ التقييم ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    return np.mean(average_precisions) * 100 if average_precisions else 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    return np.mean(rr_list) * 100 if rr_list else 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        precisions.append(num_relevant / num_retrieved if num_retrieved > 0 else 0)\n",
        "    return np.mean(precisions) * 100 if precisions else 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    return np.mean(recalls) * 100 if recalls else 0, recalls\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    return np.mean(precisions) * 100 if precisions else 0, precisions\n",
        "\n",
        "# === ✅ تشغيل النظام مع التصحيح فقط ===\n",
        "\n",
        "queries_with_correction = process_queries_with_correction_only(queries_tokens)\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"With Spelling Correction Only\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"استخراج المستندات للنظام: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== تقييم النظام: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# تشغيل التقييم\n",
        "evaluate_system(queries_with_correction, \"النظام مع التصحيح فقط\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtdKyZRPcK9v",
        "outputId": "63a0cb6f-6a93-4301-fa0c-247a9ca17f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "INFO: pip is looking at multiple versions of textblob to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: textblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.19.0\n",
            "    Uninstalling textblob-0.19.0:\n",
            "      Successfully uninstalled textblob-0.19.0\n",
            "Successfully installed textblob-0.18.0.post0\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "استخراج المستندات للنظام: النظام مع التصحيح فقط: 100%|██████████| 5000/5000 [18:48<00:00,  4.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== تقييم النظام: النظام مع التصحيح فقط ==\n",
            "MAP: 51.31 %\n",
            "MRR: 54.71 %\n",
            "Mean Precision: 8.91 %\n",
            "Mean Recall: 66.54 %\n",
            "Mean Precision@10: 8.84 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت الحزم\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "\n",
        "# تحميل موارد NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "#     cleaned_data = []\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🧼 تنظيف وتخزين المستندات\"):\n",
        "#         doc_id = row[\"doc_id\"]\n",
        "#         raw_text = row[\"text\"]\n",
        "#         processed_tokens = clean_text(raw_text)\n",
        "#         processed_text = \" \".join(processed_tokens)\n",
        "#         light_text = light_clean(raw_text)\n",
        "#         cleaned_data.append({\n",
        "#             \"doc_id\": doc_id,\n",
        "#             \"text\": raw_text,\n",
        "#             \"processed_text\": processed_text,\n",
        "#             \"dataset_name\": dataset_name,\n",
        "#             \"light_clean_text\": light_text\n",
        "#         })\n",
        "#     cleaned_df = pd.DataFrame(cleaned_data)\n",
        "#     cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df)\n",
        "\n",
        "# def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "#     df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "#     index = defaultdict(set)\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🔧 بناء الفهرس\"):\n",
        "#         doc_id = str(row[\"doc_id\"])\n",
        "#         tokens = str(row[\"processed_text\"]).split()\n",
        "#         for token in tokens:\n",
        "#             index[token].add(doc_id)\n",
        "#     os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "#     with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "#         json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "#     return index\n",
        "\n",
        "# inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "tqdm.pandas(desc=\">> تجهيز النصوص\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# === ✅ ميزة توسعة الاستعلام فقط ===\n",
        "\n",
        "def query_expansion(tokens):\n",
        "    expanded_tokens = set(tokens)\n",
        "    for token in tokens:\n",
        "        synsets = wordnet.synsets(token)\n",
        "        for syn in synsets:\n",
        "            for lemma in syn.lemmas():\n",
        "                lemma_name = lemma.name().replace('_', ' ').lower()\n",
        "                if lemma_name not in stop_words and lemma_name.isalpha():\n",
        "                    expanded_tokens.add(lemma_name)\n",
        "    return list(expanded_tokens)\n",
        "\n",
        "def process_queries_with_expansion_only(queries_tokens):\n",
        "    new_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        expanded = query_expansion(tokens)\n",
        "        new_queries[qid] = expanded\n",
        "    return new_queries\n",
        "\n",
        "# === ✅ التقييم ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    if average_precisions:\n",
        "        return np.mean(average_precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    if rr_list:\n",
        "        return np.mean(rr_list) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        if num_retrieved == 0:\n",
        "            precisions.append(0)\n",
        "        else:\n",
        "            precisions.append(num_relevant / num_retrieved)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    if recalls:\n",
        "        return np.mean(recalls) * 100, recalls\n",
        "    return 0, []\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100, precisions\n",
        "    return 0, []\n",
        "\n",
        "# === ✅ تشغيل النظام مع التوسعة فقط ===\n",
        "\n",
        "queries_with_expansion = process_queries_with_expansion_only(queries_tokens)\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"With Expansion Only\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"استخراج المستندات للنظام: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== تقييم النظام: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# تشغيل التقييم\n",
        "evaluate_system(queries_with_expansion, \"النظام مع توسعة الاستعلام فقط\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR_QteRZsM0U",
        "outputId": "42afee63-4220-4ebc-b888-9cb91151918d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "استخراج المستندات للنظام: النظام مع توسعة الاستعلام فقط: 100%|██████████| 5000/5000 [26:46<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== تقييم النظام: النظام مع توسعة الاستعلام فقط ==\n",
            "MAP: 46.03 %\n",
            "MRR: 49.04 %\n",
            "Mean Precision: 8.47 %\n",
            "Mean Recall: 64.52 %\n",
            "Mean Precision@10: 8.37 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت الحزم\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm fuzzywuzzy[speedup]\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np, pandas as pd\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "from fuzzywuzzy import process, fuzz\n",
        "\n",
        "# تحميل موارد NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "tqdm.pandas(desc=\">> تجهيز النصوص\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# ===== دمج QueryRefiner مع توسعة الاستعلام =====\n",
        "\n",
        "# قراءة المستندات المعالجة\n",
        "processed_docs_df = pd.read_csv(CLEANED_TSV_PATH, sep=\"\\t\")\n",
        "\n",
        "# استخراج جميع المصطلحات من العمود processed_text\n",
        "all_processed_terms = [\n",
        "    term\n",
        "    for doc in processed_docs_df[\"processed_text\"]\n",
        "    for term in str(doc).split()\n",
        "]\n",
        "\n",
        "term_frequencies = Counter(all_processed_terms)\n",
        "processed_terms_set = set(term_frequencies)\n",
        "\n",
        "class QueryRefiner:\n",
        "    def __init__(self, processed_terms):\n",
        "        self.term_frequencies = Counter(processed_terms)\n",
        "        self.processed_terms = set(processed_terms)\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def reduce_repeated_letters(self, word):\n",
        "        return re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
        "\n",
        "    def suggest_correction(self, query):\n",
        "        words = query.split()\n",
        "        corrected = []\n",
        "        for word in words:\n",
        "            lw = self.reduce_repeated_letters(word.lower())\n",
        "            if lw in self.stop_words or all(c in string.punctuation for c in lw):\n",
        "                corrected.append(word)\n",
        "                continue\n",
        "            best_match = process.extractOne(lw, self.processed_terms, scorer=fuzz.ratio)\n",
        "            if best_match and best_match[1] > 85:\n",
        "                corrected.append(best_match[0])\n",
        "            else:\n",
        "                corrected.append(word)\n",
        "        corrected_query = ' '.join(corrected)\n",
        "        if corrected_query.lower() != query.lower():\n",
        "            return corrected_query\n",
        "        return None\n",
        "\n",
        "    def get_synonyms(self, word):\n",
        "        synonyms = set()\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                name = lemma.name().lower()\n",
        "                if (\n",
        "                    name != word.lower()\n",
        "                    and name in self.processed_terms\n",
        "                    and len(name) > 2\n",
        "                    and name.isalpha()\n",
        "                ):\n",
        "                    synonyms.add(name)\n",
        "        return list(synonyms)\n",
        "\n",
        "    def expand_query(self, query):\n",
        "        words = query.split()\n",
        "        expanded_terms = set()\n",
        "        for word in words:\n",
        "            corrected_word = self.suggest_correction(word)\n",
        "            base_word = corrected_word if corrected_word is not None else word\n",
        "            base_word = base_word.lower()\n",
        "            if base_word in self.processed_terms:\n",
        "                expanded_terms.add(base_word)\n",
        "                syns = self.get_synonyms(base_word)\n",
        "                expanded_terms.update(syns)\n",
        "        if not expanded_terms:\n",
        "            return query\n",
        "        return ' '.join(expanded_terms)\n",
        "\n",
        "# إنشاء كائن QueryRefiner\n",
        "query_refiner = QueryRefiner(all_processed_terms)\n",
        "\n",
        "# تعديل دالة توسعة الاستعلام لتستخدم QueryRefiner\n",
        "def process_queries_with_expansion_refiner(queries_tokens):\n",
        "    new_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        original_query = \" \".join(tokens)\n",
        "        expanded_query = query_refiner.expand_query(original_query)\n",
        "        # رجعنا كـ list من الكلمات\n",
        "        new_queries[qid] = expanded_query.split()\n",
        "    return new_queries\n",
        "\n",
        "# === تقييم النظام مع توسعة الاستعلام بواسطة QueryRefiner ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    if average_precisions:\n",
        "        return np.mean(average_precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    if rr_list:\n",
        "        return np.mean(rr_list) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        if num_retrieved == 0:\n",
        "            precisions.append(0)\n",
        "        else:\n",
        "            precisions.append(num_relevant / num_retrieved)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    if recalls:\n",
        "        return np.mean(recalls) * 100, recalls\n",
        "    return 0, []\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100, precisions\n",
        "    return 0, []\n",
        "\n",
        "# تشغيل النظام مع توسعة الاستعلام باستخدام QueryRefiner\n",
        "queries_with_expansion = process_queries_with_expansion_refiner(queries_tokens)\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"With QueryRefiner Expansion\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"استخراج المستندات للنظام: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== تقييم النظام: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# تقييم النظام مع توسعة الاستعلام بواسطة QueryRefiner\n",
        "evaluate_system(queries_with_expansion, \"النظام مع توسعة الاستعلام بواسطة QueryRefiner\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE64xpR5ViUI",
        "outputId": "aa344e23-1bae-4c8c-98c2-bf02321db32a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting fuzzywuzzy[speedup]\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting python-levenshtein>=0.12 (from fuzzywuzzy[speedup])\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-levenshtein>=0.12->fuzzywuzzy[speedup])\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-levenshtein>=0.12->fuzzywuzzy[speedup])\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, pyahocorasick, nltk, anyascii, textsearch, Levenshtein, python-levenshtein, contractions\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.27.1 anyascii-0.3.3 contractions-0.1.73 fuzzywuzzy-0.18.0 nltk-3.8.1 pyahocorasick-2.2.0 python-levenshtein-0.27.1 rapidfuzz-3.13.0 textsearch-0.0.24\n",
            "Collecting country_converter\n",
            "  Downloading country_converter-1.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Downloading country_converter-1.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: country_converter\n",
            "Successfully installed country_converter-1.3\n",
            "Collecting datefinder\n",
            "  Downloading datefinder-0.7.3-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n",
            "Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: datefinder\n",
            "Successfully installed datefinder-0.7.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "استخراج المستندات للنظام: النظام مع توسعة الاستعلام بواسطة QueryRefiner: 100%|██████████| 5000/5000 [26:49<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== تقييم النظام: النظام مع توسعة الاستعلام بواسطة QueryRefiner ==\n",
            "MAP: 47.63 %\n",
            "MRR: 50.68 %\n",
            "Mean Precision: 8.65 %\n",
            "Mean Recall: 65.93 %\n",
            "Mean Precision@10: 8.55 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== تثبيت الحزم المطلوبة =====\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm fuzzywuzzy[speedup] symspellpy marisa-trie\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# ===== استيراد المكتبات =====\n",
        "import os, re, string, json, joblib\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np, pandas as pd\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import contractions\n",
        "from fuzzywuzzy import process, fuzz\n",
        "from symspellpy import SymSpell\n",
        "import marisa_trie\n",
        "from functools import lru_cache\n",
        "\n",
        "# ===== تحميل موارد NLTK =====\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# ===== إعداد المسارات =====\n",
        "DATASET_NAME = \"quora\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/dataset_quora_dev/\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_beir.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_beir.tsv\"\n",
        "CLEANED_TSV_PATH = f\"/content/drive/MyDrive/utils/clean_docs/{DATASET_NAME}_cleaned_docs_beir.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_beir.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessed = processing(text)\n",
        "    tokenized = tokenize(preprocessed)\n",
        "    return tokenized\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "tqdm.pandas(desc=\">> تجهيز النصوص\")\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        preprocessor=processing,\n",
        "        tokenizer=tokenize,\n",
        "        lowercase=False,\n",
        "        token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "# ===== دمج QueryRefiner مع توسعة الاستعلام =====\n",
        "\n",
        "# قراءة المستندات المعالجة\n",
        "processed_docs_df = pd.read_csv(CLEANED_TSV_PATH, sep=\"\\t\")\n",
        "\n",
        "# استخراج جميع المصطلحات من العمود processed_text\n",
        "all_processed_terms = [\n",
        "    term\n",
        "    for doc in processed_docs_df[\"processed_text\"]\n",
        "    for term in str(doc).split()\n",
        "]\n",
        "\n",
        "term_frequencies = Counter(all_processed_terms)\n",
        "processed_terms_set = set(term_frequencies)\n",
        "\n",
        "# ===== كلاس QueryRefiner الجديد =====\n",
        "class QueryRefiner:\n",
        "    def __init__(self, processed_terms):\n",
        "        self.processed_terms = set(processed_terms)\n",
        "        self.term_frequencies = Counter(processed_terms)\n",
        "        self.stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "        # إعداد SymSpell مرة واحدة\n",
        "        self.sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "        dictionary_path = os.path.join(\"/content/drive/MyDrive/utils\", \"symspell_data\", \"frequency_dictionary_en_82_765.txt\")\n",
        "        if not self.sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
        "            raise FileNotFoundError(\"فشل تحميل القاموس\")\n",
        "\n",
        "        # إدخال كلمات إضافية عند الحاجة\n",
        "        self.sym_spell.create_dictionary_entry(\"change\", 5000)\n",
        "        self.sym_spell.create_dictionary_entry(\"environment\", 5000)\n",
        "        self.sym_spell.create_dictionary_entry(\"surroundings\", 3000)\n",
        "\n",
        "        # إعداد Trie للإكمال التلقائي السريع\n",
        "        self.trie = marisa_trie.Trie(self.processed_terms)\n",
        "\n",
        "    def reduce_repeated_letters(self, word):\n",
        "        return re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
        "\n",
        "    def correct_spelling(self, query: str) -> str:\n",
        "        query = self.reduce_repeated_letters(query.lower())\n",
        "        suggestions = self.sym_spell.lookup_compound(query, max_edit_distance=2)\n",
        "        return suggestions[0].term if suggestions else query\n",
        "\n",
        "    def suggest_correction(self, query: str) -> str:\n",
        "        corrected = self.correct_spelling(query)\n",
        "        return corrected if corrected.lower() != query.lower() else None\n",
        "\n",
        "    @lru_cache(maxsize=10000)\n",
        "    def get_synonyms(self, word, max_synonyms=10):\n",
        "        synonyms = set()\n",
        "        word = word.lower()\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                name = lemma.name().lower()\n",
        "                if name != word and len(name) > 2 and name.isalpha():\n",
        "                    synonyms.add(name.replace(\"_\", \" \"))\n",
        "                    if len(synonyms) >= max_synonyms:\n",
        "                        return list(synonyms)\n",
        "        return list(synonyms)\n",
        "\n",
        "    def expand_query(self, query):\n",
        "        corrected = self.correct_spelling(query)\n",
        "        words = corrected.lower().split()\n",
        "        expanded_terms = set()\n",
        "\n",
        "        for word in words:\n",
        "            if word in self.stop_words:\n",
        "                continue\n",
        "            expanded_terms.add(word)\n",
        "            expanded_terms.update(self.get_synonyms(word))\n",
        "\n",
        "        return \" \".join(expanded_terms) if expanded_terms else query\n",
        "\n",
        "# إنشاء كائن QueryRefiner\n",
        "query_refiner = QueryRefiner(all_processed_terms)\n",
        "\n",
        "def process_queries_with_expansion_refiner(queries_tokens):\n",
        "    new_queries = {}\n",
        "    for qid, tokens in queries_tokens.items():\n",
        "        original_query = \" \".join(tokens)\n",
        "        expanded_query = query_refiner.expand_query(original_query)\n",
        "        new_queries[qid] = expanded_query.split()\n",
        "    return new_queries\n",
        "\n",
        "# ===== مثال لاستخدام التوسعة =====\n",
        "expanded_queries_tokens = process_queries_with_expansion_refiner(queries_tokens)\n",
        "\n",
        "# ===== استدعاء استرجاع وترتيب المستندات مع الاستعلام الموسع =====\n",
        "for qid, tokens in list(expanded_queries_tokens.items())[:3]:  # على سبيل المثال 3 استعلامات\n",
        "    retrieved = retrieve_docs(tokens, inverted_index)\n",
        "    query_vec = represent_query(tokens)\n",
        "    ranked_docs = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=5)\n",
        "    print(f\"Query ID: {qid}\")\n",
        "    print(f\"Expanded Query Tokens: {tokens}\")\n",
        "    print(\"Top Documents:\", ranked_docs)\n",
        "    print(\"=\"*50)\n",
        "\n",
        "\n",
        "# ===== دوال التقييم =====\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"])\n",
        "        docid = str(row[\"doc_id\"])\n",
        "        rel = row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrel, retrieved):\n",
        "    average_precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if not relevant_docs:\n",
        "            continue\n",
        "        num_relevant = 0\n",
        "        precision_sum = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                num_relevant += 1\n",
        "                precision_sum += num_relevant / i\n",
        "        if num_relevant > 0:\n",
        "            average_precisions.append(precision_sum / len(relevant_docs))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    if average_precisions:\n",
        "        return np.mean(average_precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mrr(qrel, retrieved):\n",
        "    rr_list = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        rr = 0\n",
        "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
        "            if doc_id in relevant_docs:\n",
        "                rr = 1 / i\n",
        "                break\n",
        "        rr_list.append(rr)\n",
        "    if rr_list:\n",
        "        return np.mean(rr_list) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_precision(qrel, retrieved):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        num_retrieved = len(retrieved_docs)\n",
        "        if num_retrieved == 0:\n",
        "            precisions.append(0)\n",
        "        else:\n",
        "            precisions.append(num_relevant / num_retrieved)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100\n",
        "    return 0\n",
        "\n",
        "def calculate_mean_recall(qrel, real_relevant, retrieved):\n",
        "    recalls = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = real_relevant.get(qid, set())\n",
        "        retrieved_docs = retrieved.get(qid, [])\n",
        "        if len(relevant_docs) == 0:\n",
        "            recalls.append(0)\n",
        "            continue\n",
        "        retrieved_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        recalls.append(retrieved_relevant / len(relevant_docs))\n",
        "    if recalls:\n",
        "        return np.mean(recalls) * 100, recalls\n",
        "    return 0, []\n",
        "\n",
        "def calculate_precision_at_k(qrel, retrieved, k=10):\n",
        "    precisions = []\n",
        "    for qid in qrel:\n",
        "        relevant_docs = {docid for docid, rel in qrel[qid].items() if rel > 0}\n",
        "        retrieved_docs = retrieved.get(qid, [])[:k]\n",
        "        num_relevant = sum(1 for d in retrieved_docs if d in relevant_docs)\n",
        "        precisions.append(num_relevant / k)\n",
        "    if precisions:\n",
        "        return np.mean(precisions) * 100, precisions\n",
        "    return 0, []\n",
        "\n",
        "# ===== دالة تقييم النظام مع توسعة الاستعلام =====\n",
        "\n",
        "def evaluate_system(queries_tokens, system_name=\"With QueryRefiner Expansion\"):\n",
        "    retrieved_docs_dict = {}\n",
        "    for qid, tokens in tqdm(queries_tokens.items(), desc=f\"استخراج المستندات للنظام: {system_name}\"):\n",
        "        retrieved = retrieve_docs(tokens, inverted_index)\n",
        "        query_vec = represent_query(tokens)\n",
        "        ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "        retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== تقييم النظام: {system_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ===== تشغيل التقييم =====\n",
        "evaluate_system(expanded_queries_tokens, \"النظام مع توسعة الاستعلام بواسطة QueryRefiner\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7wtCZOcnjWG",
        "outputId": "d0a87e76-eaff-4f23-e332-3736c433c1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.11/dist-packages (6.9.0)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: fuzzywuzzy[speedup] in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-levenshtein>=0.12 in /usr/local/lib/python3.11/dist-packages (from fuzzywuzzy[speedup]) (0.27.1)\n",
            "Requirement already satisfied: editdistpy>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from symspellpy) (0.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from marisa-trie) (75.2.0)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.11/dist-packages (from python-levenshtein>=0.12->fuzzywuzzy[speedup]) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein==0.27.1->python-levenshtein>=0.12->fuzzywuzzy[speedup]) (3.13.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query ID: query_id\n",
            "Expanded Query Tokens: ['schoolbook', 'text', 'textbook']\n",
            "Top Documents: [('152620', np.float64(0.6575043527286554)), ('499049', np.float64(0.626028443010714)), ('12220', np.float64(0.6076405510167578)), ('10830', np.float64(0.6032707625876371)), ('499048', np.float64(0.5939698364225973))]\n",
            "==================================================\n",
            "Query ID: 318\n",
            "Expanded Query Tokens: ['flavor', 'aspect', 'moderator', 'expression', 'look', 'face', 'flavour', 'feel', 'quota', 'feeling', 'tone', 'looking', 'spirit']\n",
            "Top Documents: [('78199', np.float64(0.372801521466937)), ('374620', np.float64(0.3650840664733963)), ('374619', np.float64(0.3650840664733963)), ('130782', np.float64(0.36361559301751706)), ('235521', np.float64(0.358664252208077))]\n",
            "==================================================\n",
            "Query ID: 378\n",
            "Expanded Query Tokens: ['sprightliness', 'spirit', 'choose', 'animation', 'biography', 'affair', 'defy', 'lifetime', 'refuse', 'deny', 'prefer', 'matter', 'life', 'living', 'select', 'liveliness', 'scraps', 'opt', 'reject', 'garbage', 'take', 'unlike', 'dissimilar', 'lifespan', 'different', 'thing', 'resist', 'aliveness', 'decline']\n",
            "Top Documents: [('163894', np.float64(0.29800387566047715)), ('163895', np.float64(0.2684110323076999)), ('497482', np.float64(0.23343770506422953)), ('257197', np.float64(0.23170656285378238)), ('257196', np.float64(0.23170656285378238))]\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "استخراج المستندات للنظام: النظام مع توسعة الاستعلام بواسطة QueryRefiner: 100%|██████████| 5000/5000 [22:13<00:00,  3.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== تقييم النظام: النظام مع توسعة الاستعلام بواسطة QueryRefiner ==\n",
            "MAP: 40.28 %\n",
            "MRR: 43.12 %\n",
            "Mean Precision: 7.56 %\n",
            "Mean Recall: 58.08 %\n",
            "Mean Precision@10: 7.53 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9gD5VYSmnodc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}