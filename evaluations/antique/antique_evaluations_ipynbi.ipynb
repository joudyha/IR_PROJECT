{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwFK1eDqYvF2",
        "outputId": "d9ba37a7-cd48-469a-8ae9-94b1aa1e734f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx6Trdl_j0eb"
      },
      "source": [
        "#TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owTArWQr7r3q",
        "outputId": "68111c4f-c314-417e-8801-d9f21674555d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "🧼 تنظيف وتخزين المستندات: 100%|██████████| 403458/403458 [26:16<00:00, 255.93it/s]\n",
            "🔧 بناء الفهرس: 100%|██████████| 400862/400862 [00:41<00:00, 9578.45it/s] \n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "🔍 استرجاع وترتيب المستندات: 100%|██████████| 200/200 [01:12<00:00,  2.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== Evaluation for dataset: antique ==\n",
            "MAP: 10.27 %\n",
            "MRR: 76.37 %\n",
            "Mean Precision: 40.05 %\n",
            "Mean Recall: 12.84 %\n",
            "Mean Precision@10: 38.4 %\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# تثبيت الحزم\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import datefinder\n",
        "import country_converter as coco\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# تحميل موارد NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_antique9.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_antique9.tsv\"\n",
        "CLEANED_TSV_PATH = \"cleaned_docs_antique9.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_antique9.joblib\"\n",
        "Json_file = os.path.join(BASE_PATH, \"qas.search.json\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=\"cleaned_docs_antique9.tsv\"):\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🧼 تنظيف وتخزين المستندات\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "\n",
        "        if not processed_tokens:  # تجاهل المستندات التي أصبحت فارغة\n",
        "            continue\n",
        "\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=\"cleaned_docs_antique9.tsv\")\n",
        "\n",
        "def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "    df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "    index = defaultdict(set)\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🔧 بناء الفهرس\"):\n",
        "        doc_id = str(row[\"doc_id\"])\n",
        "        tokens = str(row[\"processed_text\"]).split()\n",
        "        for token in tokens:\n",
        "            index[token].add(doc_id)\n",
        "    os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "    with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "    return index\n",
        "\n",
        "inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "# def load_and_clean_queries(path):\n",
        "#     data = []\n",
        "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(\"\\t\")\n",
        "#             if len(parts) >= 2:\n",
        "#                 query_id = parts[0]\n",
        "#                 text = \" \".join(parts[1:])\n",
        "#                 data.append((query_id, text))\n",
        "#     df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "#     df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "#     with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "#         for _, row in df.iterrows():\n",
        "#             f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "#     return df\n",
        "\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        next(f)  # تخطي السطر الأول (الرأس)\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\", maxsplit=1)\n",
        "            if len(parts) == 2:\n",
        "                query_id = parts[0]\n",
        "                text = parts[1].replace('\\t', ' ')  # لو كان النص فيه كلمات مفصولة بـ tab\n",
        "                data.append((query_id, text))\n",
        "\n",
        "    # إنشاء DataFrame\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "\n",
        "    # تنظيف النص\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "    # حفظ النتائج إلى ملف جديد\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "tqdm.pandas(desc=\">> تجهيز النصوص\")\n",
        "\n",
        "def custom_preprocessor(text):\n",
        "    return processing(text)\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    return tokenize(text)\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "    lowercase=False,\n",
        "    preprocessor=custom_preprocessor,\n",
        "    tokenizer=custom_tokenizer,\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=100):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "retrieved_docs_dict = {}\n",
        "for i, (qid, tokens) in enumerate(tqdm(queries_tokens.items(), desc=\"🔍 استرجاع وترتيب المستندات\"), start=1):\n",
        "    retrieved = retrieve_docs(tokens, inverted_index)\n",
        "    query_vec = represent_query(tokens)\n",
        "    if query_vec is None:\n",
        "      print(\"استعلام فارغ، تم تجاهله\")\n",
        "      continue\n",
        "    ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "# def get_qrels(path=QRELS_PATH):\n",
        "#     df = pd.read_csv(path, sep=\"\\t\")\n",
        "#     df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "#     qrel_dict = defaultdict(dict)\n",
        "#     relevant_set = defaultdict(set)\n",
        "#     for _, row in df.iterrows():\n",
        "#         qid = str(row[\"query_id\"])\n",
        "#         docid = str(row[\"doc_id\"])\n",
        "#         rel = row[\"relevance\"]\n",
        "#         qrel_dict[qid][docid] = rel\n",
        "#         if rel > 0:\n",
        "#             relevant_set[qid].add(docid)\n",
        "#     return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    # نقرأ الملف لكن نتعامل مع الأسطر الغلط ونتجاهلها\n",
        "    try:\n",
        "        df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], skiprows=1, engine='python')\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ خطأ أثناء قراءة الملف: {e}\")\n",
        "        return {}, {}\n",
        "\n",
        "    # حذف الأسطر اللي فيها أعمدة ناقصة أو فيها قيم ناقصة\n",
        "    df = df.dropna(subset=[\"query_id\", \"doc_id\", \"relevance\"])\n",
        "\n",
        "    # تحويل العمود لقيم صحيحة والتعامل مع القيم الخاطئة\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"]).strip()\n",
        "        docid = str(row[\"doc_id\"]).strip()\n",
        "        rel = row[\"relevance\"]\n",
        "\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(dataset_name=\"antique\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs_dict).items()}\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "cal_evaluations(\"antique\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mme6GFigjmeR"
      },
      "source": [
        "#BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eltfuak0jPO4",
        "outputId": "7f344308-1ac2-4eda-c0b3-c0972a325900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "تم بناء وحفظ نموذج BM25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع المستندات باستخدام BM25: 100%|██████████| 200/200 [02:35<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP: 22.49%\n",
            "MRR: 93.60%\n",
            "Mean Precision: 73.20%\n",
            "Mean Recall: 24.05%\n"
          ]
        }
      ],
      "source": [
        "# تثبيت الحزم (تحتاج مرة واحدة فقط)\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm rank_bm25\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm import tqdm\n",
        "\n",
        "# تحميل موارد NLTK (مرة واحدة)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات كما في كودك\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_antique.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_antique.tsv\"\n",
        "CLEANED_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/cleaned_docs_antique.tsv\"\n",
        "BM25_MODEL_PATH = f\"/content/drive/MyDrive/utils/bm25_model/{DATASET_NAME}_bm25.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "#     cleaned_data = []\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🧼 تنظيف وتخزين المستندات\"):\n",
        "#         doc_id = row[\"doc_id\"]\n",
        "#         raw_text = row[\"text\"]\n",
        "#         processed_tokens = clean_text(raw_text)\n",
        "#         processed_text = \" \".join(processed_tokens)\n",
        "#         light_text = light_clean(raw_text)\n",
        "\n",
        "#         cleaned_data.append({\n",
        "#             \"doc_id\": doc_id,\n",
        "#             \"text\": raw_text,\n",
        "#             \"processed_text\": processed_text,\n",
        "#             \"dataset_name\": dataset_name,\n",
        "#             \"light_clean_text\": light_text\n",
        "#         })\n",
        "\n",
        "#     cleaned_df = pd.DataFrame(cleaned_data)\n",
        "#     cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "# === بناء نموذج BM25 ===\n",
        "def build_bm25_model(cleaned_docs_tsv=CLEANED_TSV_PATH, save_path=BM25_MODEL_PATH):\n",
        "    df = pd.read_csv(cleaned_docs_tsv, sep=\"\\t\")\n",
        "    df = df[df['processed_text'].notna() & (df['processed_text'].str.strip() != \"\")]\n",
        "\n",
        "    corpus = [doc.split() for doc in df[\"processed_text\"]]\n",
        "    doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "\n",
        "    bm25 = BM25Okapi(corpus)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    joblib.dump({\"bm25\": bm25, \"doc_ids\": doc_ids, \"corpus\": corpus}, save_path)\n",
        "    print(\"تم بناء وحفظ نموذج BM25\")\n",
        "\n",
        "def load_bm25_model(path=BM25_MODEL_PATH):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(\"نموذج BM25 غير موجود، قم ببنائه أولاً.\")\n",
        "    return joblib.load(path)\n",
        "\n",
        "def clean_and_tokenize_query(raw_query):\n",
        "    return clean_text(raw_query)\n",
        "\n",
        "def retrieve_bm25_docs(query_tokens, bm25_data, top_k=10):\n",
        "    bm25 = bm25_data[\"bm25\"]\n",
        "    doc_ids = bm25_data[\"doc_ids\"]\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "    results = [(doc_ids[i], scores[i]) for i in ranked_indices]\n",
        "    return results\n",
        "\n",
        "# === تحميل وتنظيف البيانات ===\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH)\n",
        "\n",
        "# === بناء نموذج BM25 ===\n",
        "build_bm25_model(CLEANED_TSV_PATH, BM25_MODEL_PATH)\n",
        "\n",
        "# === تحميل واستدعاء النموذج ===\n",
        "bm25_data = load_bm25_model(BM25_MODEL_PATH)\n",
        "\n",
        "# === تحميل وتنظيف الاستعلامات ===\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "    return df\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# === استرجاع المستندات باستخدام BM25 ===\n",
        "retrieved_docs_dict = {}\n",
        "for qid, tokens in tqdm(queries_tokens.items(), desc=\"🔍 استرجاع المستندات باستخدام BM25\"):\n",
        "    ranked = retrieve_bm25_docs(tokens, bm25_data, top_k=10)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "# === دوال التقييم نفسها من كودك الأصلي ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], skiprows=1)\n",
        "    df = df.dropna(subset=[\"query_id\", \"doc_id\", \"relevance\"])\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict, relevant_set = defaultdict(dict), defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]).strip(), str(row[\"doc_id\"]).strip(), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0: relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall\n",
        "    return np.mean(list(recall_scores.values())) * 100\n",
        "\n",
        "# === تحميل الـ qrels ===\n",
        "qrel_dict, real_relevant = get_qrels(QRELS_PATH)\n",
        "\n",
        "# === استرجاع المستندات بدون الدرجات فقط ===\n",
        "retrieved_only_docs = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "# === تقييم الأداء ===\n",
        "map_score = calculate_map(qrel_dict, retrieved_only_docs)\n",
        "mrr_score = calculate_mrr(qrel_dict, retrieved_only_docs)\n",
        "mean_precision = calculate_mean_precision(qrel_dict, retrieved_only_docs)\n",
        "mean_recall = calculate_mean_recall(qrel_dict, real_relevant, retrieved_only_docs)\n",
        "\n",
        "print(f\"MAP: {map_score:.2f}%\")\n",
        "print(f\"MRR: {mrr_score:.2f}%\")\n",
        "print(f\"Mean Precision: {mean_precision:.2f}%\")\n",
        "print(f\"Mean Recall: {mean_recall:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wqt6GeXkDZ1"
      },
      "source": [
        "#TFIDF WITH TOPK=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s6ukDMenIfn",
        "outputId": "07874c3d-22e3-47e2-8cd7-22c2cc9ef9ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> تثبيت الحزم (إذا لم تكن منصبة)...\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            ">> استيراد المكتبات...\n",
            ">> تحميل موارد NLTK...\n",
            ">> تهيئة التنظيف والمعالجة...\n",
            ">> تحميل البيانات...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - عدد المستندات: 403458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🧼 تنظيف وتخزين المستندات: 100%|██████████| 403458/403458 [17:14<00:00, 389.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ تم حفظ الملف في: cleaned_docs12.tsv\n",
            ">> بناء الفهرس العكسي (Inverted Index)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔧 بناء الفهرس: 100%|██████████| 403458/403458 [17:24<00:00, 386.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - عدد كلمات الفهرس: 160415\n",
            ">> تحميل وتنظيف الاستعلامات...\n",
            "   - عدد الاستعلامات: 200\n",
            ">> تحميل الفهرس العكسي من الملف...\n",
            ">> استرجاع المستندات واستعمال TF-IDF...\n",
            "from file\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ">> تجهيز النصوص: 100%|██████████| 403458/403458 [15:48<00:00, 425.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> من البداية: تدريب نموذج TF-IDF\n",
            ">> استرجاع وترتيب المستندات لكل الاستعلامات...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:   6%|▌         | 11/200 [00:02<00:35,  5.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 10 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  10%|█         | 21/200 [00:04<00:28,  6.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 20 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  15%|█▌        | 30/200 [00:06<00:39,  4.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 30 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  20%|██        | 41/200 [00:08<00:27,  5.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 40 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  26%|██▌       | 51/200 [00:10<00:30,  4.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 50 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  30%|███       | 61/200 [00:12<00:25,  5.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 60 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  35%|███▌      | 70/200 [00:13<00:22,  5.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 70 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  40%|████      | 81/200 [00:15<00:19,  6.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 80 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  45%|████▌     | 90/200 [00:17<00:18,  5.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 90 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  50%|█████     | 101/200 [00:19<00:19,  5.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 100 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  56%|█████▌    | 111/200 [00:21<00:16,  5.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 110 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  60%|██████    | 120/200 [00:23<00:22,  3.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 120 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  65%|██████▌   | 130/200 [00:26<00:12,  5.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 130 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  70%|███████   | 141/200 [00:28<00:09,  6.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 140 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  75%|███████▌  | 150/200 [00:29<00:08,  5.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 150 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  80%|████████  | 161/200 [00:31<00:05,  7.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 160 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  86%|████████▌ | 171/200 [00:33<00:05,  5.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 170 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  90%|█████████ | 181/200 [00:35<00:03,  6.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 180 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات:  96%|█████████▌| 191/200 [00:38<00:02,  4.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 190 من 200 استعلامات\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "🔍 استرجاع وترتيب المستندات: 100%|██████████| 200/200 [00:40<00:00,  4.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - تمت معالجة 200 من 200 استعلامات\n",
            ">> تحميل qrels وتحضير التقييم...\n",
            ">> بدء التقييم النهائي...\n",
            "\n",
            "== Evaluation for dataset: antique ==\n",
            "MAP: 20.09 %\n",
            "MRR: 76.6 %\n",
            "Mean Precision: 16.36 %\n",
            "Mean Recall: 44.08 %\n",
            "Mean Precision@10: 38.45 %\n"
          ]
        }
      ],
      "source": [
        "#تثبيت الحزم (مرة واحدة فقط)\n",
        "print(\">> تثبيت الحزم (إذا لم تكن منصبة)...\")\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "\n",
        "\n",
        "# استيراد المكتبات\n",
        "print(\">> استيراد المكتبات...\")\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "# تحميل موارد NLTK\n",
        "print(\">> تحميل موارد NLTK...\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index12.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens12.tsv\"\n",
        "\n",
        "print(\">> تهيئة التنظيف والمعالجة...\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "# ربط الوسوم الصرفية بووردنت\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "# قائمة الاختصارات الشائعة\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\",\n",
        "    \"r\": \"are\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"i'm\": \"i am\",\n",
        "    # يمكن إضافة المزيد\n",
        "}\n",
        "\n",
        "# توسيع الاختصارات\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "\n",
        "# المعالجة الأساسية للنص\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    # text = normalize_date(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()  # إزالة الرموز غير القابلة للترميز\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)     # حذف الروابط\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)                     # حذف الإيميلات\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # حذف علامات الترقيم\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)                 # حذف الأرقام الصغيرة فقط\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)                  # حذف أرقام طويلة جداً (ممكن ID أو رقم هاتف)\n",
        "    return text\n",
        "\n",
        "# توكنيزيشن وتلميتايزيشن\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "# الدالة النهائية\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def save_cleaned_docs(df, dataset_name=\"antique\", path_out=\"cleaned_docs12.tsv\"):\n",
        "    \"\"\"\n",
        "    تنظيف المستندات وتخزينها في ملف TSV مع الأعمدة التالية:\n",
        "    doc_id, text, processed_text, dataset_name, light_clean_text\n",
        "    \"\"\"\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🧼 تنظيف وتخزين المستندات\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "\n",
        "        # التنظيف العميق\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "\n",
        "        # التنظيف الخفيف\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "    print(f\"✅ تم حفظ الملف في: {path_out}\")\n",
        "\n",
        "\n",
        "print(\">> تحميل البيانات...\")\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "print(f\"   - عدد المستندات: {len(df)}\")\n",
        "\n",
        "save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=\"cleaned_docs12.tsv\")\n",
        "\n",
        "\n",
        "\n",
        "print(\">> بناء الفهرس العكسي (Inverted Index)...\")\n",
        "def build_inverted_index(df):\n",
        "    index = defaultdict(set)\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"🔧 بناء الفهرس\"):\n",
        "        for token in clean_text(row[\"text\"]):\n",
        "            index[token].add(str(row[\"doc_id\"]))\n",
        "    os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "    with open(INVERTED_PATH, \"w\") as f:\n",
        "        json.dump({k: list(v) for k, v in index.items()}, f)\n",
        "    return index\n",
        "inverted_index = build_inverted_index(df)\n",
        "print(f\"   - عدد كلمات الفهرس: {len(inverted_index)}\")\n",
        "\n",
        "print(\">> تحميل وتنظيف الاستعلامات...\")\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "print(f\"   - عدد الاستعلامات: {len(queries_df)}\")\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "print(\">> تحميل الفهرس العكسي من الملف...\")\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "print(\">> استرجاع المستندات واستعمال TF-IDF...\")\n",
        "print(\"from file\")\n",
        "tqdm.pandas(desc=\">> تجهيز النصوص\")\n",
        "\n",
        "# نظف النصوص مسبقاً\n",
        "# df[\"clean_text\"] = df[\"text\"].progress_apply(lambda x: \" \".join(clean_text(x)))\n",
        "\n",
        "# # نموذج TF-IDF\n",
        "# tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf12.joblib\"\n",
        "\n",
        "# if os.path.exists(tfidf_model_path):\n",
        "#     tfidf_data = joblib.load(tfidf_model_path)\n",
        "#     vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "# else:\n",
        "#     print(\">> من البداية: تدريب نموذج TF-IDF\")\n",
        "#     vectorizer = TfidfVectorizer(lowercase=False)  # لا حاجة لـ tokenizer/preprocessor الآن\n",
        "#     X = vectorizer.fit_transform(df[\"clean_text\"])\n",
        "#     tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "#     os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "#     joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].progress_apply(lambda x: \" \".join(clean_text(x)))\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    print(\">> من البداية: تدريب نموذج TF-IDF\")\n",
        "     vectorizer = TfidfVectorizer(\n",
        "     preprocessor=processing,\n",
        "     tokenizer=tokenize,\n",
        "     lowercase=False,\n",
        "     token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"clean_text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "\n",
        "def represent_query(tokens): return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices: return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx if sims[i] > 0]\n",
        "\n",
        "print(\">> استرجاع وترتيب المستندات لكل الاستعلامات...\")\n",
        "retrieved_docs_dict = {}\n",
        "for i, (qid, tokens) in enumerate(tqdm(queries_tokens.items(), desc=\"🔍 استرجاع وترتيب المستندات\"), start=1):\n",
        "    retrieved = retrieve_docs(tokens, inverted_index)\n",
        "    query_vec = represent_query(tokens)\n",
        "    ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=100)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    if i % 10 == 0 or i == len(queries_tokens):\n",
        "        print(f\"   - تمت معالجة {i} من {len(queries_tokens)} استعلامات\")\n",
        "\n",
        "# التقييم\n",
        "print(\">> تحميل qrels وتحضير التقييم...\")\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], skiprows=1)\n",
        "    df = df.dropna(subset=[\"query_id\", \"doc_id\", \"relevance\"])\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict, relevant_set = defaultdict(dict), defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]).strip(), str(row[\"doc_id\"]).strip(), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0: relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        # if num_relevant == 0:\n",
        "        #     continue  # تجاهل الاستعلامات التي ليس لها مستندات ذات صلة\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "print(\">> بدء التقييم النهائي...\")\n",
        "def cal_evaluations(dataset_name=\"antique\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "cal_evaluations(\"antique\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qeq7pHYLkl-d"
      },
      "source": [
        "#EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMkREBslkjzd",
        "outputId": "6c687708-6c32-45ea-dcac-517504378408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 تحميل التمثيلات من: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding_antique.joblib\n",
            "📥 تحميل الاستعلامات من: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "🔎 عدد الاستعلامات بعد التنظيف: 200\n",
            "🧪 عينة استعلامات:\n",
            "  query_id                                               text  \\\n",
            "0  3990512          how can we get concentration onsomething?   \n",
            "1   714612  Why doesn't the water fall off earth if it's r...   \n",
            "2  2528767  How do I determine the charge of the iron ion ...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "🧠 تم تمثيل 200 استعلام\n",
            "🧠 تم تمثيل 200 استعلام\n",
            "📥 تم استرجاع نتائج لـ 200 استعلام\n",
            "\n",
            "📊 بدء التقييم...\n",
            "📌 استعلامات للتقييم: 200\n",
            "🔎 عدد استعلامات qrels: 200\n",
            "🔎 عدد استعلامات الاسترجاع: 200\n",
            "🔗 عدد الاستعلامات المشتركة بين qrels والاسترجاع: 200\n",
            "❗ عدد المستندات المشار لها في qrels لكنها غير موجودة في التمثيل: 0\n",
            "\n",
            "== Evaluation for dataset: antique ==\n",
            "MAP: 16.34 %\n",
            "MRR: 79.34 %\n",
            "Mean Precision: 17.62 %\n",
            "Mean Recall: 27.93 %\n",
            "Mean Precision@10: 39.7 %\n"
          ]
        }
      ],
      "source": [
        "import os, re, string, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ======== تحميل موارد NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== إعداد المسارات ========\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_TSV_PATH =f\"/content/drive/MyDrive/utils/clean_docs/antique_cleaned_docs_antique\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"/content/drive/MyDrive/utils/queries_tokens/light_cleaned_queries_antique.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_antique.joblib\")\n",
        "TOP_K = 50\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== الدوال المساعدة للنصوص ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== تحميل المستندات وتمثيلها ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"📦 تحميل التمثيلات من: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"🚀 لم يتم العثور على التمثيلات، جاري التوليد...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"🗂️ عدد المستندات المحملة: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        print(\"⚠️ عمود 'light_clean_text' غير موجود — سيتم توليده.\")\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"light_clean_text\"].fillna(\"\").astype(str)\n",
        "\n",
        "    print(\"🧪 عينة من المستندات المنظفة:\")\n",
        "    print(df_docs[[\"doc_id\", \"light_clean_text\"]].head())\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    print(f\"✅ تم حفظ التمثيلات في: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"📥 تحميل الاستعلامات من: {path}\")\n",
        "\n",
        "    # معالجة الملف يدويًا لأن التنسيق غير منتظم (الكلمات موزعة على أعمدة كثيرة)\n",
        "    cleaned_queries = []\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        next(f)  # تخطي الرأس\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = ' '.join(parts[1:])\n",
        "                cleaned_queries.append((query_id, text))\n",
        "\n",
        "    # إنشاء DataFrame\n",
        "    df = pd.DataFrame(cleaned_queries, columns=['query_id', 'text'])\n",
        "\n",
        "    # تنظيف النصوص\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(f\"🔎 عدد الاستعلامات بعد التنظيف: {len(df)}\")\n",
        "\n",
        "    print(\"🧪 عينة استعلامات:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    # حفظ النسخة النظيفة إلى ملف TSV بصيغة: query_id \\t light_clean_text\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# def load_and_clean_queries(path):\n",
        "#     print(f\"📥 تحميل الاستعلامات من: {path}\")\n",
        "#     df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str)\n",
        "#     print(f\"🔎 عدد الاستعلامات بعد التنظيف: {len(df)}\")\n",
        "\n",
        "#     df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "#     print(\"🧪 عينة استعلامات:\")\n",
        "#     print(df.head(3))\n",
        "\n",
        "#     with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "#         for _, row in df.iterrows():\n",
        "#             f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "#     return df\n",
        "\n",
        "\n",
        "# ======== تمثيل الاستعلامات ========\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            tokens = parts[1].split()\n",
        "            if not all(isinstance(t, str) for t in tokens):\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = tokens\n",
        "    return tokens_dict\n",
        "\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = []\n",
        "    query_texts = []\n",
        "    for qid, tokens in queries_tokens_dict.items():\n",
        "        if isinstance(tokens, list) and all(isinstance(t, str) for t in tokens):\n",
        "            query_ids.append(qid)\n",
        "            query_texts.append(\" \".join(tokens))\n",
        "    print(f\"🧠 تم تمثيل {len(query_texts)} استعلام\")\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== الاسترجاع ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "# ======== تحميل QRELS ========\n",
        "# def get_qrels(path=QRELS_PATH):\n",
        "#     df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "#     df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "#     qrel_dict = defaultdict(dict)\n",
        "#     relevant_set = defaultdict(set)\n",
        "#     for _, row in df.iterrows():\n",
        "#         qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "#         qrel_dict[qid][docid] = rel\n",
        "#         if rel > 0:\n",
        "#             relevant_set[qid].add(docid)\n",
        "#     return qrel_dict, relevant_set\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    # قراءة الملف مع السماح بأي مسافات أو تاب كفاصل\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=0, names=[\"query_id\", \"doc_id\", \"relevance\"])\n",
        "\n",
        "    # تأكد أن عمود relevance رقمي\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "# ======== التقييم ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "# ======== التقييم النهائي ========\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"antique\"):\n",
        "    print(\"\\n📊 بدء التقييم...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "    print(f\"📌 استعلامات للتقييم: {len(retrieved_docs_str)}\")\n",
        "    print(\"🔎 عدد استعلامات qrels:\", len(qrel_dict))\n",
        "    print(\"🔎 عدد استعلامات الاسترجاع:\", len(retrieved_docs_str))\n",
        "    print(\"🔗 عدد الاستعلامات المشتركة بين qrels والاسترجاع:\", len(set(qrel_dict.keys()) & set(retrieved_docs_str.keys())))\n",
        "\n",
        "# ⛔ تحليل المستندات المفقودة في التمثيل\n",
        "    all_qrels_doc_ids = {docid for rels in qrel_dict.values() for docid in rels}\n",
        "    missing_doc_ids = all_qrels_doc_ids - set(doc_ids)\n",
        "    print(f\"❗ عدد المستندات المشار لها في qrels لكنها غير موجودة في التمثيل: {len(missing_doc_ids)}\")\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== تشغيل كامل ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"🧠 تم تمثيل {len(query_embeddings)} استعلام\")\n",
        "\n",
        "    retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "    print(f\"📥 تم استرجاع نتائج لـ {len(retrieved_docs)} استعلام\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=\"antique\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HYBRID"
      ],
      "metadata": {
        "id": "dD0IzSKT7zQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت الحزم\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "# استيراد المكتبات\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "# تحميل موارد NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# إعداد المسارات\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_antique1.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_antique.joblib\"\n",
        "embedding_file_path = f\"/content/drive/MyDrive/embedding_model_joblib_file/{DATASET_NAME}_embedding_antique.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "            for word, pos in tokens_pos\n",
        "            if word not in stop_words and len(word) > 1]\n",
        "\n",
        "def clean_text(text):\n",
        "    return tokenize(processing(text))\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"📥 تحميل الاستعلامات من: {path}\")\n",
        "\n",
        "    # نقرأ الملف نصياً ثم نحول \\t إلى تبويب حقيقي\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        lines = [line.encode().decode(\"unicode_escape\").strip() for line in f if line.strip()]\n",
        "\n",
        "    query_ids = []\n",
        "    texts = []\n",
        "\n",
        "    for i, line in enumerate(lines, 1):\n",
        "        parts = line.split('\\t', 1)  # نقسم فقط على أول تبويب\n",
        "        if len(parts) != 2:\n",
        "            print(f\"⚠️ تجاهل السطر {i}: يحتوي على {len(parts)} أعمدة بدلاً من 2\")\n",
        "            continue\n",
        "        query_ids.append(parts[0])\n",
        "        texts.append(parts[1])\n",
        "\n",
        "    df = pd.DataFrame({'query_id': query_ids, 'text': texts})\n",
        "    print(f\"🔎 عدد الاستعلامات بعد التنظيف: {len(df)}\")\n",
        "\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(\"🧪 عينة استعلامات:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) != 2:\n",
        "                print(f\"⚠️ تجاهل السطر {i}: فيه {len(parts)} جزء بدل 2\")\n",
        "                continue\n",
        "            tokens = parts[1].split()\n",
        "            if not all(isinstance(t, str) for t in tokens):\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = tokens\n",
        "    return tokens_dict\n",
        "\n",
        "# تحميل بيانات الاستعلامات\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# تحميل نموذج TF-IDF\n",
        "tfidf_data = joblib.load(tfidf_model_path)\n",
        "vectorizer = tfidf_data[\"vectorizer\"]\n",
        "X = tfidf_data[\"vectors\"]\n",
        "tfidf_doc_ids = tfidf_data[\"doc_ids\"]\n",
        "\n",
        "# تحميل نموذج Embedding والتمثيلات\n",
        "embedding_model_path = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "embedding_model = SentenceTransformer(embedding_model_path)\n",
        "embedding_data = joblib.load(embedding_file_path)\n",
        "doc_embeddings = embedding_data[\"embeddings\"]\n",
        "embedding_doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "def represent_query_tfidf(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def represent_query_embedding(query_text):\n",
        "    return embedding_model.encode([query_text], convert_to_numpy=True)[0]\n",
        "\n",
        "def hybrid_rank(query_tokens, query_text, top_k=10, alpha=0.5):\n",
        "    tfidf_vec = represent_query_tfidf(query_tokens)\n",
        "    embedding_vec = represent_query_embedding(query_text)\n",
        "\n",
        "    tfidf_sims = cosine_similarity(tfidf_vec, X).flatten()\n",
        "\n",
        "    embedding_sims = cosine_similarity([embedding_vec], doc_embeddings).flatten()\n",
        "    embedding_sims = embedding_sims[1:]\n",
        "\n",
        "    hybrid_sims = alpha * tfidf_sims + (1 - alpha) * embedding_sims\n",
        "    top_indices = np.argsort(-hybrid_sims)[:top_k]\n",
        "    return [(tfidf_doc_ids[i], hybrid_sims[i]) for i in top_indices]\n",
        "\n",
        "# تطبيق الاسترجاع لجميع الاستعلامات\n",
        "retrieved_docs_dict = {}\n",
        "for qid, tokens in tqdm(queries_tokens.items(), desc=\"Hybrid Retrieval\"):\n",
        "    text = \" \".join(tokens)\n",
        "    results = hybrid_rank(tokens, text, top_k=10, alpha=0.5)\n",
        "    retrieved_docs_dict[qid] = results\n",
        "\n",
        "# التقييم (مثل السابق)\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0: continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, []))\n",
        "        if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(dataset_name=\"antique\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs_str, k=10)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# استدعاء التقييم\n",
        "cal_evaluations(\"antique\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84bMECsA71lB",
        "outputId": "d4821917-87e5-4f7f-af96-5527f26d0b7f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 تحميل الاستعلامات من: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "⚠️ تجاهل السطر 1: يحتوي على 1 أعمدة بدلاً من 2\n",
            "🔎 عدد الاستعلامات بعد التنظيف: 200\n",
            "🧪 عينة استعلامات:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hybrid Retrieval: 100%|██████████| 200/200 [03:22<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== Evaluation for dataset: antique ==\n",
            "MAP: 8.32 %\n",
            "MRR: 72.42 %\n",
            "Mean Precision: 35.35 %\n",
            "Mean Recall: 11.29 %\n",
            "Mean Precision@10: 35.35 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMBEDDING WITH VECTOR STORE"
      ],
      "metadata": {
        "id": "38NCbAB_OQVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm faiss-cpu\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import os, re, string, time, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ======== تحميل موارد NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== إعداد المسارات ========\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/antique_cleaned_docs_antique\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_antique.joblib\")\n",
        "FAISS_INDEX_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding.index\")\n",
        "FAISS_META_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_meta.json\")\n",
        "TOP_K = 50\n",
        "RETRIEVAL_MODE = \"faiss\"  # اختر بين \"cosine\" أو \"faiss\"\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== تنظيف نصوص ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== تحميل أو توليد تمثيلات المستندات ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"📦 تحميل التمثيلات من: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"🚀 لم يتم العثور على التمثيلات، جاري التوليد...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"🗂️ عدد المستندات المحملة: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    # حفظ metadata لـ FAISS لاحقاً\n",
        "    with open(FAISS_META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"✅ تم حفظ التمثيلات في: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def build_and_save_faiss_index(embeddings, index_path, meta_path, doc_ids):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)  # استخدم نوع الفهرس المناسب\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    # حفظ الـ metadata (doc_ids)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"✅ تم بناء وحفظ فهرس FAISS في: {index_path}\")\n",
        "\n",
        "\n",
        "# ======== تحميل الاستعلامات ========\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"📥 تحميل الاستعلامات من: {path}\")\n",
        "\n",
        "    # نقرأ الملف نصياً ثم نحول \\t إلى تبويب حقيقي\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        lines = [line.encode().decode(\"unicode_escape\").strip() for line in f if line.strip()]\n",
        "\n",
        "    query_ids = []\n",
        "    texts = []\n",
        "\n",
        "    for i, line in enumerate(lines, 1):\n",
        "        parts = line.split('\\t', 1)  # نقسم فقط على أول تبويب\n",
        "        if len(parts) != 2:\n",
        "            print(f\"⚠️ تجاهل السطر {i}: يحتوي على {len(parts)} أعمدة بدلاً من 2\")\n",
        "            continue\n",
        "        query_ids.append(parts[0])\n",
        "        texts.append(parts[1])\n",
        "\n",
        "    df = pd.DataFrame({'query_id': query_ids, 'text': texts})\n",
        "    print(f\"🔎 عدد الاستعلامات بعد التنظيف: {len(df)}\")\n",
        "\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(\"🧪 عينة استعلامات:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== استرجاع ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "def retrieve_with_faiss(query_embeddings_dict, index_path, doc_ids, top_k=10):\n",
        "    index = faiss.read_index(index_path)\n",
        "    query_ids = list(query_embeddings_dict.keys())\n",
        "    query_embeddings = np.array([query_embeddings_dict[qid] for qid in query_ids]).astype(\"float32\")\n",
        "    scores, indices = index.search(query_embeddings, top_k)\n",
        "\n",
        "    results = {}\n",
        "    for i, qid in enumerate(query_ids):\n",
        "        results[qid] = [(doc_ids[idx], float(scores[i][j])) for j, idx in enumerate(indices[i])]\n",
        "    return results\n",
        "\n",
        "def load_doc_ids_from_meta(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ======== تحميل QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== التقييم ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"antique\"):\n",
        "    print(\"\\n📊 بدء التقييم...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== التشغيل الكامل ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    build_and_save_faiss_index(embedding_data[\"embeddings\"], FAISS_INDEX_PATH, FAISS_META_PATH, embedding_data[\"doc_ids\"])\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"🧠 تم تمثيل {len(query_embeddings)} استعلام\")\n",
        "\n",
        "    if RETRIEVAL_MODE == \"cosine\":\n",
        "        print(\"\\n🚀 الاسترجاع باستخدام cosine_similarity...\")\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    elif RETRIEVAL_MODE == \"faiss\":\n",
        "        print(\"\\n⚡ الاسترجاع باستخدام FAISS...\")\n",
        "        doc_ids = load_doc_ids_from_meta(FAISS_META_PATH)\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_faiss(query_embeddings, FAISS_INDEX_PATH, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    else:\n",
        "        raise ValueError(f\"❌ Retrieval mode '{RETRIEVAL_MODE}' not supported.\")\n",
        "\n",
        "    print(f\"📥 تم استرجاع نتائج لـ {len(retrieved_docs)} استعلام في {round(duration, 2)} ثانية\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=f\"{DATASET_NAME}-{RETRIEVAL_MODE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_ai5HTKOPtH",
        "outputId": "c1857b5e-0d03-4425-b915-d7c4f121a7be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 تحميل الاستعلامات من: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "⚠️ تجاهل السطر 1: يحتوي على 1 أعمدة بدلاً من 2\n",
            "🔎 عدد الاستعلامات بعد التنظيف: 200\n",
            "🧪 عينة استعلامات:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "📦 تحميل التمثيلات من: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding_antique.joblib\n",
            "✅ تم بناء وحفظ فهرس FAISS في: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding.index\n",
            "📥 تحميل الاستعلامات من: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "⚠️ تجاهل السطر 1: يحتوي على 1 أعمدة بدلاً من 2\n",
            "🔎 عدد الاستعلامات بعد التنظيف: 200\n",
            "🧪 عينة استعلامات:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "🧠 تم تمثيل 200 استعلام\n",
            "\n",
            "⚡ الاسترجاع باستخدام FAISS...\n",
            "📥 تم استرجاع نتائج لـ 200 استعلام في 2.51 ثانية\n",
            "\n",
            "📊 بدء التقييم...\n",
            "\n",
            "== Evaluation for dataset: antique-faiss ==\n",
            "MAP: 16.34 %\n",
            "MRR: 79.34 %\n",
            "Mean Precision: 17.62 %\n",
            "Mean Recall: 27.93 %\n",
            "Mean Precision@10: 39.7 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMBEDDING WITH COSINE"
      ],
      "metadata": {
        "id": "yeoQucVSSdam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm faiss-cpu\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import os, re, string, time, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ======== تحميل موارد NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== إعداد المسارات ========\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/antique_cleaned_docs_antique\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_antique.joblib\")\n",
        "FAISS_INDEX_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding.index\")\n",
        "FAISS_META_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_meta.json\")\n",
        "TOP_K = 50\n",
        "RETRIEVAL_MODE = \"cosine\"  # اختر بين \"cosine\" أو \"faiss\"\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== تنظيف نصوص ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== تحميل أو توليد تمثيلات المستندات ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"📦 تحميل التمثيلات من: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"🚀 لم يتم العثور على التمثيلات، جاري التوليد...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"🗂️ عدد المستندات المحملة: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    # حفظ metadata لـ FAISS لاحقاً\n",
        "    with open(FAISS_META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"✅ تم حفظ التمثيلات في: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def build_and_save_faiss_index(embeddings, index_path, meta_path, doc_ids):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)  # استخدم نوع الفهرس المناسب\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    # حفظ الـ metadata (doc_ids)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"✅ تم بناء وحفظ فهرس FAISS في: {index_path}\")\n",
        "\n",
        "\n",
        "# ======== تحميل الاستعلامات ========\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"📥 تحميل الاستعلامات من: {path}\")\n",
        "\n",
        "    # نقرأ الملف نصياً ثم نحول \\t إلى تبويب حقيقي\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        lines = [line.encode().decode(\"unicode_escape\").strip() for line in f if line.strip()]\n",
        "\n",
        "    query_ids = []\n",
        "    texts = []\n",
        "\n",
        "    for i, line in enumerate(lines, 1):\n",
        "        parts = line.split('\\t', 1)  # نقسم فقط على أول تبويب\n",
        "        if len(parts) != 2:\n",
        "            print(f\"⚠️ تجاهل السطر {i}: يحتوي على {len(parts)} أعمدة بدلاً من 2\")\n",
        "            continue\n",
        "        query_ids.append(parts[0])\n",
        "        texts.append(parts[1])\n",
        "\n",
        "    df = pd.DataFrame({'query_id': query_ids, 'text': texts})\n",
        "    print(f\"🔎 عدد الاستعلامات بعد التنظيف: {len(df)}\")\n",
        "\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(\"🧪 عينة استعلامات:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== استرجاع ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "def retrieve_with_faiss(query_embeddings_dict, index_path, doc_ids, top_k=10):\n",
        "    index = faiss.read_index(index_path)\n",
        "    query_ids = list(query_embeddings_dict.keys())\n",
        "    query_embeddings = np.array([query_embeddings_dict[qid] for qid in query_ids]).astype(\"float32\")\n",
        "    scores, indices = index.search(query_embeddings, top_k)\n",
        "\n",
        "    results = {}\n",
        "    for i, qid in enumerate(query_ids):\n",
        "        results[qid] = [(doc_ids[idx], float(scores[i][j])) for j, idx in enumerate(indices[i])]\n",
        "    return results\n",
        "\n",
        "def load_doc_ids_from_meta(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ======== تحميل QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== التقييم ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"antique\"):\n",
        "    print(\"\\n📊 بدء التقييم...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== التشغيل الكامل ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    build_and_save_faiss_index(embedding_data[\"embeddings\"], FAISS_INDEX_PATH, FAISS_META_PATH, embedding_data[\"doc_ids\"])\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"🧠 تم تمثيل {len(query_embeddings)} استعلام\")\n",
        "\n",
        "    if RETRIEVAL_MODE == \"cosine\":\n",
        "        print(\"\\n🚀 الاسترجاع باستخدام cosine_similarity...\")\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    elif RETRIEVAL_MODE == \"faiss\":\n",
        "        print(\"\\n⚡ الاسترجاع باستخدام FAISS...\")\n",
        "        doc_ids = load_doc_ids_from_meta(FAISS_META_PATH)\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_faiss(query_embeddings, FAISS_INDEX_PATH, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    else:\n",
        "        raise ValueError(f\"❌ Retrieval mode '{RETRIEVAL_MODE}' not supported.\")\n",
        "\n",
        "    print(f\"📥 تم استرجاع نتائج لـ {len(retrieved_docs)} استعلام في {round(duration, 2)} ثانية\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=f\"{DATASET_NAME}-{RETRIEVAL_MODE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk6P-hyYSdIz",
        "outputId": "800f70a9-2607-43a3-8ca9-b8d0ec7cdaf7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 تحميل الاستعلامات من: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "⚠️ تجاهل السطر 1: يحتوي على 1 أعمدة بدلاً من 2\n",
            "🔎 عدد الاستعلامات بعد التنظيف: 200\n",
            "🧪 عينة استعلامات:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "📦 تحميل التمثيلات من: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding_antique.joblib\n",
            "✅ تم بناء وحفظ فهرس FAISS في: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding.index\n",
            "📥 تحميل الاستعلامات من: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "⚠️ تجاهل السطر 1: يحتوي على 1 أعمدة بدلاً من 2\n",
            "🔎 عدد الاستعلامات بعد التنظيف: 200\n",
            "🧪 عينة استعلامات:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "🧠 تم تمثيل 200 استعلام\n",
            "\n",
            "🚀 الاسترجاع باستخدام cosine_similarity...\n",
            "📥 تم استرجاع نتائج لـ 200 استعلام في 149.89 ثانية\n",
            "\n",
            "📊 بدء التقييم...\n",
            "\n",
            "== Evaluation for dataset: antique-cosine ==\n",
            "MAP: 16.34 %\n",
            "MRR: 79.34 %\n",
            "Mean Precision: 17.62 %\n",
            "Mean Recall: 27.93 %\n",
            "Mean Precision@10: 39.7 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}