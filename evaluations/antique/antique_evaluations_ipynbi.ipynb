{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwFK1eDqYvF2",
        "outputId": "d9ba37a7-cd48-469a-8ae9-94b1aa1e734f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx6Trdl_j0eb"
      },
      "source": [
        "#TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owTArWQr7r3q",
        "outputId": "68111c4f-c314-417e-8801-d9f21674555d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403458/403458 [26:16<00:00, 255.93it/s]\n",
            "ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400862/400862 [00:41<00:00, 9578.45it/s] \n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:12<00:00,  2.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== Evaluation for dataset: antique ==\n",
            "MAP: 10.27 %\n",
            "MRR: 76.37 %\n",
            "Mean Precision: 40.05 %\n",
            "Mean Recall: 12.84 %\n",
            "Mean Precision@10: 38.4 %\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù…\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import datefinder\n",
        "import country_converter as coco\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_antique9.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_antique9.tsv\"\n",
        "CLEANED_TSV_PATH = \"cleaned_docs_antique9.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_antique9.joblib\"\n",
        "Json_file = os.path.join(BASE_PATH, \"qas.search.json\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=\"cleaned_docs_antique9.tsv\"):\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "\n",
        "        if not processed_tokens:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ØªÙŠ Ø£ØµØ¨Ø­Øª ÙØ§Ø±ØºØ©\n",
        "            continue\n",
        "\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=\"cleaned_docs_antique9.tsv\")\n",
        "\n",
        "def build_inverted_index_from_tsv(tsv_path=CLEANED_TSV_PATH):\n",
        "    df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
        "    index = defaultdict(set)\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³\"):\n",
        "        doc_id = str(row[\"doc_id\"])\n",
        "        tokens = str(row[\"processed_text\"]).split()\n",
        "        for token in tokens:\n",
        "            index[token].add(doc_id)\n",
        "    os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "    with open(INVERTED_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({k: list(v) for k, v in index.items()}, f, ensure_ascii=False, indent=2)\n",
        "    return index\n",
        "\n",
        "inverted_index = build_inverted_index_from_tsv()\n",
        "\n",
        "# def load_and_clean_queries(path):\n",
        "#     data = []\n",
        "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(\"\\t\")\n",
        "#             if len(parts) >= 2:\n",
        "#                 query_id = parts[0]\n",
        "#                 text = \" \".join(parts[1:])\n",
        "#                 data.append((query_id, text))\n",
        "#     df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "#     df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "#     with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "#         for _, row in df.iterrows():\n",
        "#             f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "#     return df\n",
        "\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        next(f)  # ØªØ®Ø·ÙŠ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„ (Ø§Ù„Ø±Ø£Ø³)\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\", maxsplit=1)\n",
        "            if len(parts) == 2:\n",
        "                query_id = parts[0]\n",
        "                text = parts[1].replace('\\t', ' ')  # Ù„Ùˆ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙÙŠÙ‡ ÙƒÙ„Ù…Ø§Øª Ù…ÙØµÙˆÙ„Ø© Ø¨Ù€ tab\n",
        "                data.append((query_id, text))\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ DataFrame\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "\n",
        "    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "tqdm.pandas(desc=\">> ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ\")\n",
        "\n",
        "def custom_preprocessor(text):\n",
        "    return processing(text)\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    return tokenize(text)\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "    lowercase=False,\n",
        "    preprocessor=custom_preprocessor,\n",
        "    tokenizer=custom_tokenizer,\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "loaded_model = joblib.load(tfidf_model_path)\n",
        "vectorizer = loaded_model[\"vectorizer\"]\n",
        "X = loaded_model[\"vectors\"]\n",
        "tfidf_doc_ids = loaded_model[\"doc_ids\"]\n",
        "\n",
        "def represent_query(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=100):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices:\n",
        "        return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx]\n",
        "\n",
        "retrieved_docs_dict = {}\n",
        "for i, (qid, tokens) in enumerate(tqdm(queries_tokens.items(), desc=\"ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"), start=1):\n",
        "    retrieved = retrieve_docs(tokens, inverted_index)\n",
        "    query_vec = represent_query(tokens)\n",
        "    if query_vec is None:\n",
        "      print(\"Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ§Ø±ØºØŒ ØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡\")\n",
        "      continue\n",
        "    ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=10)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "# def get_qrels(path=QRELS_PATH):\n",
        "#     df = pd.read_csv(path, sep=\"\\t\")\n",
        "#     df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "#     qrel_dict = defaultdict(dict)\n",
        "#     relevant_set = defaultdict(set)\n",
        "#     for _, row in df.iterrows():\n",
        "#         qid = str(row[\"query_id\"])\n",
        "#         docid = str(row[\"doc_id\"])\n",
        "#         rel = row[\"relevance\"]\n",
        "#         qrel_dict[qid][docid] = rel\n",
        "#         if rel > 0:\n",
        "#             relevant_set[qid].add(docid)\n",
        "#     return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    # Ù†Ù‚Ø±Ø£ Ø§Ù„Ù…Ù„Ù Ù„ÙƒÙ† Ù†ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ØºÙ„Ø· ÙˆÙ†ØªØ¬Ø§Ù‡Ù„Ù‡Ø§\n",
        "    try:\n",
        "        df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], skiprows=1, engine='python')\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}\")\n",
        "        return {}, {}\n",
        "\n",
        "    # Ø­Ø°Ù Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ Ø£Ø¹Ù…Ø¯Ø© Ù†Ø§Ù‚ØµØ© Ø£Ùˆ ÙÙŠÙ‡Ø§ Ù‚ÙŠÙ… Ù†Ø§Ù‚ØµØ©\n",
        "    df = df.dropna(subset=[\"query_id\", \"doc_id\", \"relevance\"])\n",
        "\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ù„Ù‚ÙŠÙ… ØµØ­ÙŠØ­Ø© ÙˆØ§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø®Ø§Ø·Ø¦Ø©\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        qid = str(row[\"query_id\"]).strip()\n",
        "        docid = str(row[\"doc_id\"]).strip()\n",
        "        rel = row[\"relevance\"]\n",
        "\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(dataset_name=\"antique\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs_dict).items()}\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "cal_evaluations(\"antique\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mme6GFigjmeR"
      },
      "source": [
        "#BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eltfuak0jPO4",
        "outputId": "7f344308-1ac2-4eda-c0b3-c0972a325900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ BM25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BM25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:35<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP: 22.49%\n",
            "MRR: 93.60%\n",
            "Mean Precision: 73.20%\n",
            "Mean Recall: 24.05%\n"
          ]
        }
      ],
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… (ØªØ­ØªØ§Ø¬ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm rank_bm25\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ÙƒÙ…Ø§ ÙÙŠ ÙƒÙˆØ¯Ùƒ\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index_antique.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_antique.tsv\"\n",
        "CLEANED_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/cleaned_docs_antique.tsv\"\n",
        "BM25_MODEL_PATH = f\"/content/drive/MyDrive/utils/bm25_model/{DATASET_NAME}_bm25.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# def save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH):\n",
        "#     cleaned_data = []\n",
        "#     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"):\n",
        "#         doc_id = row[\"doc_id\"]\n",
        "#         raw_text = row[\"text\"]\n",
        "#         processed_tokens = clean_text(raw_text)\n",
        "#         processed_text = \" \".join(processed_tokens)\n",
        "#         light_text = light_clean(raw_text)\n",
        "\n",
        "#         cleaned_data.append({\n",
        "#             \"doc_id\": doc_id,\n",
        "#             \"text\": raw_text,\n",
        "#             \"processed_text\": processed_text,\n",
        "#             \"dataset_name\": dataset_name,\n",
        "#             \"light_clean_text\": light_text\n",
        "#         })\n",
        "\n",
        "#     cleaned_df = pd.DataFrame(cleaned_data)\n",
        "#     cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "# === Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ BM25 ===\n",
        "def build_bm25_model(cleaned_docs_tsv=CLEANED_TSV_PATH, save_path=BM25_MODEL_PATH):\n",
        "    df = pd.read_csv(cleaned_docs_tsv, sep=\"\\t\")\n",
        "    df = df[df['processed_text'].notna() & (df['processed_text'].str.strip() != \"\")]\n",
        "\n",
        "    corpus = [doc.split() for doc in df[\"processed_text\"]]\n",
        "    doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "\n",
        "    bm25 = BM25Okapi(corpus)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    joblib.dump({\"bm25\": bm25, \"doc_ids\": doc_ids, \"corpus\": corpus}, save_path)\n",
        "    print(\"ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ BM25\")\n",
        "\n",
        "def load_bm25_model(path=BM25_MODEL_PATH):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(\"Ù†Ù…ÙˆØ°Ø¬ BM25 ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¦Ù‡ Ø£ÙˆÙ„Ø§Ù‹.\")\n",
        "    return joblib.load(path)\n",
        "\n",
        "def clean_and_tokenize_query(raw_query):\n",
        "    return clean_text(raw_query)\n",
        "\n",
        "def retrieve_bm25_docs(query_tokens, bm25_data, top_k=10):\n",
        "    bm25 = bm25_data[\"bm25\"]\n",
        "    doc_ids = bm25_data[\"doc_ids\"]\n",
        "    scores = bm25.get_scores(query_tokens)\n",
        "    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
        "    results = [(doc_ids[i], scores[i]) for i in ranked_indices]\n",
        "    return results\n",
        "\n",
        "# === ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===\n",
        "df = load_dataset(DOCS_PATH)\n",
        "# save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=CLEANED_TSV_PATH)\n",
        "\n",
        "# === Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ BM25 ===\n",
        "build_bm25_model(CLEANED_TSV_PATH, BM25_MODEL_PATH)\n",
        "\n",
        "# === ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ===\n",
        "bm25_data = load_bm25_model(BM25_MODEL_PATH)\n",
        "\n",
        "# === ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ===\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "    return df\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# === Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BM25 ===\n",
        "retrieved_docs_dict = {}\n",
        "for qid, tokens in tqdm(queries_tokens.items(), desc=\"ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BM25\"):\n",
        "    ranked = retrieve_bm25_docs(tokens, bm25_data, top_k=10)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "# === Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù†ÙØ³Ù‡Ø§ Ù…Ù† ÙƒÙˆØ¯Ùƒ Ø§Ù„Ø£ØµÙ„ÙŠ ===\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], skiprows=1)\n",
        "    df = df.dropna(subset=[\"query_id\", \"doc_id\", \"relevance\"])\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict, relevant_set = defaultdict(dict), defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]).strip(), str(row[\"doc_id\"]).strip(), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0: relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall\n",
        "    return np.mean(list(recall_scores.values())) * 100\n",
        "\n",
        "# === ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù€ qrels ===\n",
        "qrel_dict, real_relevant = get_qrels(QRELS_PATH)\n",
        "\n",
        "# === Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø¯Ø±Ø¬Ø§Øª ÙÙ‚Ø· ===\n",
        "retrieved_only_docs = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "# === ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ ===\n",
        "map_score = calculate_map(qrel_dict, retrieved_only_docs)\n",
        "mrr_score = calculate_mrr(qrel_dict, retrieved_only_docs)\n",
        "mean_precision = calculate_mean_precision(qrel_dict, retrieved_only_docs)\n",
        "mean_recall = calculate_mean_recall(qrel_dict, real_relevant, retrieved_only_docs)\n",
        "\n",
        "print(f\"MAP: {map_score:.2f}%\")\n",
        "print(f\"MRR: {mrr_score:.2f}%\")\n",
        "print(f\"Mean Precision: {mean_precision:.2f}%\")\n",
        "print(f\"Mean Recall: {mean_recall:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wqt6GeXkDZ1"
      },
      "source": [
        "#TFIDF WITH TOPK=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s6ukDMenIfn",
        "outputId": "07874c3d-22e3-47e2-8cd7-22c2cc9ef9ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… (Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ù†ØµØ¨Ø©)...\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            ">> Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\n",
            ">> ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK...\n",
            ">> ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...\n",
            ">> ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: 403458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403458/403458 [17:14<00:00, 389.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ: cleaned_docs12.tsv\n",
            ">> Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¹ÙƒØ³ÙŠ (Inverted Index)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403458/403458 [17:24<00:00, 386.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - Ø¹Ø¯Ø¯ ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙÙ‡Ø±Ø³: 160415\n",
            ">> ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª...\n",
            "   - Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: 200\n",
            ">> ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¹ÙƒØ³ÙŠ Ù…Ù† Ø§Ù„Ù…Ù„Ù...\n",
            ">> Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ø³ØªØ¹Ù…Ø§Ù„ TF-IDF...\n",
            "from file\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ">> ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403458/403458 [15:48<00:00, 425.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ TF-IDF\n",
            ">> Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„ÙƒÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:   6%|â–Œ         | 11/200 [00:02<00:35,  5.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 10 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  10%|â–ˆ         | 21/200 [00:04<00:28,  6.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 20 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  15%|â–ˆâ–Œ        | 30/200 [00:06<00:39,  4.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 30 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  20%|â–ˆâ–ˆ        | 41/200 [00:08<00:27,  5.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 40 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [00:10<00:30,  4.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 50 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [00:12<00:25,  5.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 60 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [00:13<00:22,  5.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 70 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [00:15<00:19,  6.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 80 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [00:17<00:18,  5.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 90 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [00:19<00:19,  5.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 100 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [00:21<00:16,  5.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 110 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [00:23<00:22,  3.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 120 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [00:26<00:12,  5.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 130 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [00:28<00:09,  6.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 140 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [00:29<00:08,  5.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 150 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [00:31<00:05,  7.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 160 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [00:33<00:05,  5.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 170 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [00:35<00:03,  6.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 180 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [00:38<00:02,  4.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 190 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:40<00:00,  4.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© 200 Ù…Ù† 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
            ">> ØªØ­Ù…ÙŠÙ„ qrels ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\n",
            ">> Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...\n",
            "\n",
            "== Evaluation for dataset: antique ==\n",
            "MAP: 20.09 %\n",
            "MRR: 76.6 %\n",
            "Mean Precision: 16.36 %\n",
            "Mean Recall: 44.08 %\n",
            "Mean Precision@10: 38.45 %\n"
          ]
        }
      ],
      "source": [
        "#ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·)\n",
        "print(\">> ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù… (Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ù†ØµØ¨Ø©)...\")\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "print(\">> Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª...\")\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk, contractions\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
        "print(\">> ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK...\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "INVERTED_PATH = f\"/content/drive/MyDrive/utils/inverted_index/{DATASET_NAME}_index12.json\"\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens12.tsv\"\n",
        "\n",
        "print(\">> ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "# Ø±Ø¨Ø· Ø§Ù„ÙˆØ³ÙˆÙ… Ø§Ù„ØµØ±ÙÙŠØ© Ø¨ÙˆÙˆØ±Ø¯Ù†Øª\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø§Ø®ØªØµØ§Ø±Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\",\n",
        "    \"r\": \"are\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"i'm\": \"i am\",\n",
        "    # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯\n",
        "}\n",
        "\n",
        "# ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØµØ§Ø±Ø§Øª\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "\n",
        "# Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ù†Øµ\n",
        "def processing(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    # text = normalize_date(text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² ØºÙŠØ± Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ±Ù…ÙŠØ²\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)     # Ø­Ø°Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø·\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)                     # Ø­Ø°Ù Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…\n",
        "    text = re.sub(r\"\\b\\d{1,2}\\b\", \"\", text)                 # Ø­Ø°Ù Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØµØºÙŠØ±Ø© ÙÙ‚Ø·\n",
        "    text = re.sub(r\"\\b\\d{5,}\\b\", \"\", text)                  # Ø­Ø°Ù Ø£Ø±Ù‚Ø§Ù… Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ (Ù…Ù…ÙƒÙ† ID Ø£Ùˆ Ø±Ù‚Ù… Ù‡Ø§ØªÙ)\n",
        "    return text\n",
        "\n",
        "# ØªÙˆÙƒÙ†ÙŠØ²ÙŠØ´Ù† ÙˆØªÙ„Ù…ÙŠØªØ§ÙŠØ²ÙŠØ´Ù†\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    lemmatized = [\n",
        "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        for word, pos in tokens_pos\n",
        "        if word not in stop_words and len(word) > 1\n",
        "    ]\n",
        "    return lemmatized\n",
        "\n",
        "# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
        "def clean_text(text):\n",
        "    preprocessing = processing(text)\n",
        "    tokenizing = tokenize(preprocessing)\n",
        "    return tokenizing\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def save_cleaned_docs(df, dataset_name=\"antique\", path_out=\"cleaned_docs12.tsv\"):\n",
        "    \"\"\"\n",
        "    ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ Ù…Ù„Ù TSV Ù…Ø¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:\n",
        "    doc_id, text, processed_text, dataset_name, light_clean_text\n",
        "    \"\"\"\n",
        "    cleaned_data = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ§¼ ØªÙ†Ø¸ÙŠÙ ÙˆØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"):\n",
        "        doc_id = row[\"doc_id\"]\n",
        "        raw_text = row[\"text\"]\n",
        "\n",
        "        # Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù…ÙŠÙ‚\n",
        "        processed_tokens = clean_text(raw_text)\n",
        "        processed_text = \" \".join(processed_tokens)\n",
        "\n",
        "        # Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø®ÙÙŠÙ\n",
        "        light_text = light_clean(raw_text)\n",
        "\n",
        "        cleaned_data.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"text\": raw_text,\n",
        "            \"processed_text\": processed_text,\n",
        "            \"dataset_name\": dataset_name,\n",
        "            \"light_clean_text\": light_text\n",
        "        })\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    cleaned_df.to_csv(path_out, sep=\"\\t\", index=False)\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ: {path_out}\")\n",
        "\n",
        "\n",
        "print(\">> ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...\")\n",
        "def load_dataset(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"doc_id\", \"text\"], header=None, skiprows=1)\n",
        "    df[\"text\"] = df[\"text\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "df = load_dataset(DOCS_PATH)\n",
        "print(f\"   - Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: {len(df)}\")\n",
        "\n",
        "save_cleaned_docs(df, dataset_name=DATASET_NAME, path_out=\"cleaned_docs12.tsv\")\n",
        "\n",
        "\n",
        "\n",
        "print(\">> Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¹ÙƒØ³ÙŠ (Inverted Index)...\")\n",
        "def build_inverted_index(df):\n",
        "    index = defaultdict(set)\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ”§ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³\"):\n",
        "        for token in clean_text(row[\"text\"]):\n",
        "            index[token].add(str(row[\"doc_id\"]))\n",
        "    os.makedirs(os.path.dirname(INVERTED_PATH), exist_ok=True)\n",
        "    with open(INVERTED_PATH, \"w\") as f:\n",
        "        json.dump({k: list(v) for k, v in index.items()}, f)\n",
        "    return index\n",
        "inverted_index = build_inverted_index(df)\n",
        "print(f\"   - Ø¹Ø¯Ø¯ ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙÙ‡Ø±Ø³: {len(inverted_index)}\")\n",
        "\n",
        "print(\">> ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª...\")\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = \" \".join(parts[1:])\n",
        "                data.append((query_id, text))\n",
        "\n",
        "    df = pd.DataFrame(data, columns=[\"query_id\", \"text\"])\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{' '.join(row['clean_text'])}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "print(f\"   - Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª: {len(queries_df)}\")\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2: continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "def retrieve_docs(query_tokens, index):\n",
        "    docs = set()\n",
        "    for token in query_tokens:\n",
        "        docs |= set(index.get(token, []))\n",
        "    return list(docs)\n",
        "\n",
        "print(\">> ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¹ÙƒØ³ÙŠ Ù…Ù† Ø§Ù„Ù…Ù„Ù...\")\n",
        "def load_inverted_index(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "inverted_index = load_inverted_index(INVERTED_PATH)\n",
        "\n",
        "print(\">> Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ø³ØªØ¹Ù…Ø§Ù„ TF-IDF...\")\n",
        "print(\"from file\")\n",
        "tqdm.pandas(desc=\">> ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ\")\n",
        "\n",
        "# Ù†Ø¸Ù Ø§Ù„Ù†ØµÙˆØµ Ù…Ø³Ø¨Ù‚Ø§Ù‹\n",
        "# df[\"clean_text\"] = df[\"text\"].progress_apply(lambda x: \" \".join(clean_text(x)))\n",
        "\n",
        "# # Ù†Ù…ÙˆØ°Ø¬ TF-IDF\n",
        "# tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf12.joblib\"\n",
        "\n",
        "# if os.path.exists(tfidf_model_path):\n",
        "#     tfidf_data = joblib.load(tfidf_model_path)\n",
        "#     vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "# else:\n",
        "#     print(\">> Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ TF-IDF\")\n",
        "#     vectorizer = TfidfVectorizer(lowercase=False)  # Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ù€ tokenizer/preprocessor Ø§Ù„Ø¢Ù†\n",
        "#     X = vectorizer.fit_transform(df[\"clean_text\"])\n",
        "#     tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "#     os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "#     joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].progress_apply(lambda x: \" \".join(clean_text(x)))\n",
        "\n",
        "if os.path.exists(tfidf_model_path):\n",
        "    tfidf_data = joblib.load(tfidf_model_path)\n",
        "    vectorizer, X, tfidf_doc_ids = tfidf_data[\"vectorizer\"], tfidf_data[\"vectors\"], tfidf_data[\"doc_ids\"]\n",
        "else:\n",
        "    print(\">> Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©: ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ TF-IDF\")\n",
        "     vectorizer = TfidfVectorizer(\n",
        "     preprocessor=processing,\n",
        "     tokenizer=tokenize,\n",
        "     lowercase=False,\n",
        "     token_pattern=None\n",
        "    )\n",
        "    X = vectorizer.fit_transform(df[\"clean_text\"])\n",
        "    tfidf_doc_ids = df[\"doc_id\"].astype(str).tolist()\n",
        "    os.makedirs(os.path.dirname(tfidf_model_path), exist_ok=True)\n",
        "    joblib.dump({\"vectorizer\": vectorizer, \"vectors\": X, \"doc_ids\": tfidf_doc_ids}, tfidf_model_path)\n",
        "\n",
        "\n",
        "def represent_query(tokens): return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def rank_docs(query_vec, retrieved_ids, doc_vectors, doc_ids, top_k=10):\n",
        "    id_to_idx = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
        "    indices = [id_to_idx[doc_id] for doc_id in retrieved_ids if doc_id in id_to_idx]\n",
        "    if not indices: return []\n",
        "    subset_vectors = doc_vectors[indices]\n",
        "    sims = cosine_similarity(query_vec, subset_vectors).flatten()\n",
        "    ranked_idx = np.argsort(-sims)[:top_k]\n",
        "    return [(retrieved_ids[i], sims[i]) for i in ranked_idx if sims[i] > 0]\n",
        "\n",
        "print(\">> Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„ÙƒÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª...\")\n",
        "retrieved_docs_dict = {}\n",
        "for i, (qid, tokens) in enumerate(tqdm(queries_tokens.items(), desc=\"ğŸ” Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª\"), start=1):\n",
        "    retrieved = retrieve_docs(tokens, inverted_index)\n",
        "    query_vec = represent_query(tokens)\n",
        "    ranked = rank_docs(query_vec, retrieved, X, tfidf_doc_ids, top_k=100)\n",
        "    retrieved_docs_dict[qid] = ranked\n",
        "\n",
        "    if i % 10 == 0 or i == len(queries_tokens):\n",
        "        print(f\"   - ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© {i} Ù…Ù† {len(queries_tokens)} Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\")\n",
        "\n",
        "# Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "print(\">> ØªØ­Ù…ÙŠÙ„ qrels ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\")\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], skiprows=1)\n",
        "    df = df.dropna(subset=[\"query_id\", \"doc_id\", \"relevance\"])\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict, relevant_set = defaultdict(dict), defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]).strip(), str(row[\"doc_id\"]).strip(), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0: relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        # if num_relevant == 0:\n",
        "        #     continue  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙŠ Ù„ÙŠØ³ Ù„Ù‡Ø§ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "print(\">> Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...\")\n",
        "def cal_evaluations(dataset_name=\"antique\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "cal_evaluations(\"antique\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qeq7pHYLkl-d"
      },
      "source": [
        "#EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMkREBslkjzd",
        "outputId": "6c687708-6c32-45ea-dcac-517504378408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding_antique.joblib\n",
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 200\n",
            "ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\n",
            "  query_id                                               text  \\\n",
            "0  3990512          how can we get concentration onsomething?   \n",
            "1   714612  Why doesn't the water fall off earth if it's r...   \n",
            "2  2528767  How do I determine the charge of the iron ion ...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
            "ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
            "ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
            "\n",
            "ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\n",
            "ğŸ“Œ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù„Ù„ØªÙ‚ÙŠÙŠÙ…: 200\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª qrels: 200\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹: 200\n",
            "ğŸ”— Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ø¨ÙŠÙ† qrels ÙˆØ§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹: 200\n",
            "â— Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø± Ù„Ù‡Ø§ ÙÙŠ qrels Ù„ÙƒÙ†Ù‡Ø§ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØªÙ…Ø«ÙŠÙ„: 0\n",
            "\n",
            "== Evaluation for dataset: antique ==\n",
            "MAP: 16.34 %\n",
            "MRR: 79.34 %\n",
            "Mean Precision: 17.62 %\n",
            "Mean Recall: 27.93 %\n",
            "Mean Precision@10: 39.7 %\n"
          ]
        }
      ],
      "source": [
        "import os, re, string, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ========\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_TSV_PATH =f\"/content/drive/MyDrive/utils/clean_docs/antique_cleaned_docs_antique\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"/content/drive/MyDrive/utils/queries_tokens/light_cleaned_queries_antique.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_antique.joblib\")\n",
        "TOP_K = 50\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ù†ØµÙˆØµ ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØªÙ…Ø«ÙŠÙ„Ù‡Ø§ ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"ğŸš€ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§ØªØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"ğŸ—‚ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        print(\"âš ï¸ Ø¹Ù…ÙˆØ¯ 'light_clean_text' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ â€” Ø³ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡.\")\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"light_clean_text\"].fillna(\"\").astype(str)\n",
        "\n",
        "    print(\"ğŸ§ª Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù†Ø¸ÙØ©:\")\n",
        "    print(df_docs[[\"doc_id\", \"light_clean_text\"]].head())\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ÙÙŠ: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: {path}\")\n",
        "\n",
        "    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ù„Ø£Ù† Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ ØºÙŠØ± Ù…Ù†ØªØ¸Ù… (Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…ÙˆØ²Ø¹Ø© Ø¹Ù„Ù‰ Ø£Ø¹Ù…Ø¯Ø© ÙƒØ«ÙŠØ±Ø©)\n",
        "    cleaned_queries = []\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        next(f)  # ØªØ®Ø·ÙŠ Ø§Ù„Ø±Ø£Ø³\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) >= 2:\n",
        "                query_id = parts[0]\n",
        "                text = ' '.join(parts[1:])\n",
        "                cleaned_queries.append((query_id, text))\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ DataFrame\n",
        "    df = pd.DataFrame(cleaned_queries, columns=['query_id', 'text'])\n",
        "\n",
        "    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(f\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df)}\")\n",
        "\n",
        "    print(\"ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    # Ø­ÙØ¸ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ø¸ÙŠÙØ© Ø¥Ù„Ù‰ Ù…Ù„Ù TSV Ø¨ØµÙŠØºØ©: query_id \\t light_clean_text\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# def load_and_clean_queries(path):\n",
        "#     print(f\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: {path}\")\n",
        "#     df = pd.read_csv(path, sep=\"\\t\", header=0, dtype=str)\n",
        "#     print(f\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df)}\")\n",
        "\n",
        "#     df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "#     print(\"ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\")\n",
        "#     print(df.head(3))\n",
        "\n",
        "#     with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "#         for _, row in df.iterrows():\n",
        "#             f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "#     return df\n",
        "\n",
        "\n",
        "# ======== ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ========\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            tokens = parts[1].split()\n",
        "            if not all(isinstance(t, str) for t in tokens):\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = tokens\n",
        "    return tokens_dict\n",
        "\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = []\n",
        "    query_texts = []\n",
        "    for qid, tokens in queries_tokens_dict.items():\n",
        "        if isinstance(tokens, list) and all(isinstance(t, str) for t in tokens):\n",
        "            query_ids.append(qid)\n",
        "            query_texts.append(\" \".join(tokens))\n",
        "    print(f\"ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ {len(query_texts)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ QRELS ========\n",
        "# def get_qrels(path=QRELS_PATH):\n",
        "#     df = pd.read_csv(path, sep=\"\\t\", names=[\"query_id\", \"doc_id\", \"relevance\"], header=0)\n",
        "#     df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "#     qrel_dict = defaultdict(dict)\n",
        "#     relevant_set = defaultdict(set)\n",
        "#     for _, row in df.iterrows():\n",
        "#         qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "#         qrel_dict[qid][docid] = rel\n",
        "#         if rel > 0:\n",
        "#             relevant_set[qid].add(docid)\n",
        "#     return qrel_dict, relevant_set\n",
        "\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø£ÙŠ Ù…Ø³Ø§ÙØ§Øª Ø£Ùˆ ØªØ§Ø¨ ÙƒÙØ§ØµÙ„\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=0, names=[\"query_id\", \"doc_id\", \"relevance\"])\n",
        "\n",
        "    # ØªØ£ÙƒØ¯ Ø£Ù† Ø¹Ù…ÙˆØ¯ relevance Ø±Ù‚Ù…ÙŠ\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "\n",
        "# ======== Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "# ======== Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ========\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"antique\"):\n",
        "    print(\"\\nğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "    print(f\"ğŸ“Œ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù„Ù„ØªÙ‚ÙŠÙŠÙ…: {len(retrieved_docs_str)}\")\n",
        "    print(\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª qrels:\", len(qrel_dict))\n",
        "    print(\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹:\", len(retrieved_docs_str))\n",
        "    print(\"ğŸ”— Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ø¨ÙŠÙ† qrels ÙˆØ§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹:\", len(set(qrel_dict.keys()) & set(retrieved_docs_str.keys())))\n",
        "\n",
        "# â›” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØªÙ…Ø«ÙŠÙ„\n",
        "    all_qrels_doc_ids = {docid for rels in qrel_dict.values() for docid in rels}\n",
        "    missing_doc_ids = all_qrels_doc_ids - set(doc_ids)\n",
        "    print(f\"â— Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø± Ù„Ù‡Ø§ ÙÙŠ qrels Ù„ÙƒÙ†Ù‡Ø§ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØªÙ…Ø«ÙŠÙ„: {len(missing_doc_ids)}\")\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== ØªØ´ØºÙŠÙ„ ÙƒØ§Ù…Ù„ ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ {len(query_embeddings)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
        "\n",
        "    retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "    print(f\"ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ {len(retrieved_docs)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=\"antique\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HYBRID"
      ],
      "metadata": {
        "id": "dD0IzSKT7zQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù…\n",
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm\n",
        "!pip install country_converter --upgrade\n",
        "!pip install datefinder\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os, re, string, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_PATH = os.path.join(BASE_PATH, \"docs.tsv\")\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "CLEAN_QUERIES = f\"/content/drive/MyDrive/utils/queries_tokens/{DATASET_NAME}_queries_tokens_antique1.tsv\"\n",
        "tfidf_model_path = f\"/content/drive/MyDrive/tfidf_models/{DATASET_NAME}_tfidf_antique.joblib\"\n",
        "embedding_file_path = f\"/content/drive/MyDrive/embedding_model_joblib_file/{DATASET_NAME}_embedding_antique.joblib\"\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "contractions_dict = {\n",
        "    \"u\": \"you\", \"r\": \"are\", \"wanna\": \"want to\",\n",
        "    \"can't\": \"cannot\", \"don't\": \"do not\", \"didn't\": \"did not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\"\n",
        "}\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'): return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'): return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contractions_dict.keys()) + r')\\b')\n",
        "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
        "\n",
        "def processing(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = expand_contractions(text, contractions_dict)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_pos = pos_tag(tokens)\n",
        "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "            for word, pos in tokens_pos\n",
        "            if word not in stop_words and len(word) > 1]\n",
        "\n",
        "def clean_text(text):\n",
        "    return tokenize(processing(text))\n",
        "\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: {path}\")\n",
        "\n",
        "    # Ù†Ù‚Ø±Ø£ Ø§Ù„Ù…Ù„Ù Ù†ØµÙŠØ§Ù‹ Ø«Ù… Ù†Ø­ÙˆÙ„ \\t Ø¥Ù„Ù‰ ØªØ¨ÙˆÙŠØ¨ Ø­Ù‚ÙŠÙ‚ÙŠ\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        lines = [line.encode().decode(\"unicode_escape\").strip() for line in f if line.strip()]\n",
        "\n",
        "    query_ids = []\n",
        "    texts = []\n",
        "\n",
        "    for i, line in enumerate(lines, 1):\n",
        "        parts = line.split('\\t', 1)  # Ù†Ù‚Ø³Ù… ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø£ÙˆÙ„ ØªØ¨ÙˆÙŠØ¨\n",
        "        if len(parts) != 2:\n",
        "            print(f\"âš ï¸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·Ø± {i}: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {len(parts)} Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 2\")\n",
        "            continue\n",
        "        query_ids.append(parts[0])\n",
        "        texts.append(parts[1])\n",
        "\n",
        "    df = pd.DataFrame({'query_id': query_ids, 'text': texts})\n",
        "    print(f\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df)}\")\n",
        "\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(\"ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    with open(CLEAN_QUERIES, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) != 2:\n",
        "                print(f\"âš ï¸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·Ø± {i}: ÙÙŠÙ‡ {len(parts)} Ø¬Ø²Ø¡ Ø¨Ø¯Ù„ 2\")\n",
        "                continue\n",
        "            tokens = parts[1].split()\n",
        "            if not all(isinstance(t, str) for t in tokens):\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = tokens\n",
        "    return tokens_dict\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
        "queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES)\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ TF-IDF\n",
        "tfidf_data = joblib.load(tfidf_model_path)\n",
        "vectorizer = tfidf_data[\"vectorizer\"]\n",
        "X = tfidf_data[\"vectors\"]\n",
        "tfidf_doc_ids = tfidf_data[\"doc_ids\"]\n",
        "\n",
        "# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embedding ÙˆØ§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª\n",
        "embedding_model_path = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "embedding_model = SentenceTransformer(embedding_model_path)\n",
        "embedding_data = joblib.load(embedding_file_path)\n",
        "doc_embeddings = embedding_data[\"embeddings\"]\n",
        "embedding_doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "def represent_query_tfidf(tokens):\n",
        "    return vectorizer.transform([\" \".join(tokens)])\n",
        "\n",
        "def represent_query_embedding(query_text):\n",
        "    return embedding_model.encode([query_text], convert_to_numpy=True)[0]\n",
        "\n",
        "def hybrid_rank(query_tokens, query_text, top_k=10, alpha=0.5):\n",
        "    tfidf_vec = represent_query_tfidf(query_tokens)\n",
        "    embedding_vec = represent_query_embedding(query_text)\n",
        "\n",
        "    tfidf_sims = cosine_similarity(tfidf_vec, X).flatten()\n",
        "\n",
        "    embedding_sims = cosine_similarity([embedding_vec], doc_embeddings).flatten()\n",
        "    embedding_sims = embedding_sims[1:]\n",
        "\n",
        "    hybrid_sims = alpha * tfidf_sims + (1 - alpha) * embedding_sims\n",
        "    top_indices = np.argsort(-hybrid_sims)[:top_k]\n",
        "    return [(tfidf_doc_ids[i], hybrid_sims[i]) for i in top_indices]\n",
        "\n",
        "# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
        "retrieved_docs_dict = {}\n",
        "for qid, tokens in tqdm(queries_tokens.items(), desc=\"Hybrid Retrieval\"):\n",
        "    text = \" \".join(tokens)\n",
        "    results = hybrid_rank(tokens, text, top_k=10, alpha=0.5)\n",
        "    retrieved_docs_dict[qid] = results\n",
        "\n",
        "# Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Ù…Ø«Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚)\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0: continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, []))\n",
        "        if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(dataset_name=\"antique\"):\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = get_retrieved_docs_formatted(retrieved_docs_dict)\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, recall_scores = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, precision_scores = calculate_precision_at_k(qrel_dict, retrieved_docs_str, k=10)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "cal_evaluations(\"antique\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84bMECsA71lB",
        "outputId": "d4821917-87e5-4f7f-af96-5527f26d0b7f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: country_converter in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from country_converter) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->country_converter) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0->country_converter) (1.17.0)\n",
            "Requirement already satisfied: datefinder in /usr/local/lib/python3.11/dist-packages (0.7.3)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from datefinder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datefinder) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.17.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "âš ï¸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·Ø± 1: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 1 Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 2\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 200\n",
            "ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hybrid Retrieval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [03:22<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== Evaluation for dataset: antique ==\n",
            "MAP: 8.32 %\n",
            "MRR: 72.42 %\n",
            "Mean Precision: 35.35 %\n",
            "Mean Recall: 11.29 %\n",
            "Mean Precision@10: 35.35 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMBEDDING WITH VECTOR STORE"
      ],
      "metadata": {
        "id": "38NCbAB_OQVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm faiss-cpu\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import os, re, string, time, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ========\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/antique_cleaned_docs_antique\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_antique.joblib\")\n",
        "FAISS_INDEX_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding.index\")\n",
        "FAISS_META_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_meta.json\")\n",
        "TOP_K = 50\n",
        "RETRIEVAL_MODE = \"faiss\"  # Ø§Ø®ØªØ± Ø¨ÙŠÙ† \"cosine\" Ø£Ùˆ \"faiss\"\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== ØªÙ†Ø¸ÙŠÙ Ù†ØµÙˆØµ ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ ØªÙˆÙ„ÙŠØ¯ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"ğŸš€ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§ØªØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"ğŸ—‚ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    # Ø­ÙØ¸ metadata Ù„Ù€ FAISS Ù„Ø§Ø­Ù‚Ø§Ù‹\n",
        "    with open(FAISS_META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ÙÙŠ: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def build_and_save_faiss_index(embeddings, index_path, meta_path, doc_ids):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)  # Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙˆØ¹ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    # Ø­ÙØ¸ Ø§Ù„Ù€ metadata (doc_ids)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ: {index_path}\")\n",
        "\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ========\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: {path}\")\n",
        "\n",
        "    # Ù†Ù‚Ø±Ø£ Ø§Ù„Ù…Ù„Ù Ù†ØµÙŠØ§Ù‹ Ø«Ù… Ù†Ø­ÙˆÙ„ \\t Ø¥Ù„Ù‰ ØªØ¨ÙˆÙŠØ¨ Ø­Ù‚ÙŠÙ‚ÙŠ\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        lines = [line.encode().decode(\"unicode_escape\").strip() for line in f if line.strip()]\n",
        "\n",
        "    query_ids = []\n",
        "    texts = []\n",
        "\n",
        "    for i, line in enumerate(lines, 1):\n",
        "        parts = line.split('\\t', 1)  # Ù†Ù‚Ø³Ù… ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø£ÙˆÙ„ ØªØ¨ÙˆÙŠØ¨\n",
        "        if len(parts) != 2:\n",
        "            print(f\"âš ï¸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·Ø± {i}: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {len(parts)} Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 2\")\n",
        "            continue\n",
        "        query_ids.append(parts[0])\n",
        "        texts.append(parts[1])\n",
        "\n",
        "    df = pd.DataFrame({'query_id': query_ids, 'text': texts})\n",
        "    print(f\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df)}\")\n",
        "\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(\"ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "def retrieve_with_faiss(query_embeddings_dict, index_path, doc_ids, top_k=10):\n",
        "    index = faiss.read_index(index_path)\n",
        "    query_ids = list(query_embeddings_dict.keys())\n",
        "    query_embeddings = np.array([query_embeddings_dict[qid] for qid in query_ids]).astype(\"float32\")\n",
        "    scores, indices = index.search(query_embeddings, top_k)\n",
        "\n",
        "    results = {}\n",
        "    for i, qid in enumerate(query_ids):\n",
        "        results[qid] = [(doc_ids[idx], float(scores[i][j])) for j, idx in enumerate(indices[i])]\n",
        "    return results\n",
        "\n",
        "def load_doc_ids_from_meta(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"antique\"):\n",
        "    print(\"\\nğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    build_and_save_faiss_index(embedding_data[\"embeddings\"], FAISS_INDEX_PATH, FAISS_META_PATH, embedding_data[\"doc_ids\"])\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ {len(query_embeddings)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
        "\n",
        "    if RETRIEVAL_MODE == \"cosine\":\n",
        "        print(\"\\nğŸš€ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… cosine_similarity...\")\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    elif RETRIEVAL_MODE == \"faiss\":\n",
        "        print(\"\\nâš¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS...\")\n",
        "        doc_ids = load_doc_ids_from_meta(FAISS_META_PATH)\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_faiss(query_embeddings, FAISS_INDEX_PATH, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    else:\n",
        "        raise ValueError(f\"âŒ Retrieval mode '{RETRIEVAL_MODE}' not supported.\")\n",
        "\n",
        "    print(f\"ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ {len(retrieved_docs)} Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ {round(duration, 2)} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=f\"{DATASET_NAME}-{RETRIEVAL_MODE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_ai5HTKOPtH",
        "outputId": "c1857b5e-0d03-4425-b915-d7c4f121a7be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "âš ï¸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·Ø± 1: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 1 Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 2\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 200\n",
            "ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding_antique.joblib\n",
            "âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding.index\n",
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "âš ï¸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·Ø± 1: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 1 Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 2\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 200\n",
            "ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
            "\n",
            "âš¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS...\n",
            "ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ 200 Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ 2.51 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\n",
            "\n",
            "== Evaluation for dataset: antique-faiss ==\n",
            "MAP: 16.34 %\n",
            "MRR: 79.34 %\n",
            "Mean Precision: 17.62 %\n",
            "Mean Recall: 27.93 %\n",
            "Mean Precision@10: 39.7 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EMBEDDING WITH COSINE"
      ],
      "metadata": {
        "id": "yeoQucVSSdam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk==3.8.1 contractions scikit-learn tqdm faiss-cpu\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "import os, re, string, time, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ§Ø±Ø¯ NLTK ========\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# ======== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ========\n",
        "DATASET_NAME = \"antique\"\n",
        "BASE_PATH = f\"/content/drive/MyDrive/datasets1/{DATASET_NAME}\"\n",
        "DOCS_TSV_PATH = \"/content/drive/MyDrive/utils/clean_docs/antique_cleaned_docs_antique\"\n",
        "QUERIES_PATH = os.path.join(BASE_PATH, \"queries.tsv\")\n",
        "QRELS_PATH = os.path.join(BASE_PATH, \"qrels.tsv\")\n",
        "BERT_MODEL_PATH = \"/content/drive/MyDrive/embedding_models/Bert_model/all-MiniLM-L6-v2\"\n",
        "CLEAN_QUERIES_PATH = \"light_cleaned_queries.tsv\"\n",
        "EMBEDDING_OUTPUT_PATH = \"/content/drive/MyDrive/embedding_model_joblib_file\"\n",
        "EMBEDDING_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_antique.joblib\")\n",
        "FAISS_INDEX_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding.index\")\n",
        "FAISS_META_PATH = os.path.join(EMBEDDING_OUTPUT_PATH, f\"{DATASET_NAME}_embedding_meta.json\")\n",
        "TOP_K = 50\n",
        "RETRIEVAL_MODE = \"cosine\"  # Ø§Ø®ØªØ± Ø¨ÙŠÙ† \"cosine\" Ø£Ùˆ \"faiss\"\n",
        "\n",
        "bert_model = SentenceTransformer(BERT_MODEL_PATH)\n",
        "\n",
        "# ======== ØªÙ†Ø¸ÙŠÙ Ù†ØµÙˆØµ ========\n",
        "def light_clean(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ ØªÙˆÙ„ÙŠØ¯ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ========\n",
        "def load_or_generate_doc_embeddings():\n",
        "    if os.path.exists(EMBEDDING_PATH):\n",
        "        print(f\"ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: {EMBEDDING_PATH}\")\n",
        "        return joblib.load(EMBEDDING_PATH)\n",
        "\n",
        "    print(\"ğŸš€ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§ØªØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...\")\n",
        "    df_docs = pd.read_csv(DOCS_TSV_PATH, sep=\"\\t\")\n",
        "    print(f\"ğŸ—‚ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(df_docs)}\")\n",
        "\n",
        "    if \"light_clean_text\" not in df_docs.columns:\n",
        "        df_docs[\"light_clean_text\"] = df_docs[\"text\"].astype(str).apply(light_clean)\n",
        "\n",
        "    doc_embeddings = bert_model.encode(\n",
        "        df_docs[\"light_clean_text\"].tolist(),\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    doc_ids = df_docs[\"doc_id\"].astype(str).tolist()\n",
        "    raw_docs = dict(zip(doc_ids, df_docs[\"text\"]))\n",
        "\n",
        "    os.makedirs(EMBEDDING_OUTPUT_PATH, exist_ok=True)\n",
        "    joblib.dump({\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }, EMBEDDING_PATH)\n",
        "\n",
        "    # Ø­ÙØ¸ metadata Ù„Ù€ FAISS Ù„Ø§Ø­Ù‚Ø§Ù‹\n",
        "    with open(FAISS_META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª ÙÙŠ: {EMBEDDING_PATH}\")\n",
        "    return {\n",
        "        \"embeddings\": doc_embeddings,\n",
        "        \"doc_ids\": doc_ids,\n",
        "        \"raw_docs\": raw_docs\n",
        "    }\n",
        "\n",
        "\n",
        "def build_and_save_faiss_index(embeddings, index_path, meta_path, doc_ids):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)  # Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙˆØ¹ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\n",
        "    index.add(embeddings.astype(\"float32\"))\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    # Ø­ÙØ¸ Ø§Ù„Ù€ metadata (doc_ids)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(doc_ids, f)\n",
        "\n",
        "    print(f\"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ: {index_path}\")\n",
        "\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ========\n",
        "\n",
        "def load_and_clean_queries(path):\n",
        "    print(f\"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: {path}\")\n",
        "\n",
        "    # Ù†Ù‚Ø±Ø£ Ø§Ù„Ù…Ù„Ù Ù†ØµÙŠØ§Ù‹ Ø«Ù… Ù†Ø­ÙˆÙ„ \\t Ø¥Ù„Ù‰ ØªØ¨ÙˆÙŠØ¨ Ø­Ù‚ÙŠÙ‚ÙŠ\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        lines = [line.encode().decode(\"unicode_escape\").strip() for line in f if line.strip()]\n",
        "\n",
        "    query_ids = []\n",
        "    texts = []\n",
        "\n",
        "    for i, line in enumerate(lines, 1):\n",
        "        parts = line.split('\\t', 1)  # Ù†Ù‚Ø³Ù… ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø£ÙˆÙ„ ØªØ¨ÙˆÙŠØ¨\n",
        "        if len(parts) != 2:\n",
        "            print(f\"âš ï¸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·Ø± {i}: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {len(parts)} Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 2\")\n",
        "            continue\n",
        "        query_ids.append(parts[0])\n",
        "        texts.append(parts[1])\n",
        "\n",
        "    df = pd.DataFrame({'query_id': query_ids, 'text': texts})\n",
        "    print(f\"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df)}\")\n",
        "\n",
        "    df[\"light_clean_text\"] = df[\"text\"].apply(light_clean)\n",
        "\n",
        "    print(\"ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    with open(CLEAN_QUERIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _, row in df.iterrows():\n",
        "            f.write(f\"{row['query_id']}\\t{row['light_clean_text']}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "\n",
        "\n",
        "def load_queries_tokens_from_tsv(path):\n",
        "    tokens_dict = {}\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            tokens_dict[str(parts[0])] = parts[1].split()\n",
        "    return tokens_dict\n",
        "\n",
        "def encode_queries(queries_tokens_dict, model):\n",
        "    query_ids = list(queries_tokens_dict.keys())\n",
        "    query_texts = [\" \".join(tokens) for tokens in queries_tokens_dict.values()]\n",
        "    embeddings = model.encode(query_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    return dict(zip(query_ids, embeddings))\n",
        "\n",
        "# ======== Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ========\n",
        "def retrieve_with_embedding(query_embeddings_dict, doc_embeddings, doc_ids, top_k=10):\n",
        "    results = {}\n",
        "    for qid, q_embed in query_embeddings_dict.items():\n",
        "        sims = cosine_similarity([q_embed], doc_embeddings).flatten()\n",
        "        ranked_idx = np.argsort(-sims)[:top_k]\n",
        "        results[qid] = [(doc_ids[i], float(sims[i])) for i in ranked_idx]\n",
        "    return results\n",
        "\n",
        "def retrieve_with_faiss(query_embeddings_dict, index_path, doc_ids, top_k=10):\n",
        "    index = faiss.read_index(index_path)\n",
        "    query_ids = list(query_embeddings_dict.keys())\n",
        "    query_embeddings = np.array([query_embeddings_dict[qid] for qid in query_ids]).astype(\"float32\")\n",
        "    scores, indices = index.search(query_embeddings, top_k)\n",
        "\n",
        "    results = {}\n",
        "    for i, qid in enumerate(query_ids):\n",
        "        results[qid] = [(doc_ids[idx], float(scores[i][j])) for j, idx in enumerate(indices[i])]\n",
        "    return results\n",
        "\n",
        "def load_doc_ids_from_meta(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# ======== ØªØ­Ù…ÙŠÙ„ QRELS ========\n",
        "def get_qrels(path=QRELS_PATH):\n",
        "    df = pd.read_csv(path, sep=r'\\s+', header=0)\n",
        "    df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "    qrel_dict = defaultdict(dict)\n",
        "    relevant_set = defaultdict(set)\n",
        "    for _, row in df.iterrows():\n",
        "        qid, docid, rel = str(row[\"query_id\"]), str(row[\"doc_id\"]), row[\"relevance\"]\n",
        "        qrel_dict[qid][docid] = rel\n",
        "        if rel > 0:\n",
        "            relevant_set[qid].add(docid)\n",
        "    return qrel_dict, relevant_set\n",
        "\n",
        "# ======== Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ========\n",
        "def get_retrieved_docs_formatted(retrieved):\n",
        "    return {qid: [doc_id for doc_id, _ in docs] for qid, docs in retrieved.items()}\n",
        "\n",
        "def calculate_map(qrels, retrieved):\n",
        "    ap_list = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        num_relevant = sum(1 for r in rel_docs.values() if r > 0)\n",
        "        if num_relevant == 0:\n",
        "            continue\n",
        "        ret_docs = retrieved.get(qid, [])\n",
        "        hits, score = 0, 0\n",
        "        for i, d in enumerate(ret_docs, 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                hits += 1\n",
        "                score += hits / i\n",
        "        ap_list.append(score / num_relevant)\n",
        "    return np.mean(ap_list) * 100 if ap_list else 0\n",
        "\n",
        "def calculate_mrr(qrels, retrieved):\n",
        "    scores = []\n",
        "    for qid, rel_docs in qrels.items():\n",
        "        for i, d in enumerate(retrieved.get(qid, []), 1):\n",
        "            if d in rel_docs and rel_docs[d] > 0:\n",
        "                scores.append(1 / i)\n",
        "                break\n",
        "        else:\n",
        "            scores.append(0)\n",
        "    return np.mean(scores) * 100\n",
        "\n",
        "def calculate_mean_precision(qrels, retrieved):\n",
        "    return np.mean([\n",
        "        sum(1 for d in retrieved.get(qid, []) if d in rel_docs and rel_docs[d] > 0) / len(retrieved.get(qid, [])) if retrieved.get(qid) else 0\n",
        "        for qid, rel_docs in qrels.items()\n",
        "    ]) * 100\n",
        "\n",
        "def calculate_mean_recall(qrels, real_relevant, retrieved):\n",
        "    recall_scores = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = set(retrieved[qid])\n",
        "        rel_docs = real_relevant.get(qid, set())\n",
        "        recall = len(ret_docs & rel_docs) / len(rel_docs) if rel_docs else 0\n",
        "        recall_scores[qid] = recall * 100\n",
        "    return np.mean(list(recall_scores.values())), recall_scores\n",
        "\n",
        "def calculate_precision_at_k(qrels, retrieved, k=10):\n",
        "    precisions = {}\n",
        "    for qid in retrieved:\n",
        "        ret_docs = retrieved[qid][:k]\n",
        "        rel_docs = qrels.get(qid, {})\n",
        "        hits = sum(1 for d in ret_docs if d in rel_docs and rel_docs[d] > 0)\n",
        "        precisions[qid] = (hits / k) * 100\n",
        "    return np.mean(list(precisions.values())), precisions\n",
        "\n",
        "def cal_evaluations(retrieved_docs, dataset_name=\"antique\"):\n",
        "    print(\"\\nğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\")\n",
        "    qrel_dict, real_relevant = get_qrels()\n",
        "    retrieved_docs_str = {qid: [str(docid) for docid in docs] for qid, docs in get_retrieved_docs_formatted(retrieved_docs).items()}\n",
        "\n",
        "    print(f\"\\n== Evaluation for dataset: {dataset_name} ==\")\n",
        "    print(\"MAP:\", round(calculate_map(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"MRR:\", round(calculate_mrr(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    print(\"Mean Precision:\", round(calculate_mean_precision(qrel_dict, retrieved_docs_str), 2), \"%\")\n",
        "    mean_recall, _ = calculate_mean_recall(qrel_dict, real_relevant, retrieved_docs_str)\n",
        "    print(\"Mean Recall:\", round(mean_recall, 2), \"%\")\n",
        "    mean_precision_at_k, _ = calculate_precision_at_k(qrel_dict, retrieved_docs_str)\n",
        "    print(\"Mean Precision@10:\", round(mean_precision_at_k, 2), \"%\")\n",
        "\n",
        "# ======== Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ ========\n",
        "if __name__ == \"__main__\":\n",
        "    embedding_data = load_or_generate_doc_embeddings()\n",
        "    build_and_save_faiss_index(embedding_data[\"embeddings\"], FAISS_INDEX_PATH, FAISS_META_PATH, embedding_data[\"doc_ids\"])\n",
        "    doc_embeddings = embedding_data[\"embeddings\"]\n",
        "    doc_ids = embedding_data[\"doc_ids\"]\n",
        "\n",
        "    queries_df = load_and_clean_queries(QUERIES_PATH)\n",
        "    queries_tokens = load_queries_tokens_from_tsv(CLEAN_QUERIES_PATH)\n",
        "    query_embeddings = encode_queries(queries_tokens, model=bert_model)\n",
        "\n",
        "    print(f\"ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ {len(query_embeddings)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
        "\n",
        "    if RETRIEVAL_MODE == \"cosine\":\n",
        "        print(\"\\nğŸš€ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… cosine_similarity...\")\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_embedding(query_embeddings, doc_embeddings, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    elif RETRIEVAL_MODE == \"faiss\":\n",
        "        print(\"\\nâš¡ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS...\")\n",
        "        doc_ids = load_doc_ids_from_meta(FAISS_META_PATH)\n",
        "        start = time.time()\n",
        "        retrieved_docs = retrieve_with_faiss(query_embeddings, FAISS_INDEX_PATH, doc_ids, top_k=TOP_K)\n",
        "        duration = time.time() - start\n",
        "    else:\n",
        "        raise ValueError(f\"âŒ Retrieval mode '{RETRIEVAL_MODE}' not supported.\")\n",
        "\n",
        "    print(f\"ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ {len(retrieved_docs)} Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ {round(duration, 2)} Ø«Ø§Ù†ÙŠØ©\")\n",
        "\n",
        "    cal_evaluations(retrieved_docs, dataset_name=f\"{DATASET_NAME}-{RETRIEVAL_MODE}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk6P-hyYSdIz",
        "outputId": "800f70a9-2607-43a3-8ca9-b8d0ec7cdaf7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.11/dist-packages (3.8.1)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "âš ï¸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·Ø± 1: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 1 Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 2\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 200\n",
            "ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…Ù†: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding_antique.joblib\n",
            "âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ: /content/drive/MyDrive/embedding_model_joblib_file/antique_embedding.index\n",
            "ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ù†: /content/drive/MyDrive/datasets1/antique/queries.tsv\n",
            "âš ï¸ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·Ø± 1: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 1 Ø£Ø¹Ù…Ø¯Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 2\n",
            "ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: 200\n",
            "ğŸ§ª Ø¹ÙŠÙ†Ø© Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:\n",
            "  query_id                                               text  \\\n",
            "0  3990512     how\\tcan\\twe\\tget\\tconcentration\\tonsomething?   \n",
            "1   714612  Why\\tdoesn't\\tthe\\twater\\tfall\\toff\\tearth\\tif...   \n",
            "2  2528767  How\\tdo\\tI\\tdetermine\\tthe\\tcharge\\tof\\tthe\\ti...   \n",
            "\n",
            "                                    light_clean_text  \n",
            "0           how can we get concentration onsomething  \n",
            "1  why doesn t the water fall off earth if it s r...  \n",
            "2  how do i determine the charge of the iron ion ...  \n",
            "ğŸ§  ØªÙ… ØªÙ…Ø«ÙŠÙ„ 200 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n",
            "\n",
            "ğŸš€ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… cosine_similarity...\n",
            "ğŸ“¥ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ 200 Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ 149.89 Ø«Ø§Ù†ÙŠØ©\n",
            "\n",
            "ğŸ“Š Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…...\n",
            "\n",
            "== Evaluation for dataset: antique-cosine ==\n",
            "MAP: 16.34 %\n",
            "MRR: 79.34 %\n",
            "Mean Precision: 17.62 %\n",
            "Mean Recall: 27.93 %\n",
            "Mean Precision@10: 39.7 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}